{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc22338c85d486a2",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "tune = \"0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99491884dd57ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omadbek/.conda/envs/archetype/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#https://colab.research.google.com/drive/1BEZ_qgtVqSmOmCTuhHs7lHiYB5M5_myg?usp=sharing\n",
    "\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "#import gzipƒ\n",
    "from tqdm.auto import tqdm\n",
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from itertools import chain\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from retry import retry\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9606158edf70524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEED = 42\n",
    "#random.seed(SEED)\n",
    "#np.random.seed(SEED)\n",
    "#torch.manual_seed(SEED)\n",
    "#torch.cuda.manual_seed_all(SEED)\n",
    "#torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d565ecc3-2433-4b7b-bfa7-d8225b0db815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EST_CHARS_PER_TOKEN=4\n",
      "MAX_LEN=2000*EST_CHARS_PER_TOKEN\n",
      "INTEGER_SET = set(r\"0123456789,/\\+-.^_()[] :\")\n",
      "BOOLEAN_SET = set([\"True\", \"true\", \"False\", \"false\", \"yes\", \"Yes\", \"No\", \"no\"])\n",
      "\n",
      "ARCHETYPE_PATH = \"/home/omadbek/projects/ArcheType\"\n",
      "DOTENV_PATH = \"/home/omadbek/projects/ArcheType/.env\"\n"
     ]
    }
   ],
   "source": [
    "!cat /home/omadbek/projects/ArcheType/src/const.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "881a695b-e8d4-4c8b-ab64-52f8152df91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/omadbek/projects/ArcheType\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e14d5a0-ee3e-4fd7-8731-41a907223247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.predict import ArcheTypePredictor\n",
    "\n",
    "TEST_FILE_PATH = \"./table_samples/Book_5sentidoseditora.pt_September2020_CTA.json\"\n",
    "df = pd.read_json(TEST_FILE_PATH, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a93e55-abb0-4938-86f5-eb75925a80ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Desejo Subtil (eBook)</td>\n",
       "      <td>978-972-0-68199-7</td>\n",
       "      <td>336</td>\n",
       "      <td>2013-06-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Desejo Subtil</td>\n",
       "      <td>978-972-0-04396-2</td>\n",
       "      <td>336</td>\n",
       "      <td>2014-04-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rendida</td>\n",
       "      <td>978-972-0-04429-7</td>\n",
       "      <td>352</td>\n",
       "      <td>2016-05-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Preferida</td>\n",
       "      <td>978-989-745-023-5</td>\n",
       "      <td>448</td>\n",
       "      <td>2016-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Confia em mim</td>\n",
       "      <td>978-989-745-028-0</td>\n",
       "      <td>272</td>\n",
       "      <td>2016-06-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0                  1    2           3\n",
       "0  Desejo Subtil (eBook)  978-972-0-68199-7  336  2013-06-12\n",
       "1          Desejo Subtil  978-972-0-04396-2  336  2014-04-23\n",
       "2                Rendida  978-972-0-04429-7  352  2016-05-25\n",
       "3              Preferida  978-989-745-023-5  448  2016-12-05\n",
       "4          Confia em mim  978-989-745-028-0  272  2016-06-16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dafde7b-0eef-4af8-a652-012a533151a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())      # → 1\n",
    "print(torch.cuda.get_device_name(0))  # → the one you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f14519-48c9-4c42-9aca-9e0945b3aeda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee7c1b25-6937-45ad-ac6e-1b41a995bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./custom_data\"\n",
    "\n",
    "# Testing\n",
    "X_test = pd.read_parquet(f\"{data_dir}/test_data.parquet\")\n",
    "y_test = pd.read_parquet(f\"{data_dir}/test_labels.parquet\")\n",
    "\n",
    "#cta_gt = load_and_remap_cta_gt(f\"{data_dir}/cta_gt.csv\", LABEL_MAP_LC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a9e21bc-f2ed-497d-9787-3105aa55f53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['age', 'case_status', 'contact_setting', 'date', 'gender', 'id',\n",
       "       'location', 'medical_boolean', 'occupation', 'outcome', 'symptoms'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "229d1025-a37a-4c7d-95f9-5714da81c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_labels(label, label_set):\n",
    "  label = label.lower().strip()\n",
    "  ldm = {k.lower().strip() : v.lower().strip() for k, v in label_set['dict_map'].items()}\n",
    "  if label_set.get(\"abbrev_map\", -1) != -1:\n",
    "    lda = {k.lower().strip() : v.lower().strip() for k, v in label_set['abbrev_map'].items()}\n",
    "    ldares = lda.get(label, \"\")\n",
    "    if ldares != \"\":\n",
    "      label = ldares\n",
    "  if label.endswith(\"/name\"):\n",
    "    label = label[:-5]\n",
    "  remap = ldm.get(label, -1)\n",
    "  if remap != -1:\n",
    "    label = remap\n",
    "  return label.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b47fba85-2e32-4803-82d4-278a673a03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LABELS = ['age', 'case_status', 'contact_setting', 'date', 'gender', 'id',\n",
    "#       'location', 'medical_boolean', 'occupation', 'outcome', 'symptoms']\n",
    "\n",
    "LABELS = ['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1db475a1-be44-4499-980d-127a62cab476",
   "metadata": {},
   "outputs": [],
   "source": [
    "sotab_integer_labels = [\"age\", \"date\"]\n",
    "sotab_float_labels   = []\n",
    "\n",
    "# everything else must go here\n",
    "sotab_other_labels = [\n",
    "  \"case_status\",\n",
    "  \"gender\",\n",
    "  \"id\",\n",
    "  \"location\",\n",
    "  \"medical_boolean\",\n",
    "  \"occupation\",\n",
    "  \"outcome\",\n",
    "  \"symptoms\"\n",
    "]\n",
    "\n",
    "sotab_top_hier = {\n",
    "  \"integer\": sotab_integer_labels,\n",
    "  \"float\":   sotab_float_labels,\n",
    "  \"other\":   sotab_other_labels\n",
    "}\n",
    "\n",
    "sotab_identifier = [\"id\"]\n",
    "sotab_category   = [\"gender\", \"medical_boolean\", \"outcome\"]\n",
    "sotab_text       = [\"location\", \"symptoms\", \"occupation\"]\n",
    "\n",
    "\n",
    "sotab_other_hier = {\n",
    "  \"Identifier\": sotab_identifier,\n",
    "  \"category\":   sotab_category,\n",
    "  \"text\":       sotab_text\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60e79430-47a7-4d5a-9f97-b5524ed9305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed=13\n",
    "EST_CHARS_PER_TOKEN=4\n",
    "MAX_LEN=2000*EST_CHARS_PER_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff3bbf8b-5706-486b-9cd0-857411301200",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/omadbek/projects/alpaca/outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "596e3d86-08c9-4112-be36-fb59dcf5fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You have a fixed set of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2834bac3-855c-4481-af36-1bfd99418fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Picker agent\n",
    "#        prompt = f\"\"\"\n",
    "#                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
    "#                    \n",
    "#                    INSTRUCTIONS:\n",
    "#                    • You have a fixed set of labels. OPTIONS: {options_str}\n",
    "#                    • Choose exactly ONE label from that list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select OPTIONS if it's just a little bit matches with INPUT.\n",
    "#                    • If none OPTIONS apply, output only NONE.\n",
    "#                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
    "#                    \n",
    "#                    INPUT: {input_list}\n",
    "#                    OPTIONS: {options_str}\n",
    "#                    ANSWER:\n",
    "#                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c10f1c02-1fd2-4795-9ee0-17b6b16ad4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPTS\n",
    "\n",
    "def llm_prompts(agent, input_list, options_str = None):\n",
    "\n",
    "    if agent == \"generator\":\n",
    "        # Generate agent\n",
    "        prompt = f\"\"\"\n",
    "            System: You are an epidemiology data steward.\n",
    "            \n",
    "            INSTRUCTIONS:\n",
    "            • You’ll get a comma-separated list of anonymized tokens (IDs, dates, names, etc.).\n",
    "            • Invent exactly one **broad**, high-level snake_case label that best summarizes what kind of field this is.\n",
    "              – Think “date”, “identifier”, “status”, “count”, “category”, etc.\n",
    "              – Avoid overly specific terms like “case_date_range_2020_july” or “lab_test_ids”.\n",
    "            • Do NOT copy or overlap any part of the input tokens.\n",
    "            • Return only that one label, no extra text.\n",
    "            \n",
    "            Now it’s your turn:\n",
    "            Input: {input_list}\n",
    "            Output:\n",
    "        \"\"\"\n",
    "    else:\n",
    "        \n",
    "        # Picker agent\n",
    "        prompt = f\"\"\"\n",
    "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
    "                    \n",
    "                    INSTRUCTIONS:\n",
    "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
    "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
    "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
    "                    \n",
    "                    INPUT: {input_list}\n",
    "                    OPTIONS: {options_str}\n",
    "                    ANSWER:\n",
    "                \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7bdfa-04c5-4106-b791-166e74bbc98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba93a7e3-1802-4db0-96e5-6e9f8c9e1921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76159fc4-0916-44b2-b99d-e6eca91be82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import infer_auto_device_map, init_empty_weights, load_checkpoint_and_dispatch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, \\\n",
    "    T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "import langchain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
    "\n",
    "\n",
    "def set_pipeline(k=1):\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=MAX_LEN,\n",
    "        temperature=0.5 * k,\n",
    "        top_p=0.80 - (0.1 * k),\n",
    "        repetition_penalty=1.3\n",
    "    )\n",
    "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    llm_chain = LLMChain(prompt=pt,\n",
    "                         llm=local_llm\n",
    "                         )\n",
    "    return pipe, local_llm, llm_chain\n",
    "\n",
    "\n",
    "curr_model = \"\"\n",
    "\n",
    "\n",
    "def init_model(model):\n",
    "    curr_model = model\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    if model == \"llama-65b\":\n",
    "        LLAMA_PATH = \"/scratch/bf996/text-generation-webui/models/llama-65b-hf\"\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(LLAMA_PATH)\n",
    "        config = AutoConfig.from_pretrained(LLAMA_PATH,\n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            load_in_8bit=True)\n",
    "        with init_empty_weights():\n",
    "            base_model = AutoModelForCausalLM.from_config(config)\n",
    "        base_model.tie_weights()\n",
    "        device_map = infer_auto_device_map(base_model, max_memory={0: \"60GiB\", \"cpu\": \"96GiB\"})\n",
    "        base_model = load_checkpoint_and_dispatch(\n",
    "            base_model,\n",
    "            LLAMA_PATH,\n",
    "            device_map=device_map\n",
    "        )\n",
    "    elif model == \"alpaca-13b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "        #tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "        base_model = LlamaForCausalLM.from_pretrained(\n",
    "            #model_path,\n",
    "            \"chavinlo/alpaca-native\",\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_8bit=True,\n",
    "            device_map='auto',\n",
    "        )\n",
    "    elif model == \"alpaca-fine-tuned\":\n",
    "        MAX_LEN = 2048\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    elif model == \"vicuna-13b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"eachadea/vicuna-13b\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"eachadea/vicuna-13b\",\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_8bit=True,\n",
    "            device_map='auto',\n",
    "        )\n",
    "    elif model == \"gpt4-x-alpaca\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"chavinlo/gpt4-x-alpaca\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\"chavinlo/gpt4-x-alpaca\", device_map=\"auto\",\n",
    "                                                          load_in_8bit=True)\n",
    "    elif model == \"t0pp\":\n",
    "        MAX_LEN = 512\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0pp\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\",\n",
    "                                                           torch_dtype=torch.float16, load_in_8bit=True)\n",
    "    elif model == \"flan-t5-xxl\":\n",
    "        MAX_LEN = 512\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\",\n",
    "                                                           torch_dtype=torch.float16, load_in_8bit=True)\n",
    "    elif model == \"flan-ul2\":\n",
    "        MAX_LEN = 512\n",
    "        base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16,\n",
    "                                                                device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n",
    "    elif model == \"galpaca-30b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"GeorgiaTechResearchInstitute/galpaca-30b\", device_map=\"auto\",\n",
    "                                                  torch_dtype=torch.float16, load_in_8bit=True)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\"GeorgiaTechResearchInstitute/galpaca-30b\")\n",
    "    elif model == \"opt-iml-max-30b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-iml-max-30b\", use_fast=False, padding_side='left')\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-iml-max-30b\", device_map=\"auto\",\n",
    "                                                          torch_dtype=torch.float16)\n",
    "    if model in [\"flan-t5-xxl\", \"t0pp\", \"flan-ul2\"]:\n",
    "        template = \"\"\"{instruction}\"\"\"\n",
    "    else:\n",
    "        template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "        ### Instruction: \n",
    "        {instruction}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    pt = PromptTemplate(template=template, input_variables=[\"instruction\"])\n",
    "    #Convert length from tokens to characters, leave room for model response\n",
    "    MAX_LEN = MAX_LEN * EST_CHARS_PER_TOKEN - 200\n",
    "    return base_model, tokenizer, template, pt, MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faa69500-a350-4dc9-9204-f5bea462888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sherlock_resp(df, gt_df, prompt_dict, model, label_indices, base_prompt, lsd):\n",
    "    isd4 = \"d4\" in lsd['name']\n",
    "    if \"sherlock\" in model:\n",
    "        model = sherlock_model\n",
    "        data_m = pd.Series(df[label_indices].astype(str).T.values.tolist())\n",
    "        extract_features(\n",
    "            \"../temporary.csv\",\n",
    "            data_m\n",
    "        )\n",
    "        feature_vectors = pd.read_csv(\"../temporary.csv\", dtype=np.float32)\n",
    "        predicted_labels = model.predict(feature_vectors, \"sherlock\")\n",
    "        iter_len = len(data_m)\n",
    "    elif \"doduo\" in model:\n",
    "        model = doduo_model\n",
    "        data_m = df[label_indices]\n",
    "        try:\n",
    "            annot_m = doduo_model.annotate_columns(data_m)\n",
    "            predicted_labels = annot_m.coltypes\n",
    "        except Exception as e:\n",
    "            print(f\"Exception {e} in Doduo, returning default \\n\")\n",
    "            predicted_labels = [\"text\" for i in range(len(data_m))]\n",
    "        iter_len = len(predicted_labels)\n",
    "    predicted_labels_dict = {i: sherlock_to_cta.get(predicted_labels[i], [predicted_labels[i]]) for i in\n",
    "                             range(iter_len)}\n",
    "\n",
    "    for idx, label_idx in zip(range(iter_len), label_indices):\n",
    "        prompt = base_prompt + \"_\" + str(label_idx)\n",
    "        if isd4:\n",
    "            ans = predicted_labels[0]\n",
    "            label = [s.lower() for s in lsd['d4_map'][gt_df]]\n",
    "        else:\n",
    "            gt_row = gt_df[gt_df['column_index'] == label_idx]\n",
    "            if len(gt_row) != 1:\n",
    "                continue\n",
    "            label = fix_labels(gt_row['label'].item(), lsd)\n",
    "            ans = [fix_labels(item, lsd) for item in predicted_labels_dict[idx]]\n",
    "        if isd4:\n",
    "            res = ans in label\n",
    "        else:\n",
    "            assert isinstance(ans, list), \"ans should be a list\"\n",
    "            res = label in ans\n",
    "        ans_dict = {\"response\": ans, \"context\": None, \"ground_truth\": label, \"correct\": res,\n",
    "                    \"orig_model_label\": predicted_labels[idx]}\n",
    "        prompt_dict[prompt] = ans_dict\n",
    "    return prompt\n",
    "\n",
    "\n",
    "@retry(Exception, tries=3, delay=3)\n",
    "def get_chatgpt_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None,\n",
    "                     method=[\"similarity\"], max_len=15000):\n",
    "    fixed_labels = [fix_labels(s, lsd) for s in lsd['label_set']]\n",
    "    model = \"gpt-3.5\"\n",
    "    context_labels = \", \".join(fixed_labels)\n",
    "    fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n",
    "    prompt = prompt_context_insert(context_labels, context, max_len, \"gpt-3.5\")\n",
    "    d_p = prompt_dict.get(prompt, -1)\n",
    "    if d_p != -1 and \"skip-existing\" in method:\n",
    "        #recompute_results(prompt_dict, prompt, model, cbc_pred=None, label_set=lsd)\n",
    "        return prompt\n",
    "    elif d_p != -1:\n",
    "        while prompt_dict.get(prompt, -1) != -1:\n",
    "            prompt = prompt + \"*\"\n",
    "    if response:\n",
    "        ans = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0,\n",
    "        ).choices[0]['message']['content']\n",
    "        #print(f\"Original ans is {ans}\")\n",
    "    ans_n = fuzzy_label_match(ans, fixed_labels, None, None, prompt, lsd, model, method=method)\n",
    "    #print(f\"Fuzzy ans is {ans_n}\")\n",
    "    res = ans_n == ground_truth\n",
    "    ans_dict = {\"response\": ans_n, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res,\n",
    "                \"original_model_answer\": ans}\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "    return prompt\n",
    "\n",
    "\n",
    "@retry(Exception, tries=5, delay=3)\n",
    "def get_ada_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None):\n",
    "    prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"ada-personal\")\n",
    "    if prompt_dict.get(prompt, -1) != -1:\n",
    "        #recompute_results(prompt_dict, prompt, \"ada-personal\", label_set=lsd)\n",
    "        return prompt\n",
    "    if response:\n",
    "        proc = subprocess.run(\n",
    "            [\"openai\", \"api\", \"completions.create\", \"-m\", \"ada:ft-personal:-2023-03-14-11-52-45\", \"-M\", \"3\", \"-p\",\n",
    "             prompt], capture_output=True, check=True)\n",
    "        ans = proc.stdout.decode(\"utf-8\")[len(prompt):].strip()\n",
    "    else:\n",
    "        ans = \"\"\n",
    "    res = ans.lower().strip().startswith(ground_truth)\n",
    "    ans_dict = {\"response\": ans, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res}\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "    return prompt\n",
    "\n",
    "\"\"\"\n",
    "def call_llama_model(session, link, prompt, lsd, var_params):\n",
    "    fixed_labels = [fix_labels(s, lsd) for s in lsd['label_set']]\n",
    "    if session:\n",
    "        ans = session.post(link, json=make_json(prompt, var_params))\n",
    "    else:\n",
    "        ans = requests.post(link, json=make_json(prompt, var_params))\n",
    "    ans = ans.json()[\"data\"]\n",
    "    ans_n = fix_labels(ans[0][len(prompt):].strip(), lsd)\n",
    "    return ans_n\n",
    "\"\"\"\n",
    "\n",
    "def call_llama_model_label_generation(session, link, prompt, lsd, var_params):\n",
    "    # Build the payload expected by the new LLaMA endpoint\n",
    "    payload = {\n",
    "        \"model\":      \"llama3.1:8b-instruct-q8_0\",\n",
    "        \"prompt\":     prompt,\n",
    "        \"max_tokens\": 60,\n",
    "        \"stream\":     False\n",
    "    }\n",
    "\n",
    "    # Choose session-based or direct requests call\n",
    "    client = session or requests\n",
    "    resp = client.post(link, json=payload)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    # Extract the generated text\n",
    "    data = resp.json()\n",
    "    text = data.get(\"response\", \"\")\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def call_llama_model(session, link, prompt, lsd, var_params):\n",
    "    # Build the payload expected by the new LLaMA endpoint\n",
    "    payload = {\n",
    "        \"model\":      \"llama3.1:8b-instruct-q8_0\",\n",
    "        \"prompt\":     prompt,\n",
    "        \"max_tokens\": 30,\n",
    "        \"stream\":     False\n",
    "    }\n",
    "\n",
    "    # Choose session-based or direct requests call\n",
    "    client = session or requests\n",
    "    resp = client.post(link, json=payload)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    # Extract the generated text\n",
    "    data = resp.json()\n",
    "    text = data.get(\"response\", \"\")\n",
    "\n",
    "    #if text == \"none\":\n",
    "    #    return generate_label(session, link, prompt, lsd, var_params)\n",
    "    #else:\n",
    "    return fix_labels(text.strip(), lsd)\n",
    "\n",
    "temperature = 0\n",
    "top_p = 0\n",
    "\n",
    "def extract_answer(orig_ans: str) -> str:\n",
    "    \"\"\"\n",
    "    If orig_ans contains 'ANSWER:...', return the text after the colon.\n",
    "    Otherwise, return orig_ans unchanged (stripped).\n",
    "    \"\"\"\n",
    "    m = re.search(r\"ANSWER\\s*:\\s*(.*)\", orig_ans, re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return orig_ans.strip()\n",
    "\n",
    "#generated_labels_list = []\n",
    "\n",
    "def generate_label(session, link, old_prompt, lsd, var_params, generated_labels_list, orig_ans):\n",
    "\n",
    "    if orig_ans.lower() != \"none\" and orig_ans not in generated_labels_list:\n",
    "        generated_labels_list.append(orig_ans)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = re.search(r\"INPUT\\s*:\\s*(\\[[^\\]]*\\])\", old_prompt)\n",
    "    if not m:\n",
    "        raise ValueError(\"Couldn't find an INPUT[...] section\")\n",
    "        \n",
    "    input_list = m.group(1) # take INPUT values\n",
    "\n",
    "    # Generate agent\n",
    "    new_prompt = llm_prompts(\"generator\", input_list)\n",
    "    \n",
    "    # label was generated\n",
    "    new_label = call_llama_model_label_generation(prompt = new_prompt, link = link, session = session, lsd=lsd, var_params=var_params)\n",
    "\n",
    "    print(f\"Generator generated label: {new_label}\")\n",
    "\n",
    "    if new_label.lower() != \"none\" and new_label not in generated_labels_list:\n",
    "        generated_labels_list.append(new_label)\n",
    "\n",
    "    #picker_prompt = prompt_context_insert(\n",
    "    #    context_labels=\"\",            # not used by llama-branch\n",
    "    #    context=input_list,           # your array literal string\n",
    "    #    model=\"llama\",\n",
    "    #    options=generated_labels_list # ← here’s your dynamic list\n",
    "    #)\n",
    "\n",
    "    # build options\n",
    "    options_str = \" - \".join(generated_labels_list)\n",
    "\n",
    "    picker_prompt = llm_prompts(\"picker\", input_list, options_str)\n",
    "\n",
    "    # call the picker again (now it should pick new_label)\n",
    "    picked = call_llama_model(session, link, picker_prompt, lsd, var_params)\n",
    "\n",
    "    print(f\"Picker 2 final label to pickup: {picked}\")\n",
    "\n",
    "    \"\"\"\n",
    "          \n",
    "    return picker_prompt, picked\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "@retry(Exception, tries=3, delay=3)\n",
    "def get_topp_resp(prompt, k):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "\n",
    "    temperature = 0.1 * k\n",
    "    top_p   = 0.90 - (0.1 * k)\n",
    "\n",
    "    outputs = base_model.generate(inputs,\n",
    "                                  max_length=MAX_LEN,\n",
    "                                  #do_sample=False,\n",
    "                                  #num_beams=1\n",
    "                                  temperature=temperature,\n",
    "                                  top_p=top_p,\n",
    "                                  repetition_penalty=1.3\n",
    "                                  )\n",
    "    orig_ans = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return extract_answer(orig_ans)\n",
    "    # 2. Second - Fine-Tuned\n",
    "\n",
    "    \"\"\"\n",
    "    m = re.search(r\"ANSWER\\s*:\\s*(.*)\", orig_ans)\n",
    "    if m:\n",
    "        # group(1) is “whatever comes after the colon, up until the end of the line”\n",
    "        after = m.group(1).strip()\n",
    "        # If after is non‐empty, split on whitespace or newline to get the first token\n",
    "        if after:\n",
    "            predicted = after.split()[0]\n",
    "        else:\n",
    "            # Matched “ANSWER:” but there was nothing after it\n",
    "            predicted = \"Didn't printed answer!\"\n",
    "    else:\n",
    "        # No “ANSWER:” at all in full_text\n",
    "        predicted = \"NO ANSWER PROVIDED!\"\n",
    "\n",
    "    return predicted\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # 3. Full answer\n",
    "    if \"ANSWER:\" in orig_ans:\n",
    "        # keep the colon, if you like, otherwise change to split(\"ANSWER:\", 1)[1]\n",
    "        after = orig_ans.split(\"ANSWER:\", 1)[1].strip()\n",
    "    else:\n",
    "        after = orig_ans.strip()\n",
    "    \n",
    "    return after\n",
    "     \"\"\"\n",
    "\n",
    "\n",
    "@retry(Exception, tries=3, delay=3)\n",
    "def get_llama_resp(lsd: dict, context: list, ground_truth: str, prompt_dict: dict, link: str, response=True,\n",
    "                   session=None, cbc=None, model=\"llama\", limited_context=None,\n",
    "                   method=[\"ans_contains_gt\", \"gt_contains_ans\", \"resample\"], generated_labels_list = None):\n",
    "    #print(f\"in get llama resp, gt is {ground_truth}, context is {context}\")\n",
    "    isd4 = \"d4\" in lsd['name']\n",
    "    if isd4:\n",
    "        gtv = lsd['d4_map'][ground_truth]\n",
    "        if isinstance(gtv, str):\n",
    "            gtv = [gtv]\n",
    "        ground_truth = [s.lower() for s in gtv]\n",
    "    if \"hierarchical\" in method and not isd4:\n",
    "        dtype = get_base_dtype(limited_context)\n",
    "        fixed_labels = sotab_top_hier[dtype]\n",
    "    else:\n",
    "        fixed_labels = list(set([fix_labels(s, lsd) for s in lsd['label_set']]))\n",
    "    context_labels = \", \".join(fixed_labels)\n",
    "    fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n",
    "    if model in [\"llama-zs\", \"opt-iml-30b-zs\"]:\n",
    "        pipe, local_llm, llm_chain = set_pipeline(k=1)\n",
    "    prompt = prompt_context_insert(context_labels, context, MAX_LEN, model, options = generated_labels_list)\n",
    "    d_p = prompt_dict.get(prompt, -1)\n",
    "    #skip existing logic\n",
    "    if d_p != -1 and \"skip-existing\" in method:\n",
    "        # recompute_results(prompt_dict, prompt, \"llama\", cbc, lsd)\n",
    "        return prompt, prompt_dict[prompt][\"response\"]\n",
    "    elif d_p != -1:\n",
    "        while prompt_dict.get(prompt, -1) != -1:\n",
    "            prompt = prompt + \"*\"\n",
    "    #print(\"GET LLAMA NEETY GREEDY:\")\n",
    "    #print(prompt)\n",
    "    #response logic\n",
    "    if not response:\n",
    "        orig_ans = ans_n = \"\"\n",
    "    else:\n",
    "        orig_ans = apply_basic_rules(limited_context, None)\n",
    "        if orig_ans is None:\n",
    "            orig_ans = query_correct_model(model, prompt, context_labels, context, session, link, lsd)\n",
    "            \n",
    "            #hierarchical matching logic\n",
    "            if \"hierarchical\" in method and dtype == \"other\" and orig_ans not in ['email', 'URL', 'WebHTMLAction',\n",
    "                                                                                  'Photograph']:\n",
    "                next_label_set = sotab_other_hier.get(orig_ans, -1)\n",
    "                if next_label_set == -1:\n",
    "                    print(f\"Original answer {orig_ans} not found in hierarchy\")\n",
    "                    next_label_set = sotab_other_hier['text']\n",
    "                fixed_labels = list(set([fix_labels(s, lsd) for s in next_label_set]))\n",
    "                context_labels = \", \".join(fixed_labels)\n",
    "                fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n",
    "                orig_ans = query_correct_model(model, prompt, context_labels, context, session, link, lsd)\n",
    "                #fuzzy matching logic\n",
    "            #ans_n = fuzzy_label_match(orig_ans, fixed_labels, session, link, prompt, lsd, model, method=method).lower()\n",
    "            ans_n = orig_ans.lower()\n",
    "        else:\n",
    "            ans_n = orig_ans.lower()\n",
    "\n",
    "    print(f\"LLM Picker 1 answer: {ans_n}... Should be none in the beginning\")\n",
    "\n",
    "    if ans_n.lower() != \"none\" and ans_n not in generated_labels_list:\n",
    "        generated_labels_list.append(ans_n)\n",
    "\n",
    "    print(f\"List of label so far: {generated_labels_list}\")\n",
    "\n",
    "    \"\"\"    \n",
    "    if ans_n != \"none\":\n",
    "        #prompt, ans_n = generate_label(session = session, link = link, old_prompt = prompt, lsd = lsd, var_params = {\"OPTIONS\": generated_labels_list}, generated_labels_list=generated_labels_list)\n",
    "        \n",
    "        #prompt = prompt.strip()\n",
    "        #ans_n = ans_n.strip()\n",
    "        \n",
    "        generated_labels_list.append(new_label)\n",
    "        \n",
    "        #print(f\"Generated label by LLM: {new_label}\")\n",
    "        #ans_n = new_label\n",
    "\n",
    "    print(f\"List of label so far: {generated_labels_list}\")\n",
    "    \"\"\"\n",
    "\n",
    "    #print(f\"final label set was {fixed_labels}, prediction was {ans_n}, ground truth was {ground_truth} \\n\")\n",
    "    if isd4:\n",
    "        res = ans_n in ground_truth\n",
    "    else:\n",
    "        res = ans_n == ground_truth\n",
    "    ans_dict = {\"response\": ans_n, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res,\n",
    "                \"original_model_answer\": orig_ans}\n",
    "\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "\n",
    "    #print(f\"Final answer: {ans_n}\")\n",
    "    return prompt, ans_n\n",
    "\n",
    "@retry(Exception, tries=5, delay=3)\n",
    "def get_bloomz_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None):\n",
    "    prompt = prompt_context_insert(context_labels, context, 2000, \"bloomz\")\n",
    "    if prompt_dict.get(prompt, -1) != -1:\n",
    "        return prompt\n",
    "    if response:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda:0\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=5)\n",
    "    else:\n",
    "        response = \"\"\n",
    "    ans = tokenizer.decode(outputs[0]).split()[-1]\n",
    "    ans = ''.join(e for e in ans if e.isalnum()).lower()\n",
    "    res = ans == ground_truth\n",
    "    ans_dict = {\"response\": ans, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res}\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85950645-b679-4139-baed-fcaa30e91aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_integer(val):\n",
    "    return pd.to_numeric(val, downcast='integer', errors='ignore')\n",
    "\n",
    "\n",
    "def derive_meta_features(col):\n",
    "    features = {}\n",
    "    if not col.astype(str).apply(str.isnumeric).all():\n",
    "        return {\"std\": round(col.astype(str).str.len().std(), 2), \"mean\": round(col.astype(str).str.len().mean(), 2),\n",
    "                \"mode\": col.astype(str).str.len().mode().iloc[0].item(), \"median\": col.astype(str).str.len().median(),\n",
    "                \"max\": col.astype(str).str.len().max(), \"min\": col.astype(str).str.len().min(),\n",
    "                \"rolling-mean-window-4\": [0.0]}\n",
    "    col = col.dropna().astype(float)\n",
    "    if col.apply(float.is_integer).all():\n",
    "        col = col.astype(int)\n",
    "    #print(f\"Collecting metafeatures for column {col} \\n\")\n",
    "    features['std'] = round(col.std(), 2)\n",
    "    features['mean'] = round(col.mean(), 2)\n",
    "    features['mode'] = col.mode().iloc[0].item()\n",
    "    features['median'] = col.median()\n",
    "    features['max'] = col.max()\n",
    "    features['min'] = col.min()\n",
    "    indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=4)\n",
    "    features['rolling-mean-window-4'] = list(col.rolling(window=indexer, min_periods=1).mean())\n",
    "    return features\n",
    "\n",
    "\n",
    "def fix_mode(d):\n",
    "    if isinstance(d['mode'], pd.Series):\n",
    "        d['mode'] = d['mode'].loc[0].item()\n",
    "    return d\n",
    "\n",
    "\n",
    "def split_meta_features(d):\n",
    "    return pd.Series(\n",
    "        [d.get('std', \"N/A\"), d.get('mean', \"N/A\"), d.get('median', \"N/A\"), d.get('mode', \"N/A\"), d.get('max', \"N/A\"),\n",
    "         d.get('min', \"N/A\")])\n",
    "\n",
    "\n",
    "def prompt_context_insert(context_labels: str, context: str, max_len: int = 2000, model: str = \"gpt-3.5\", options: list[str] = None):\n",
    "    if model == \"bloomz\":\n",
    "        s = f'SYSTEM: You are an AI research assistant. You use a tone that is technical and scientific. USER: Please select the field from {context_labels} which best describes the context below. Respond with the name of the field and nothing else. \\n CONTEXT: {context}'\n",
    "    elif model == \"gpt-3.5\":\n",
    "        s = f'SYSTEM: Please select the field from {context_labels} which best describes the context. Respond only with the name of the field. \\n CONTEXT: {context}'\n",
    "    elif model == \"ada-personal\":\n",
    "        s = f'{context}$'\n",
    "    elif model == \"llama-old\":\n",
    "        s = f'INSTRUCTION: Select the field from the category which matches the input. \\n CATEGORIES: {context_labels} \\n INPUT:{context} \\n OUTPUT: '\n",
    "    elif \"-zs\" in model:\n",
    "        ct = \"[\" + \", \".join(context).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")[\n",
    "                   :MAX_LEN - 100 - len(context_labels)] + \"]\"\n",
    "        lb = \"\\n\".join([\"- \" + c for c in context_labels.split(\", \")])\n",
    "        #s = f'How might one classify the following input? \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n ANSWER:'\n",
    "        if model == \"opt-iml-max-30b-zs\":\n",
    "            s = f'Select the option which best describes the input. \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n'\n",
    "        else:\n",
    "            # Original prompt\n",
    "            s = f'INSTRUCTION: Select the option which best describes the input. \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n ANSWER:'\n",
    "            \n",
    "    elif model == \"llama\":\n",
    "        ct = \"[\" + \", \".join(context).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")[\n",
    "                   :MAX_LEN - 100 - len(context_labels)] + \"]\"\n",
    "        lb = \"\\n\".join([\"- \" + c for c in context_labels.split(\", \")])\n",
    "        \n",
    "        #s = f'INSTRUCTION: Select the category which best matches the input. \\n INPUT:{context} .\\n OPTIONS:\\n{lb} \\n CATEGORY: '\n",
    "\n",
    "        #s = f'INSTRUCTION: Select the category which best matches the input. If category is not matching the input return none. Do not provide any further text, only label if it exists or none. \\n INPUT:{context} .\\n OPTIONS:\\n - none \\n CATEGORY: '\n",
    "        #opt_set = options + [\"none\"]\n",
    "        \n",
    "        options_str = \" - \".join(options)\n",
    "        \n",
    "        #if options is not None:\n",
    "        #    options_str = \" - \".join(options)\n",
    "        #else:\n",
    "        #    options_str = \"- none\"\n",
    "            \n",
    "        print(f\"Prompt context insert {options_str}\")\n",
    "\n",
    "        # Picker 1 prompt\n",
    "        s = picker_prompt = llm_prompts(\"picker\", ct, options_str)\n",
    "\n",
    "    elif model == \"llama-retry\":\n",
    "        s = f'INSTRUCTION: Select the category which best matches the input. \\n INPUT:{context} \\n CATEGORY: '\n",
    "    #Truncate if prompt exceeds maximum length\n",
    "    if len(s) > max_len:\n",
    "        s = s[:max_len - 3]\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def recompute_results(prompt_dict, prompt, model_str, cbc_pred, label_set):\n",
    "    dict_val = prompt_dict.get(prompt, -1)\n",
    "    dict_val['cbc_pred'] = cbc_pred\n",
    "    if model_str == \"llama\":\n",
    "        if cbc_pred and (cbc_pred in catboost_cats):\n",
    "            print(f\"using cbcpred label: {cbc_pred} \\n\")\n",
    "            dict_val['response'] = fix_labels(cbc_pred, label_set)\n",
    "        dict_val['correct'] = ((dict_val['ground_truth'] == dict_val['response']) or (\n",
    "                    dict_val['response'] and (dict_val['response']) in dict_val['ground_truth']))\n",
    "    prompt_dict[prompt] = dict_val\n",
    "\n",
    "\n",
    "def make_json(prompt, var_params):\n",
    "    p = deepcopy(params)\n",
    "    if var_params:\n",
    "        for k, v in var_params.items():\n",
    "            p[k] = v\n",
    "    return {\n",
    "        \"data\": [\n",
    "            prompt,\n",
    "            p['max_new_tokens'],\n",
    "            p['do_sample'],\n",
    "            p['temperature'],\n",
    "            p['top_p'],\n",
    "            p['typical_p'],\n",
    "            p['repetition_penalty'],\n",
    "            p['encoder_repetition_penalty'],\n",
    "            p['top_k'],\n",
    "            p['min_length'],\n",
    "            p['no_repeat_ngram_size'],\n",
    "            p['num_beams'],\n",
    "            p['penalty_alpha'],\n",
    "            p['length_penalty'],\n",
    "            p['early_stopping'],\n",
    "            p['seed'],\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def ans_contains_gt(ans_n, fixed_labels):\n",
    "    for fixed_label in fixed_labels:\n",
    "        if fixed_label in ans_n:\n",
    "            print(f\"Fuzzy label {ans_n} contains gt label {fixed_label}: MATCH \\n\")\n",
    "            ans_n = fixed_label\n",
    "            return ans_n\n",
    "    return None\n",
    "\n",
    "\n",
    "def gt_contains_ans(ans_n, fixed_labels):\n",
    "    if ans_n == \"\":\n",
    "        return None\n",
    "    for fixed_label in fixed_labels:\n",
    "        if ans_n in fixed_label:\n",
    "            print(f\"GT label {fixed_label} contains fuzzy label {ans_n}: MATCH \\n\")\n",
    "            ans_n = fixed_label\n",
    "            return ans_n\n",
    "    return None\n",
    "\n",
    "\n",
    "def basic_contains(ans_n, fixed_labels, method):\n",
    "    #TODO: not sure the order should be fixed like this, could be made flexible\n",
    "    if ans_n in fixed_labels:\n",
    "        return ans_n\n",
    "    if \"ans_contains_gt\" in method:\n",
    "        res = ans_contains_gt(ans_n, fixed_labels)\n",
    "        if res:\n",
    "            return res\n",
    "    if \"gt_contains_ans\" in method:\n",
    "        res = gt_contains_ans(ans_n, fixed_labels)\n",
    "        if res:\n",
    "            return res\n",
    "    return None\n",
    "\n",
    "\n",
    "def fuzzy_label_match(orig_ans, fixed_labels, session, link, prompt, lsd, model,\n",
    "                      method=[\"ans_contains_gt\", \"gt_contains_ans\", \"resample\"]):\n",
    "    #answer is already in label set, no fuzzy match needed\n",
    "    ans_n = fix_labels(orig_ans, lsd)\n",
    "    res = basic_contains(ans_n, fixed_labels, method)\n",
    "    if res:\n",
    "        return res\n",
    "    if \"similarity\" in method:\n",
    "        ans_embedding = sent_model.encode(ans_n)\n",
    "        lbl_embeddings = sent_model.encode(fixed_labels)\n",
    "        sims = {lbl: util.pytorch_cos_sim(ans_embedding, le) for lbl, le in zip(fixed_labels, lbl_embeddings)}\n",
    "        return max(sims, key=sims.get)\n",
    "    if \"resample\" in method:\n",
    "        #fuzzy label matching strategy\n",
    "        for k in range(2, 6):\n",
    "            if \"gpt\" in model:\n",
    "                ans_n = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    temperature=0 + k / 10,\n",
    "                ).choices[0]['message']['content'].lower()\n",
    "            elif model in [\"llama-zs\", \"opt-iml-30b-zs\"]:\n",
    "                pipe, local_llm, llm_chain = set_pipeline(k=k)\n",
    "                ans_n = llm_chain.run(prompt)\n",
    "            elif model in [\"topp-zs\", \"flan-ul2-zs\"]:\n",
    "                ans_n = get_topp_resp(prompt, k)\n",
    "            else:\n",
    "                rep_pen = params['repetition_penalty']\n",
    "                top_p = params['top_p']\n",
    "                temp = params['temperature']\n",
    "                ans_n = call_llama_model(session, link, prompt, lsd,\n",
    "                                         {'no_repeat_ngram_size': 1, 'top_p': top_p - (0.1 * k), 'temperature': 0.9})\n",
    "                params['top_p'] = top_p\n",
    "                params['temperature'] = temp\n",
    "            res = basic_contains(ans_n, fixed_labels, method)\n",
    "            if res:\n",
    "                return res\n",
    "    #print(\"Applying fallback label, 'text' \\n\")\n",
    "    return 'text'\n",
    "\n",
    "\n",
    "INTEGER_SET = set(r\"0123456789,/\\+-.^_()[] :\")\n",
    "\n",
    "\n",
    "def get_base_dtype(context):\n",
    "    dtype = \"integer\"\n",
    "    for item in context:\n",
    "        if not all(char in INTEGER_SET for char in item):\n",
    "            #print(f\"String is OTHER because: {[char for char in item if char not in INTEGER_SET]}\")\n",
    "            return \"other\"\n",
    "        try:\n",
    "            if item.endswith(\".0\") or item.endswith(\",0\"):\n",
    "                item = item[:-2]\n",
    "                item = str(int(item))\n",
    "            if item.endswith(\".00\") or item.endswith(\",00\"):\n",
    "                item = item[:-3]\n",
    "                item = str(int(item))\n",
    "        except:\n",
    "            return \"float\"\n",
    "        temp_item = re.sub(r\"[^a-zA-Z0-9.]\", \"\", item)\n",
    "        if not temp_item.isdigit():\n",
    "            #print(f\"string is FLOAT because {temp_item} is not an integer\")\n",
    "            dtype = \"float\"\n",
    "    return dtype\n",
    "\n",
    "\n",
    "def query_correct_model(model, prompt, context_labels, context, session, link, lsd):\n",
    "    if model in [\"llama-zs\", \"opt-iml-max-30b-zs\"]:\n",
    "        orig_ans = llm_chain.run(prompt)\n",
    "        if orig_ans is None:\n",
    "            prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"llama-retry\")\n",
    "            orig_ans = llm_chain.run(prompt)\n",
    "    elif model in [\"topp-zs\", \"flan-ul2-zs\"]:\n",
    "        orig_ans = get_topp_resp(prompt, 1)\n",
    "    else:\n",
    "        orig_ans = call_llama_model(session, link, prompt, lsd, None)\n",
    "        if orig_ans is None:\n",
    "            prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"llama-retry\")\n",
    "            orig_ans = call_llama_model(session, link, prompt, lsd, None)\n",
    "    return orig_ans\n",
    "\n",
    "\n",
    "def get_df_sample_col(col, rand_seed, len_context, min_variance=2, replace=False):\n",
    "    df = pd.Series(col)\n",
    "    ignore_list = [\"None\", 'none', 'NaN', 'nan', 'N/A', 'na', '']\n",
    "    sample_list = list(set(p[:75] for p in pd.unique(df.astype(str)[col]) if p not in ignore_list))\n",
    "    if len(sample_list) < 1:\n",
    "        return [\"None\"] * len_context\n",
    "    if len(sample_list) < len_context:\n",
    "        sample_list = sample_list * len_context\n",
    "    if len(sample_list) > len_context:\n",
    "        sample_list = sample_list[:len_context]\n",
    "    assert len(sample_list) == len_context, f\"An index in val_indices is length {len(sample_list)}\"\n",
    "    return sample_list\n",
    "\n",
    "\n",
    "def check_substr_contains_only_set(str, acceptable_chars):\n",
    "    validation = set(str)\n",
    "    print(\"Checking if it contains only \", acceptable_chars)\n",
    "    if validation.issubset(acceptable_chars):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def insert_source(context, fname):\n",
    "    pattern = r\"_([^_]*)_\"  # Matches substrings that start and end with \"_\"\n",
    "    matcher = re.search(pattern, fname)\n",
    "    addstr = str(matcher.group()).replace(\"_\", \"\").split(\".\")[0]\n",
    "    #context.insert(0, \"SRC_FILE: \" + addstr + \"COL_VALS: \")\n",
    "    context.insert(0, \"SRC: \" + addstr)\n",
    "    return context\n",
    "\n",
    "\n",
    "def get_df_sample(df, rand_seed, val_indices, len_context, min_variance=1, replace=False, full=False, other_col=False,\n",
    "                  max_len=8000):\n",
    "    column_samples = {}\n",
    "    ignore_list = [\"None\", 'none', 'NaN', 'nan', 'N/A', 'na', '']\n",
    "    for col in df.columns:\n",
    "        sample_list = list(\n",
    "            set(p[:max_len // (len_context * 3)] for p in pd.unique(df.astype(str)[col]) if p not in ignore_list))\n",
    "        #reformat integer samples\n",
    "        sl_mod = []\n",
    "        # Meta-features\n",
    "        if full:\n",
    "            meta_features = derive_meta_features(df[col])\n",
    "            meta_features['rolling-mean-window-4'] = meta_features['rolling-mean-window-4'][:5]\n",
    "        # Sampling from other columns\n",
    "        if other_col:\n",
    "            sample_list_fill_size = len_context - len(sample_list)\n",
    "            nc = len(df.columns)\n",
    "            per_column_context = max(1, sample_list_fill_size // nc)\n",
    "            for idx, oc in enumerate(df.columns):\n",
    "                items = df[oc].astype(str).iloc[0:per_column_context].tolist()\n",
    "                sample_list = sample_list + [\"OC: \" + str(item) for item in items]\n",
    "        if not sample_list:\n",
    "            sample_list = [\"None\"]\n",
    "        if len(sample_list) < len_context:\n",
    "            sample_list = sample_list * len_context\n",
    "        if len(sample_list) > len_context:\n",
    "            sample_list = sample_list[:len_context]\n",
    "        assert len(sample_list) == len_context, \"An index in val_indices is length \" + str(len(sample_list))\n",
    "        if full:\n",
    "            if meta_features['std'] == \"N/A\":\n",
    "                sample_list = sample_list + [\"\" for k, v in meta_features.items()]\n",
    "            else:\n",
    "                sample_list = sample_list + [str(k) + \": \" + str(v) for k, v in meta_features.items()]\n",
    "        # print(\"sample list\")\n",
    "        # print(sample_list)\n",
    "        column_samples[col] = sample_list\n",
    "        # print(\"column samples\")\n",
    "        # print(column_samples)\n",
    "    return pd.DataFrame.from_dict(column_samples)\n",
    "\n",
    "\n",
    "NUMERIC_AND_COMMA = set('0123456789,')\n",
    "\n",
    "BOOLEAN_SET = [\"True\", \"true\", \"False\", \"false\", \"yes\", \"Yes\", \"No\", \"no\"]\n",
    "\n",
    "\n",
    "def apply_basic_rules(context, lbl):\n",
    "    if not context:\n",
    "        return lbl\n",
    "    if not isinstance(context, list):\n",
    "        return lbl\n",
    "    try:\n",
    "        if all(s.endswith(\" g\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" kg\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" lb\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" lbs\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" pounds\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" cal\") for s in context):\n",
    "            lbl = \"calories\"\n",
    "        if all(s.endswith(\" kcal\") for s in context):\n",
    "            lbl = \"calories\"\n",
    "        if all(s.endswith(\" calories\") for s in context):\n",
    "            lbl = \"calories\"\n",
    "        if all(\"review\" in s.lower() for s in context):\n",
    "            lbl = \"review\"\n",
    "        if all(\"recipe\" in s.lower() for s in context):\n",
    "            lbl = \"recipe\"\n",
    "        if lbl and \"openopen\" in lbl:\n",
    "            lbl = \"openinghours\"\n",
    "        if all(s in BOOLEAN_SET for s in context):\n",
    "            lbl = \"medical_boolean\"\n",
    "        return lbl\n",
    "    except Exception as e:\n",
    "        print(f\"Exception {e} in apply_basic_rules with context {context}\")\n",
    "        return lbl\n",
    "\n",
    "\n",
    "def get_cbc_pred(orig_label, numeric_labels):\n",
    "    try:\n",
    "        #FOR VALIDATION\n",
    "        #cbc_filematch = dfv[dfv['df_path'] == str(f)]\n",
    "        #FOR TEST SET\n",
    "        cbc_filematch = dft[dft['df_path'] == str(f)]\n",
    "        cbc_labelmatch = cbc_filematch[cbc_filematch['label'] == orig_label]\n",
    "        if len(cbc_labelmatch) == 1:\n",
    "            cbc_pred = numeric_labels[cbc_labelmatch['preds'].item()]\n",
    "        else:\n",
    "            cbc_pred = None\n",
    "    except Exception as e:\n",
    "        print(\"cbc excpetion: \")\n",
    "        print(e)\n",
    "        cbc_pred = None\n",
    "\n",
    "\n",
    "def run_val(model: str, save_path: str, inputs: list, label_set: list, input_df: pd.DataFrame, resume: bool = True,\n",
    "            results: bool = True, stop_early: int = -1, rand_seed: int = 13, sample_size: int = 5, link: str = None,\n",
    "            response: bool = True, summ_stats: bool = False, table_src: bool = False, other_col: bool = False,\n",
    "            skip_short: bool = False, min_var: int = 0, method: list = [\"similarity\"]):\n",
    "    inputs = [Path(f) for f in inputs]\n",
    "\n",
    "    infmods = \"sherlock\" in model or \"doduo\" in model\n",
    "    isd4 = \"d4\" in label_set['name']\n",
    "    if resume and os.path.isfile(save_path):\n",
    "        with open(save_path, 'r', encoding='utf-8') as f:\n",
    "            prompt_dict = json.load(f)\n",
    "    else:\n",
    "        prompt_dict = {}\n",
    "    s = requests.Session()\n",
    "    if \"-zs\" in model:\n",
    "        base_model.eval()\n",
    "    if isinstance(inputs, dict):\n",
    "        labels = [\"_\".join(k.split(\"_\")[:-1]) for k in inputs.keys()]\n",
    "        inputs = list(inputs.values())\n",
    "    for idx, f in tqdm(enumerate(inputs), total=len(inputs)):\n",
    "        if idx % 100 == 0:\n",
    "            with open(save_path, 'w', encoding='utf-8') as alt_f:\n",
    "                #print(\"pd\", prompt_dict, \"\\n\")\n",
    "                json.dump(prompt_dict, alt_f, ensure_ascii=False, indent=4)\n",
    "        if stop_early > -1 and idx == stop_early:\n",
    "            break\n",
    "        if isd4:\n",
    "            f_df = f\n",
    "            label_indices = [2]\n",
    "            gt_labels = labels[idx]\n",
    "        else:\n",
    "            gt_labels = input_df[input_df['table_name'] == f.name]\n",
    "            label_indices = gt_labels['column_index'].unique().tolist()\n",
    "\n",
    "            if f.suffix.lower() == '.csv':\n",
    "                f_df = pd.read_csv(f)\n",
    "            else:\n",
    "                f_df = pd.read_json(f, compression='infer', lines=True)\n",
    "\n",
    "        if infmods:\n",
    "            label_indices = [\"values\"]\n",
    "            key = get_sherlock_resp(f_df, gt_labels, prompt_dict, model, label_indices, str(f), label_set)\n",
    "            continue\n",
    "        sample_df = get_df_sample(f_df, rand_seed, label_indices, sample_size, full=summ_stats, other_col=other_col,\n",
    "                                  max_len=MAX_LEN)\n",
    "        #print(f\"in main loop, sample_df is {sample_df}\")\n",
    "        f_df_cols = f_df.columns\n",
    "        for idx, col in enumerate(f_df_cols):\n",
    "            if idx not in label_indices:\n",
    "                continue\n",
    "            #NOTE: skipping evaluation for columns with insufficient variance in the column\n",
    "            #       if len(pd.unique(sample_df.astype(str)[col])) < min_var:\n",
    "            #         continue\n",
    "            if isd4:\n",
    "                orig_label = gt_labels\n",
    "            else:\n",
    "                gt_row = gt_labels[gt_labels['column_index'] == idx]\n",
    "                orig_label = gt_row['label'].item()\n",
    "            label = fix_labels(orig_label, label_set)\n",
    "            limited_context = sample_df[col].tolist()[:sample_size]\n",
    "            #NOTE: could consider using min_var here\n",
    "            #if full and len(pd.unique(sample_df[col].tolist())) < 3:\n",
    "            if table_src:\n",
    "                context = insert_source(sample_df[col].tolist(), f.name)\n",
    "            else:\n",
    "                context = sample_df[col].tolist()\n",
    "            if \"gpt-3.5\" in model:\n",
    "                key = get_chatgpt_resp(label_set, context, label, prompt_dict, response=response, session=s,\n",
    "                                       method=method)\n",
    "            elif \"ada-personal\" in model:\n",
    "                key = get_ada_resp(label_set, context, label, prompt_dict, response=response, session=s)\n",
    "            elif \"bloomz\" in model:\n",
    "                key = get_bloomz_resp(label_set, context, label, prompt_dict, response=response, session=s)\n",
    "            elif \"llama\" in model or \"-zs\" in model:\n",
    "                #cbc_pred = get_cbc_pred(orig_label, numeric_labels)\n",
    "                cbc_pred = None\n",
    "                key = get_llama_resp(label_set, context, label, prompt_dict, link=link, response=response, session=s,\n",
    "                                     cbc=cbc_pred, model=model, limited_context=limited_context, method=method)\n",
    "                # print(\"Key: \", key, \"\\n\")\n",
    "                #print(\"pdk\", prompt_dict[key], \"\\n\")\n",
    "            prompt_dict[key]['original_label'] = orig_label\n",
    "            prompt_dict[key]['file+idx'] = str(f) + \"_\" + str(idx)\n",
    "    with open(save_path, 'w', encoding='utf-8') as my_f:\n",
    "        json.dump(prompt_dict, my_f, ensure_ascii=False, indent=4)\n",
    "    if results:\n",
    "        results_checker(save_path, skip_duplicates=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "392f663e-4f8f-4671-ab88-21575e2fffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02993b81-f6b9-4b7b-b14f-a0ba901faef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from statistics import mean\n",
    "\n",
    "ENDINGS = [\"ANSWER:\", \"CATEGORY:\"]\n",
    "\n",
    "\n",
    "def results_checker_doduo(file_name, skip_duplicates=True):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "    correct = 0\n",
    "    n = len(d)\n",
    "    per_class_results = dict()\n",
    "    for k, v in d.items():\n",
    "        response_set = set(v[\"response\"])\n",
    "        for r in response_set:\n",
    "            per_class_results.setdefault(r, {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"Total\": 0})\n",
    "        per_class_results.setdefault(v[\"ground_truth\"], {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"Total\": 0})\n",
    "        if v['correct'] == True:\n",
    "            correct += 1\n",
    "            per_class_results[v[\"ground_truth\"]][\"TP\"] += 1\n",
    "        else:\n",
    "            per_class_results[v[\"ground_truth\"]][\"FN\"] += 1\n",
    "            for r in response_set:\n",
    "                per_class_results[r][\"FP\"] += 1\n",
    "        per_class_results[v[\"ground_truth\"]][\"Total\"] += 1\n",
    "\n",
    "    for k, v in per_class_results.items():\n",
    "        v['F1'] = (2 * v[\"TP\"]) / (2 * v[\"TP\"] + v[\"FP\"] + v[\"FN\"])\n",
    "\n",
    "    weighted_f1 = sum([v[\"F1\"] * v[\"Total\"] for k, v in per_class_results.items()]) / n\n",
    "    unweighted_f1 = mean([v[\"F1\"] for k, v in per_class_results.items()])\n",
    "\n",
    "    print(\n",
    "        f\"Total entries: {n} \\n Accuracy: {round(correct / n, 4)} \\n Weighted F1: {round(weighted_f1, 4)} \\n Unweighted F1: {round(unweighted_f1, 4)}\")\n",
    "\n",
    "\n",
    "def results_checker(file_name, skip_duplicates=True):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    if skip_duplicates:\n",
    "        d = {k: v for k, v in d.items() if \"CATEGORY: *\" not in str(k)}\n",
    "\n",
    "    # build the lists\n",
    "    y_true = [v[\"ground_truth\"] for v in d.values()]\n",
    "    y_pred = [v[\"response\"] for v in d.values()]\n",
    "\n",
    "    # overall stats\n",
    "    correct = sum(1 for gt, pred in zip(y_true, y_pred) if gt == pred)\n",
    "    n = len(y_true)\n",
    "    print(f\"Total entries: {n}\")\n",
    "    print(f\"Accuracy:     {correct / n:.4f}\\n\")\n",
    "\n",
    "    # per-class report\n",
    "    print(classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        digits=4,  # 4 decimal places\n",
    "        zero_division=0  # to avoid warnings if a class is never predicted\n",
    "    ))\n",
    "\n",
    "    # --- new: build a flattened metrics dict ---\n",
    "    raw_report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    flat = {}\n",
    "    # raw_report has keys for each label, plus 'macro avg', 'weighted avg', and 'accuracy'\n",
    "    for label, m in raw_report.items():\n",
    "        if label == \"accuracy\":\n",
    "            flat[\"accuracy\"] = m\n",
    "        else:\n",
    "            for metric_name, val in m.items():\n",
    "                flat[f\"{label}_{metric_name}\"] = val\n",
    "\n",
    "    # add summary fields\n",
    "    flat[\"total_entries\"] = n\n",
    "    # filename identifier: take it from your JSON filename variable\n",
    "    flat[\"run_name\"]      = os.path.basename(file_name).replace(\".json\",\"\")\n",
    "\n",
    "    # convert to one-row DataFrame\n",
    "    df = pd.DataFrame([flat])\n",
    "\n",
    "    metrics_csv = \"./all_metrics.csv\"\n",
    "\n",
    "    # append (or create) the master CSV\n",
    "    if not os.path.isfile(metrics_csv):\n",
    "        df.to_csv(metrics_csv, index=False, float_format=\"%.4f\")\n",
    "    else:\n",
    "        df.to_csv(metrics_csv, mode=\"a\", header=False, index=False, float_format=\"%.4f\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8db2960f-ac1d-4199-9cbc-85b6d60f66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_entries(f1, f2):\n",
    "    with open(f1, \"r\") as file1:\n",
    "        d1 = json.load(file1)\n",
    "    with open(f2, \"r\") as file2:\n",
    "        d2 = json.load(file2)\n",
    "    paths1 = set([v[\"file+idx\"] for _, v in d1.items()])\n",
    "    paths2 = set([v[\"file+idx\"] for _, v in d2.items()])\n",
    "    return paths1 - paths2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23213247-0e7a-4b24-9eb5-a623264183ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_val_parquet(\n",
    "        model: str,\n",
    "        save_path: str,\n",
    "        labels_path: str,\n",
    "        data_path: str,\n",
    "        label_set: dict,\n",
    "        resume: bool = True,\n",
    "        results: bool = True,\n",
    "        stop_early: int = -1,\n",
    "        rand_seed: int = 13,\n",
    "        sample_size: int = 5,\n",
    "        link: str = None,\n",
    "        response: bool = True,\n",
    "        summ_stats: bool = False,\n",
    "        table_src: bool = False,\n",
    "        other_col: bool = False,\n",
    "        skip_short: bool = False,\n",
    "        min_var: int = 0,\n",
    "        method: list = [\"similarity\"],\n",
    "        results_checker=None,\n",
    "        MAX_LEN: int = 1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Validation loop adapted for parquet-based inputs:\n",
    "\n",
    "    - labels_path: path to a parquet file with columns ['__index_level_0__', 'type']\n",
    "    - data_path:   path to a parquet file with columns ['__index_level_0__', 'values']\n",
    "    - label_set:   dict containing 'name', 'label_set', 'dict_map', 'abbrev_map'\n",
    "\n",
    "    Each row in the merged DataFrame represents one column to predict:\n",
    "      - __index_level_0__ (column index)\n",
    "      - type (ground truth label)\n",
    "      - values (comma-separated or list of column values)\n",
    "    \"\"\"\n",
    "\n",
    "    generated_labels_list = []\n",
    "        \n",
    "    \n",
    "    # Load or initialize cache\n",
    "    if resume and os.path.isfile(save_path):\n",
    "        with open(save_path, 'r', encoding='utf-8') as f:\n",
    "            prompt_dict = json.load(f)\n",
    "    else:\n",
    "        prompt_dict = {}\n",
    "    \n",
    "\n",
    "    # Read parquet inputs and bring index into a column\n",
    "    labels_df = pd.read_parquet(labels_path).reset_index()\n",
    "    data_df = pd.read_parquet(data_path).reset_index()\n",
    "\n",
    "    # Identify the index column name (either __index_level_0__ or generic index)\n",
    "    labels_idx_col = '__index_level_0__' if '__index_level_0__' in labels_df.columns else 'index'\n",
    "    data_idx_col = '__index_level_0__' if '__index_level_0__' in data_df.columns else 'index'\n",
    "\n",
    "    # Rename for clarity: index → col_idx, type → label, values stays values\n",
    "    labels_df = labels_df.rename(columns={labels_idx_col: 'col_idx', 'type': 'label'})\n",
    "    data_df = data_df.rename(columns={data_idx_col: 'col_idx', 'values': 'values'})\n",
    "\n",
    "    # Remap labels using LABEL_MAP_LC\n",
    "    # assumes remap_labels(series, mapping) is defined and LABEL_MAP_LC is available\n",
    "\n",
    "    #labels_df['label'] = remap_labels(labels_df['label'], LABEL_MAP_LC)\n",
    "\n",
    "    # Filter out __none__ labels\n",
    "    labels_df = labels_df[labels_df['label'] != \"__none__\"]\n",
    "\n",
    "    # Merge on column index\n",
    "    merged = pd.merge(labels_df, data_df, on='col_idx', how='inner')\n",
    "\n",
    "    # Prepare session and model\n",
    "    session = requests.Session()\n",
    "    if \"-zs\" in model:\n",
    "        base_model.eval()\n",
    "\n",
    "    # Iterate over each column instance\n",
    "    for idx, row in tqdm(enumerate(merged.itertuples(index=False)), total=len(merged)):\n",
    "        \n",
    "        # Periodic cache save\n",
    "        if idx % 100 == 0:\n",
    "            with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(prompt_dict, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        if stop_early > -1 and idx == stop_early:\n",
    "            break\n",
    "\n",
    "        col_idx = row.col_idx\n",
    "        orig_label = row.label\n",
    "        raw_vals = row.values\n",
    "\n",
    "        # Parse raw values into a list\n",
    "        if isinstance(raw_vals, str):\n",
    "            vals = raw_vals.split(',')\n",
    "        else:\n",
    "            vals = list(raw_vals)\n",
    "\n",
    "        # Deduplicate and sample\n",
    "        vals = [str(x) for x in vals]\n",
    "        unique_vals = pd.unique(vals)\n",
    "        context_list = unique_vals.tolist()[:sample_size]\n",
    "\n",
    "        # Build context\n",
    "        if table_src:\n",
    "            context = insert_source(context_list, str(col_idx))\n",
    "        else:\n",
    "            context = context_list\n",
    "\n",
    "        # Model call\n",
    "        if \"gpt-3.5\" in model:\n",
    "            key = get_chatgpt_resp(label_set, context, orig_label,\n",
    "                                   prompt_dict, response=response,\n",
    "                                   session=session, method=method)\n",
    "        elif \"ada-personal\" in model:\n",
    "            key = get_ada_resp(label_set, context, orig_label,\n",
    "                               prompt_dict, response=response,\n",
    "                               session=session)\n",
    "        elif \"bloomz\" in model:\n",
    "            key = get_bloomz_resp(label_set, context, orig_label,\n",
    "                                  prompt_dict, response=response,\n",
    "                                  session=session)\n",
    "        else:\n",
    "            raw_prompt, answer = get_llama_resp(label_set, context, orig_label,\n",
    "                                 prompt_dict, link=link,\n",
    "                                 response=response,\n",
    "                                 session=session,\n",
    "                                 cbc=None,\n",
    "                                 model=model,\n",
    "                                 limited_context=context_list,\n",
    "                                 method=method,\n",
    "                                 generated_labels_list = generated_labels_list)\n",
    "            \n",
    "        # TODO: Problem is here I'm never actualling calling main functions of generating labels and etc.\n",
    "        # I need to try call it from here since it's not calling only get_llama_resp and not genrate_labels()\n",
    "        #print(f\"LLM Returned: {key}\")\n",
    "\n",
    "        # Record metadata\n",
    "        prompt_dict[raw_prompt]['original_label'] = orig_label\n",
    "        prompt_dict[raw_prompt]['file+idx'] = str(col_idx)\n",
    "\n",
    "        # but you also get to see the actual label answer:\n",
    "        print(\"PROMPT SENT:\\n\", raw_prompt)\n",
    "        print(\"MODEL ANSWER:\", answer)\n",
    "\n",
    "    # Final cache save\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(prompt_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Optional result summary\n",
    "    #if results and results_checker is not None:\n",
    "    #    results_checker(save_path, skip_duplicates=False)\n",
    "\n",
    "    return prompt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbb07a01-53a9-4639-b197-fb24a0479456",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = {\n",
    "  \"name\": \"custom_csv\",     # any string that does NOT contain \"d4\"\n",
    "  \"label_set\": LABELS,      # the list of your labels, used by similarity\n",
    "  \"dict_map\": { lab: lab for lab in LABELS },\n",
    "  \"abbrev_map\": {}          # or your real abbrev map\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cd83aaa-8825-403e-9f26-b62e8fd0765e",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a2a5700b784d6f930a325cb8ec2ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3126961/836422734.py:98: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unique_vals = pd.unique(vals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt context insert \n",
      "LLM Picker 1 answer: patient_id... Should be none in the beginning\n",
      "List of label so far: ['patient_id']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [0C101DE5RTQ2, E15R0T0CQ52D, RC18E0T03Q2D, 03QRTED050C0, 02RE0QT1C4D1]\n",
      "                    OPTIONS: \n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: patient_id\n",
      "Prompt context insert patient_id\n",
      "LLM Picker 1 answer: response_status... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [inconnu, Non, nON, oui, NON]\n",
      "                    OPTIONS: patient_id\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: response_status\n",
      "Prompt context insert patient_id - response_status\n",
      "LLM Picker 1 answer: patient_recovery_status... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [RECOVERED]\n",
      "                    OPTIONS: patient_id - response_status\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: patient_recovery_status\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [No]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean\n",
      "LLM Picker 1 answer: date_range... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [2020-06-11, 2020-07-28, 2020-07-22, 2020-07-08, 2020-07-04]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: date_range\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "LLM Picker 1 answer: date_range... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [2020-06-12, 2020-07-27, 2020-07-07, 2020-07-09, 2020-08-04]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: date_range\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "LLM Picker 1 answer: date_range... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [2020-09-08, 2020-07-06, 2020-09-23]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: date_range\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "LLM Picker 1 answer: date_range... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [2020-07-12, 2020-06-28, 2020-06-21, 2020-06-04, 2020-06-03]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: date_range\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [NO]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "LLM Picker 1 answer: date_range... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [2024-10-13, 2025-03-02, 2025-02-09, 2022-10-23, 2022-06-26]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: date_range\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Yes, No]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "LLM Picker 1 answer: response_status... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [non cas, Confirmé, Non cas, Encours]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: response_status\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "LLM Picker 1 answer: gender_type... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [F, M]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: gender_type\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Oui, Non]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "LLM Picker 1 answer: date_range... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [2020-06-06, 2020-06-20, 2020-06-05, 2020-07-14, 2020-06-08]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: date_range\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "LLM Picker 1 answer: response_status... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [NO, YES]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: response_status\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "LLM Picker 1 answer: patient_id... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [690KB5R0, 3R064BK0, RB50K040, 3K0003RB, 6KR690B0]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: patient_id\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [55, 6, 70, 8, 34]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "LLM Picker 1 answer: date_range... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [2020-06-04]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: date_range\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [YES, N]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "LLM Picker 1 answer: response_status... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Non, Oui]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: response_status\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "LLM Picker 1 answer: country_list... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Monaco, Georgia, Finland, Cuba, Colombia]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: country_list\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [3.0, 44.0, 50.0, 1.0, 47.0]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Oui, Non]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [-BRQ6-7TM7ECD300O21, BRQ2ET6-21DO-0831CM0, QM0-2ET1-30462BCR9OD, D--3B23EMO620C1TR0Q4, 0R26B0-EDT137OM-CQ2]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: response_status... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Oui, Non, Inconnu]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: response_status\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: country_list... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Japan, Chile, United Kingdom, Serbia, Peru]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: country_list\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: date_range... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [2020-06-12, 2020-03-21, 2020-06-24, 2020-03-29, 2020-08-03]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: date_range\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: gender_type... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [m, F, M]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: gender_type\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: country_list... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Kansas, Massachusetts, Missouri, New York, Florida]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: country_list\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: date_range... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [2022-05-21, 2022-01-30, 2022-05-18, 2022-02-26, 2022-03-28]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: date_range\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Inconnu, oui, Oui, Non]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Inc, Non, Oui]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: response_status... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [YES, NO]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: response_status\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [19, 55, 29, 35, 42]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: country_list... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Jamaica, United Kingdom, Kazakhstan, Venezuela, Malaysia]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: country_list\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [T40Q0ED400CR, E0Q00600CRTD, R0DT0C9Q05E2, 4R0CQE9DT009, 0D61ER400QTC]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: date_range... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [2022-01-23, 2022-01-31, 2022-02-08, 2022-01-15, 2022-05-14]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: date_range\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: response_status... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Oui, Non]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: response_status\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: date_range... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [2020-07-30, 2020-07-06, 2020-09-08, 2020-07-18, 2020-08-09]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: date_range\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [NO]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: country_list... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Carnot, Bossembélé, Bangui, Nola, Gamboula]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: country_list\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [1.0, 0.0]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: date_range... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [2020-08-19, 2020-06-15, 2020-09-11, 2020-09-06, 2020-06-13]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: date_range\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "LLM Picker 1 answer: medical_boolean \n",
      "or \n",
      "patient_recovery_status... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids', 'medical_boolean \\nor \\npatient_recovery_status']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Outpatient, Inpatient, Discharged, Transferred]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean \n",
      "or \n",
      "patient_recovery_status\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status\n",
      "LLM Picker 1 answer: patient_recovery_status... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids', 'medical_boolean \\nor \\npatient_recovery_status']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [gueri, decede]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: patient_recovery_status\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids', 'medical_boolean \\nor \\npatient_recovery_status']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [bik094, mbk228, bik074, bik001, mbk461]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids', 'medical_boolean \\nor \\npatient_recovery_status']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [NOTVACC, DOSEUNK, 2DOSE]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status\n",
      "LLM Picker 1 answer: country_list... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids', 'medical_boolean \\nor \\npatient_recovery_status']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Vakaga, Bangui, Sangha-Mbaéré, Haut-Mbomou, Ouham]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: country_list\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status\n",
      "LLM Picker 1 answer: response_status... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids', 'medical_boolean \\nor \\npatient_recovery_status']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Confirmé]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: response_status\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status\n",
      "LLM Picker 1 answer: diagnosis_details... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids', 'medical_boolean \\nor \\npatient_recovery_status', 'diagnosis_details']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Profuse watery diarrhea|, Rice-water stools|, Severe dehydration|Vomiting|Profuse watery diarrhea|, Profuse watery diarrhea|Severe dehydration|, Profuse watery diarrhea|Vomiting|Severe dehydration|]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: diagnosis_details\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status - diagnosis_details\n",
      "LLM Picker 1 answer: role_types... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids', 'medical_boolean \\nor \\npatient_recovery_status', 'diagnosis_details', 'role_types']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Pensioner, Scholar, Grant Administrator]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status - diagnosis_details\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: role_types\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status - diagnosis_details - role_types\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids', 'medical_boolean \\nor \\npatient_recovery_status', 'diagnosis_details', 'role_types']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Non, Inc, Oui]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status - diagnosis_details - role_types\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status - diagnosis_details - role_types\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids', 'medical_boolean \\nor \\npatient_recovery_status', 'diagnosis_details', 'role_types']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [NO, YES]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status - diagnosis_details - role_types\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status - diagnosis_details - role_types\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_id', 'response_status', 'patient_recovery_status', 'medical_boolean', 'date_range', 'gender_type', 'country_list', 'patient_ids', 'medical_boolean \\nor \\npatient_recovery_status', 'diagnosis_details', 'role_types']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [No]\n",
      "                    OPTIONS: patient_id - response_status - patient_recovery_status - medical_boolean - date_range - gender_type - country_list - patient_ids - medical_boolean \n",
      "or \n",
      "patient_recovery_status - diagnosis_details - role_types\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: medical_boolean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nrun_val_parquet(\\n    model=\"topp-zs\",\\n    save_path=sp,\\n    labels_path= labels_path,\\n    data_path= data_path,\\n    label_set=label_set,\\n    method=[\"similarity\"],\\n    resume=True,\\n    sample_size=5\\n)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_name = \"alpaca-fine-tuned\"\n",
    "#model_name = \"flan-ul2\"\n",
    "#model_name = \"alpaca-13b\"\n",
    "\n",
    "model_name=\"llama\"\n",
    "\n",
    "filename = f\"custom-data-{model_name}-{tune}.json\"\n",
    "\n",
    "sp = f\"./custom_data_logs/{filename}\"\n",
    "\n",
    "\n",
    "dirpath = os.path.dirname(sp)\n",
    "os.makedirs(dirpath, exist_ok=True)\n",
    "\n",
    "#model_name = \"flan-t5-xxl\"\n",
    "\n",
    "#base_model, tokenizer, template, pt, MAX_LEN = init_model(model_name)\n",
    "\n",
    "# Test set\n",
    "labels_path = \"./custom_data/test_labels.parquet\"\n",
    "data_path = \"./custom_data/test_data.parquet\"\n",
    "\n",
    "# SMOKE TEST: Train set\n",
    "#labels_path = \"./custom_data/train_labels.parquet\"\n",
    "#data_path = \"./custom_data/train_data.parquet\"\n",
    "\"\"\"\n",
    "params = {\n",
    "    'max_new_tokens': 6,\n",
    "    'do_sample': True,\n",
    "    'temperature': 0.2,\n",
    "    'top_p': 0.8,\n",
    "    'typical_p': 1,\n",
    "    'repetition_penalty': 1.3,\n",
    "    'encoder_repetition_penalty': 1.0,\n",
    "    'top_k': 0,\n",
    "    'min_length': 3,\n",
    "    'no_repeat_ngram_size': 3,\n",
    "    'num_beams': 1,\n",
    "    'penalty_alpha': 0,\n",
    "    'length_penalty': 1,\n",
    "    'early_stopping': False,\n",
    "    'seed': 42,\n",
    "}\n",
    "\"\"\"\n",
    "# LLAMA\n",
    "run_val_parquet(\n",
    "    model=\"llama\",\n",
    "    save_path=sp,\n",
    "    labels_path=labels_path,\n",
    "    data_path=data_path,\n",
    "    label_set=label_set,\n",
    "    method=[\"similarity\"],\n",
    "    resume=False,\n",
    "    sample_size=5,\n",
    "    #stop_early = 30,\n",
    "    link = \"http://localhost:11434/api/generate\"\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "run_val_parquet(\n",
    "    model=\"topp-zs\",\n",
    "    save_path=sp,\n",
    "    labels_path=labels_path,\n",
    "    data_path=data_path,\n",
    "    label_set=label_set,\n",
    "    method=[\"similarity\"],\n",
    "    resume=False,\n",
    "    sample_size=5\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# FLAN-UL2\n",
    "\"\"\"\n",
    "run_val_parquet(\n",
    "    model=\"flan-ul2-zs\",\n",
    "    save_path=sp,\n",
    "    labels_path= labels_path,\n",
    "    data_path= data_path,\n",
    "    label_set=label_set,\n",
    "    method=[\"similarity\"],\n",
    "    resume=True,\n",
    "    sample_size=5\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "## FLAN-t5\n",
    "\"\"\"\n",
    "run_val_parquet(\n",
    "    model=\"topp-zs\",\n",
    "    save_path=sp,\n",
    "    labels_path= labels_path,\n",
    "    data_path= data_path,\n",
    "    label_set=label_set,\n",
    "    method=[\"similarity\"],\n",
    "    resume=True,\n",
    "    sample_size=5\n",
    ")\n",
    "\"\"\"\n",
    "#run_val(model=\"topp-zs\", save_path=sp, inputs_folder=\"./WHO\", input_df=cta_gt, label_set=label_set, method=[\"similarity\"], resume=True, sample_size = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe4311a83efa0299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File exists: ./custom_data_logs/custom-data-llama-0.0.json\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(sp):\n",
    "    print(f\"✅ File exists: {sp}\")\n",
    "else:\n",
    "    print(f\"❌ File not found: {sp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8536b25-b8ce-48ce-a5e6-d33ad261362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries: 55\n",
      "Accuracy:     0.2545\n",
      "\n",
      "                                              precision    recall  f1-score   support\n",
      "\n",
      "                                         age     0.0000    0.0000    0.0000         3\n",
      "                                 case_status     0.0000    0.0000    0.0000         2\n",
      "                             contact_setting     0.0000    0.0000    0.0000         1\n",
      "                                country_list     0.0000    0.0000    0.0000         0\n",
      "                                        date     0.0000    0.0000    0.0000        12\n",
      "                                  date_range     0.0000    0.0000    0.0000         0\n",
      "                           diagnosis_details     0.0000    0.0000    0.0000         0\n",
      "                                      gender     0.0000    0.0000    0.0000         2\n",
      "                                 gender_type     0.0000    0.0000    0.0000         0\n",
      "                                          id     0.0000    0.0000    0.0000         5\n",
      "                                    location     0.0000    0.0000    0.0000         6\n",
      "                             medical_boolean     0.8750    0.7000    0.7778        20\n",
      "medical_boolean \n",
      "or \n",
      "patient_recovery_status     0.0000    0.0000    0.0000         0\n",
      "                                  occupation     0.0000    0.0000    0.0000         1\n",
      "                                     outcome     0.0000    0.0000    0.0000         2\n",
      "                                  patient_id     0.0000    0.0000    0.0000         0\n",
      "                                 patient_ids     0.0000    0.0000    0.0000         0\n",
      "                     patient_recovery_status     0.0000    0.0000    0.0000         0\n",
      "                             response_status     0.0000    0.0000    0.0000         0\n",
      "                                  role_types     0.0000    0.0000    0.0000         0\n",
      "                                    symptoms     0.0000    0.0000    0.0000         1\n",
      "\n",
      "                                    accuracy                         0.2545        55\n",
      "                                   macro avg     0.0417    0.0333    0.0370        55\n",
      "                                weighted avg     0.3182    0.2545    0.2828        55\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_precision</th>\n",
       "      <th>age_recall</th>\n",
       "      <th>age_f1-score</th>\n",
       "      <th>age_support</th>\n",
       "      <th>case_status_precision</th>\n",
       "      <th>case_status_recall</th>\n",
       "      <th>case_status_f1-score</th>\n",
       "      <th>case_status_support</th>\n",
       "      <th>contact_setting_precision</th>\n",
       "      <th>contact_setting_recall</th>\n",
       "      <th>...</th>\n",
       "      <th>macro avg_precision</th>\n",
       "      <th>macro avg_recall</th>\n",
       "      <th>macro avg_f1-score</th>\n",
       "      <th>macro avg_support</th>\n",
       "      <th>weighted avg_precision</th>\n",
       "      <th>weighted avg_recall</th>\n",
       "      <th>weighted avg_f1-score</th>\n",
       "      <th>weighted avg_support</th>\n",
       "      <th>total_entries</th>\n",
       "      <th>run_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55</td>\n",
       "      <td>custom-data-llama-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age_precision  age_recall  age_f1-score  age_support  \\\n",
       "0            0.0         0.0           0.0          3.0   \n",
       "\n",
       "   case_status_precision  case_status_recall  case_status_f1-score  \\\n",
       "0                    0.0                 0.0                   0.0   \n",
       "\n",
       "   case_status_support  contact_setting_precision  contact_setting_recall  \\\n",
       "0                  2.0                        0.0                     0.0   \n",
       "\n",
       "   ...  macro avg_precision  macro avg_recall  macro avg_f1-score  \\\n",
       "0  ...             0.041667          0.033333            0.037037   \n",
       "\n",
       "   macro avg_support  weighted avg_precision  weighted avg_recall  \\\n",
       "0               55.0                0.318182             0.254545   \n",
       "\n",
       "   weighted avg_f1-score  weighted avg_support  total_entries  \\\n",
       "0               0.282828                  55.0             55   \n",
       "\n",
       "                run_name  \n",
       "0  custom-data-llama-0.0  \n",
       "\n",
       "[1 rows x 95 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_checker(sp)\n",
    "\n",
    "#print(f\"Temperature: {temperature}\")\n",
    "#print(f\"Top-p: {top_p}\")\n",
    "## Load Alpaca weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0db8d392-38ea-45d5-8e85-281049b5bdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_metrics.csv      \u001b[0m\u001b[01;32mLICENSE\u001b[0m*              \u001b[01;32mrequirements_notorch.txt\u001b[0m*\n",
      "\u001b[01;34mbatch\u001b[0m/               \u001b[01;34mlogging_results\u001b[0m/      \u001b[01;32mrequirements.txt\u001b[0m*\n",
      "\u001b[01;34mconfusion_matrices\u001b[0m/  \u001b[01;34mmetadata\u001b[0m/             \u001b[01;34mscript\u001b[0m/\n",
      "\u001b[01;34mcustom_data\u001b[0m/         \u001b[01;34mnotebooks\u001b[0m/            \u001b[01;34msrc\u001b[0m/\n",
      "\u001b[01;34mcustom_data_logs\u001b[0m/    output                \u001b[01;34mtable_samples\u001b[0m/\n",
      "\u001b[01;34mdataset_samples\u001b[0m/     \u001b[01;34mpapermill_notebooks\u001b[0m/  \u001b[01;34mWHO_data\u001b[0m/\n",
      "\u001b[01;34mexperiment_results\u001b[0m/  \u001b[01;32mreadme.md\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cad31181-a79e-4f90-a55b-7d104570a862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'ArcheType'\n",
      "/home/omadbek/projects/ArcheType\n"
     ]
    }
   ],
   "source": [
    "cd ArcheType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efc5dc90d553807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Temperature: {temperature}\")\n",
    "#print(\"Top-p: {top_p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2bbabd084979a",
   "metadata": {},
   "source": [
    "# Check alpaca not needed for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "174c5f23-882e-45fd-9a03-d1deca2c363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import snapshot_download\n",
    "\n",
    "# Replace the repo_id below with the exact model‐ID you used.\n",
    "# Often it’s something like \"TheBloke/alpaca-13b-……\"\n",
    "#repo_id = \"chavinlo/alpaca-native\"\n",
    "#local_path = snapshot_download(repo_id)\n",
    "\n",
    "#print(\"Alpaca-13b is here:\", local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0aca134-208c-4793-abd4-a1478fe79538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    elif model == \"alpaca-13b\":\n",
    "#        MAX_LEN=2048\n",
    "#        tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "#        base_model = LlamaForCausalLM.from_pretrained(\n",
    "#            \"chavinlo/alpaca-native\",\n",
    "#            torch_dtype=torch.float16,\n",
    "#            load_in_8bit=True,\n",
    "#            device_map='auto',\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e1f6cd6-65d6-482f-877b-1d226ba21054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#local_path = \"/home/omadbek/.cache/huggingface/hub/models--chavinlo--alpaca-native/snapshots/3bf09cbff2fbd92d7d88a0f70ba24fca372befdf\"\n",
    "\n",
    "#tokenizer = LlamaTokenizer.from_pretrained(local_path)\n",
    "#model     = LlamaForCausalLM.from_pretrained(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96137456-a1b0-4145-bf75-e728d8f43683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python ./src/run.py --model_name=\"alpaca-7b-zs\" --model_path=\"/home/omadbek/.cache/huggingface/hub/models--chavinlo--alpaca-native/snapshots/3bf09cbff2fbd92d7d88a0f70ba24fca372befdf\" --save_path=\"./output\" --input_files=\"./projects/ArcheType/WHO_data\" --input_labels=\"./projects/ArcheType/custom_data/cta_gt.csv\" --label_set=\"custom\" --method ans_contains_gt gt_contains_ans resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59f21daf-4aa6-4c3c-9765-829a27b14a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96bb8e8-d647-4361-91f7-77e66fb6795e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d9b0404-dee8-4a46-b472-d65fc82e1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test fine-tuned Alpaca\n",
    "#tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 2) Load the sharded safetensors into a LlamaForCausalLM:\n",
    "#model = LlamaForCausalLM.from_pretrained(\n",
    "#    model_path,\n",
    "#    torch_dtype=torch.float16,    # if you saved in fp16\n",
    "#    load_in_8bit=True,            # if you want to use 8-bit (requires bitsandbytes)\n",
    "#    device_map=\"auto\",            # or replace with e.g. {\"\": 0} for single-GPU\n",
    "#    low_cpu_mem_usage=True,       # helps when loading large models\n",
    "#)\n",
    "\n",
    "# 3) Switch to eval mode (optional, but common for inference):\n",
    "#model.eval()\n",
    "\n",
    "# 4) Now you can generate with your fine-tuned Alpaca:\n",
    "#prompt_text = \"INSTRUCTION: Select the option which best describes the input. INPUT: ['6L170B0G', 'G08L03B0', 'L20GB406', '0BG90L29', '3GL0780B']. OPTIONS: - case_status - outcome - contact_setting - age - medical_boolean - symptoms - date - id - location - occupation - gender ANSWER:\"\n",
    "#input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "#outputs = model.generate(input_ids, max_length=256, do_sample=True, temperature=0.7)\n",
    "#print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7e658-0be2-469e-88cd-5155d17260b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
