{
 "cells": [
  {
   "cell_type": "code",
   "id": "cc22338c85d486a2",
   "metadata": {
    "tags": [
     "parameters"
    ],
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:28.672495Z",
     "start_time": "2025-08-19T11:36:28.668627Z"
    }
   },
   "source": "# Parameters\ndata_path     = \"/home/omadbek/projects/Sherlock/custom_data/label_generation/test_data_generation.parquet\"\nlabels_path   = \"/home/omadbek/projects/Sherlock/custom_data/label_generation/test_labels_generation.parquet\"\narchetype_directory = \"/home/omadbek/projects/ArcheType\"\nrun_all_directory = \"/home/omadbek/projects/run_all\"\nprompt_type = \"zero-shot\"\ntune = \"0.0\"",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "be99491884dd57ac",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:29.201116Z",
     "start_time": "2025-08-19T11:36:29.199500Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:31.616430Z",
     "start_time": "2025-08-19T11:36:29.596425Z"
    }
   },
   "source": "#https://colab.research.google.com/drive/1BEZ_qgtVqSmOmCTuhHs7lHiYB5M5_myg?usp=sharing\n\nimport pandas as pd\nimport shutil\nfrom pathlib import Path\nimport json\n#import gzipƒ\nfrom tqdm.auto import tqdm\nimport subprocess\nimport time\nimport re\nimport requests\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score\nfrom itertools import chain\nimport argparse\nimport os\nimport sys\nfrom copy import deepcopy\nimport torch\nfrom retry import retry\nimport random\n",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omadbek/.conda/envs/archetype/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "d565ecc3-2433-4b7b-bfa7-d8225b0db815",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:31.919819Z",
     "start_time": "2025-08-19T11:36:31.772659Z"
    }
   },
   "source": "!cat /home/omadbek/projects/ArcheType/src/const.py",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EST_CHARS_PER_TOKEN=4\r\n",
      "MAX_LEN=2000*EST_CHARS_PER_TOKEN\r\n",
      "INTEGER_SET = set(r\"0123456789,/\\+-.^_()[] :\")\r\n",
      "BOOLEAN_SET = set([\"True\", \"true\", \"False\", \"false\", \"yes\", \"Yes\", \"No\", \"no\"])\r\n",
      "\r\n",
      "ARCHETYPE_PATH = \"/home/omadbek/projects/ArcheType\"\r\n",
      "DOTENV_PATH = \"/home/omadbek/projects/ArcheType/.env\"\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "881a695b-e8d4-4c8b-ab64-52f8152df91a",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:32.012226Z",
     "start_time": "2025-08-19T11:36:32.008836Z"
    }
   },
   "source": "cd ..",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/omadbek/projects/ArcheType\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "4dafde7b-0eef-4af8-a652-012a533151a6",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:32.820766Z",
     "start_time": "2025-08-19T11:36:32.486826Z"
    }
   },
   "source": "import torch\nprint(torch.cuda.device_count())      # → 1\nprint(torch.cuda.get_device_name(0))  # → the one you chose",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "229d1025-a37a-4c7d-95f9-5714da81c743",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:33.052129Z",
     "start_time": "2025-08-19T11:36:33.046692Z"
    }
   },
   "source": "def fix_labels(label, label_set):\n  label = label.lower().strip()\n  ldm = {k.lower().strip() : v.lower().strip() for k, v in label_set['dict_map'].items()}\n  if label_set.get(\"abbrev_map\", -1) != -1:\n    lda = {k.lower().strip() : v.lower().strip() for k, v in label_set['abbrev_map'].items()}\n    ldares = lda.get(label, \"\")\n    if ldares != \"\":\n      label = ldares\n  if label.endswith(\"/name\"):\n    label = label[:-5]\n  remap = ldm.get(label, -1)\n  if remap != -1:\n    label = remap\n  return label.lower()",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "b47fba85-2e32-4803-82d4-278a673a03ff",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:33.629728Z",
     "start_time": "2025-08-19T11:36:33.625458Z"
    }
   },
   "source": "#LABELS = ['age', 'case_status', 'contact_setting', 'date', 'gender', 'id',\n#       'location', 'medical_boolean', 'occupation', 'outcome', 'symptoms']\n\nLABELS = ['none']",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "1db475a1-be44-4499-980d-127a62cab476",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:34.050987Z",
     "start_time": "2025-08-19T11:36:34.045575Z"
    }
   },
   "source": "sotab_integer_labels = [\"age\", \"date\"]\nsotab_float_labels   = []\n\n# everything else must go here\nsotab_other_labels = [\n  \"case_status\",\n  \"gender\",\n  \"id\",\n  \"location\",\n  \"medical_boolean\",\n  \"occupation\",\n  \"outcome\",\n  \"symptoms\"\n]\n\nsotab_top_hier = {\n  \"integer\": sotab_integer_labels,\n  \"float\":   sotab_float_labels,\n  \"other\":   sotab_other_labels\n}\n\nsotab_identifier = [\"id\"]\nsotab_category   = [\"gender\", \"medical_boolean\", \"outcome\"]\nsotab_text       = [\"location\", \"symptoms\", \"occupation\"]\n\n\nsotab_other_hier = {\n  \"Identifier\": sotab_identifier,\n  \"category\":   sotab_category,\n  \"text\":       sotab_text\n}",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "60e79430-47a7-4d5a-9f97-b5524ed9305a",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:34.454323Z",
     "start_time": "2025-08-19T11:36:34.450766Z"
    }
   },
   "source": "rand_seed=13\nEST_CHARS_PER_TOKEN=4\nMAX_LEN=2000*EST_CHARS_PER_TOKEN",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "ff3bbf8b-5706-486b-9cd0-857411301200",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:34.858724Z",
     "start_time": "2025-08-19T11:36:34.855271Z"
    }
   },
   "source": "model_path = \"/home/omadbek/projects/alpaca/outputs\"",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "c10f1c02-1fd2-4795-9ee0-17b6b16ad4b1",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:35.258970Z",
     "start_time": "2025-08-19T11:36:35.249999Z"
    }
   },
   "source": [
    "# PROMPTS\n",
    "\n",
    "def llm_prompts(input_list, options_str=None):\n",
    "    if prompt_type == \"zero-shot\":\n",
    "        prompt = \\\n",
    "            f\"\"\"\n",
    "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
    "\n",
    "                INSTRUCTIONS:\n",
    "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
    "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
    "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
    "\n",
    "                INPUT: {input_list}\n",
    "                OPTIONS: {options_str}\n",
    "                ANSWER:\n",
    "            \"\"\"\n",
    "    elif prompt_type == \"few-shot\":\n",
    "        prompt = \\\n",
    "            f\"\"\"\n",
    "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
    "\n",
    "                INSTRUCTIONS:\n",
    "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
    "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
    "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
    "\n",
    "                This is just examples how it could look like:\n",
    "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
    "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
    "                ANSWER: vaccine_names\n",
    "\n",
    "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
    "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
    "                ANSWER: disease\n",
    "\n",
    "                Now it's your turn:\n",
    "                INPUT: {input_list}\n",
    "                OPTIONS: {options_str}\n",
    "                ANSWER:\n",
    "            \"\"\"\n",
    "        # prompt = f\"\"\"SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
    "        #\n",
    "        #             INSTRUCTIONS:\n",
    "        #              • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
    "        #              • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label, do not generate label that strictly follows what is inside data.\n",
    "        #              • Respond with EXACTLY the label token, no additional words, punctuation, or explanation.\n",
    "        #\n",
    "        #             This is just examples how it could look like:\n",
    "        #             INPUT: [26, 29, 35, 45]\n",
    "        #             OPTIONS: [\"id\", \"age\", \"date\", \"location\"]\n",
    "        #             ANSWER: age\n",
    "        #\n",
    "        #             INPUT: [\"taxi driver\", \"lawyer\", \"pilot\", \"entrepreneur\"]\n",
    "        #             OPTIONS: [\"location\", \"medical_boolean\", \"symptoms\", \"occupation\"]\n",
    "        #             ANSWER: occupation\n",
    "        #\n",
    "        #             Now it's your turn:\n",
    "        #             INPUT: {input_list}\n",
    "        #             OPTIONS: {options_str}\n",
    "        #             ANSWER:\n",
    "        #         \"\"\"\n",
    "    elif prompt_type == \"chain-thoughts\":\n",
    "        prompt = \\\n",
    "            f\"\"\"\n",
    "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
    "\n",
    "                INSTRUCTIONS:\n",
    "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
    "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
    "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation. Do not output step by step reasoaning steps. Only output final answer **\n",
    "\n",
    "                INPUT: {input_list}\n",
    "                OPTIONS: {options_str}\n",
    "\n",
    "                Let’s think step by step:\n",
    "                1. Identify the key features in the INPUT.\n",
    "                2. Try to predict values type like: int, date, string.\n",
    "                3. Compare against each option to see which matches best.\n",
    "                4. If none match, decide on a broad new label in snake_case.\n",
    "                5. Final Answer:\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "        # SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
    "        #\n",
    "        #         INSTRUCTIONS:\n",
    "        #         • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
    "        #         • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label, do not generate label that strictly follows what is inside data.\n",
    "        #         • Respond with EXACTLY the label token, no additional words, punctuation, or explanation. Do not output step by step reasoaning steps. Only output final answer\n",
    "        #\n",
    "        #         INPUT: {input_list}\n",
    "        #         OPTIONS: {options_str}\n",
    "        #\n",
    "        #         Let’s think step by step:\n",
    "        #         1. Identify the key features in the INPUT.\n",
    "        #         2. Try to predict values type like: int, date, string.\n",
    "        #         2. Compare against each option to see which matches best.\n",
    "        #         3. If none match, decide on a broad new label in snake_case.\n",
    "        #         4. Final Answer:\n",
    "        #     \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "aff7bdfa-04c5-4106-b791-166e74bbc98c",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:35.643080Z",
     "start_time": "2025-08-19T11:36:35.640594Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ba93a7e3-1802-4db0-96e5-6e9f8c9e1921",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:37.712355Z",
     "start_time": "2025-08-19T11:36:36.093419Z"
    }
   },
   "source": "import re\nfrom typing import List, Tuple\n\nfrom sentence_transformers import SentenceTransformer, util\nfrom rapidfuzz import fuzz\n\n# Load the embedding model once at module scope\n_sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n\ndef _normalize_label(s: str) -> str:\n    \"\"\"\n    Lowercase, drop non-alphanumerics,\n    and strip a trailing 's' for crude singularization.\n    \"\"\"\n    s = s.lower()\n    s = re.sub(r'[^a-z0-9]', '', s)\n    if s.endswith('s'):\n        s = s[:-1]\n    return s\n\ndef _embed_labels(labels: List[str]):\n    \"\"\"\n    Encode a list of labels into normalized sentence‐embeddings.\n    \"\"\"\n    return _sent_model.encode(labels, normalize_embeddings=True)\n\nclass _FuzzyLabelMatcher:\n    def __init__(\n        self,\n        fixed_labels: List[str],\n        embed_threshold: float,\n        fuzz_threshold: int\n    ):\n        self.fixed_labels = fixed_labels\n        self.embed_threshold = embed_threshold\n        self.fuzz_threshold = fuzz_threshold\n        self._update_embeddings()\n\n    def _update_embeddings(self):\n        self._embeddings = (\n            _embed_labels(self.fixed_labels)\n            if self.fixed_labels else\n            None\n        )\n\n    def _find_match(self, label: str):\n\n        norm_label = label.lower()\n\n        if not self.fixed_labels:\n            return None\n\n        # 1) token- and exact-normalize as before…\n        for existing in self.fixed_labels:\n            tokens = re.split(r'[^a-zA-Z0-9]+', existing.lower())\n            if norm_label in tokens:\n                return existing\n\n        # 2) compute all fuzzy scores & pick the best\n        fuzz_scores = [\n            (existing, fuzz.token_sort_ratio(label, existing))\n            for existing in self.fixed_labels\n        ]\n        best_fuzz_label, best_fuzz_score = max(fuzz_scores, key=lambda x: x[1])\n\n        # 3) compute all embed sims & pick the best\n        emb     = _sent_model.encode([label], normalize_embeddings=True)[0]\n        sims    = util.cos_sim(emb, self._embeddings)[0].tolist()\n        best_idx = max(range(len(sims)), key=lambda i: sims[i])\n        best_emb_label, best_emb_score = (\n            self.fixed_labels[best_idx],\n            sims[best_idx],\n        )\n        \n        #print(f\"[DEBUG] Comparing '{label}' → \"\n        #      f\"best_fuzz:('{best_fuzz_label}', {best_fuzz_score}), \"\n        #      f\"best_emb:('{best_emb_label}', {best_emb_score:.2f})\")\n\n        # 4) if either passes its threshold, choose the stronger signal\n        if best_fuzz_score >= self.fuzz_threshold and best_emb_score >= self.embed_threshold:\n            # pick whichever of the two is relatively stronger\n            # (you could also prefer embed over fuzz, etc.)\n            if best_emb_score >= best_fuzz_score / 100:\n                return best_emb_label\n            else:\n                return best_fuzz_label\n\n        return None\n\n    def resolve(self, orig_label: str) -> str:\n        \"\"\"\n        If `orig_label` matches an existing one, return that existing label.\n        Otherwise, add `orig_label` to fixed_labels and return it.\n        \"\"\"\n        match = self._find_match(orig_label)\n        if match:\n            return match\n\n        # new label → add and update embeddings\n        self.fixed_labels.append(orig_label)\n        self._update_embeddings()\n        return orig_label\n\ndef resolve_label(\n    orig_label: str,\n    fixed_labels: List[str],\n    embed_threshold: float = 0.82,\n    fuzz_threshold: int = 90\n) -> str:\n    \"\"\"\n    Given a newly generated label `orig_label` and a list of\n    `fixed_labels`, return a tuple:\n      (resolved_label, updated_fixed_labels).\n\n    - If `orig_label` is similar to an existing label, `resolved_label`\n      is that existing label.\n    - Otherwise, `orig_label` is appended to fixed_labels, and returned.\n    \"\"\"\n    print(\"_FuzzyLabelMatcher list: \", fixed_labels)\n    matcher = _FuzzyLabelMatcher(fixed_labels, embed_threshold, fuzz_threshold)\n    resolved = matcher.resolve(orig_label)\n    return resolved",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:38.448509Z",
     "start_time": "2025-08-19T11:36:37.890105Z"
    }
   },
   "cell_type": "code",
   "source": "import re\nfrom typing import List\n\nfrom sentence_transformers import SentenceTransformer, util\n\n# Load the embedding model once at module scope\n_sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n\nclass _CosineLabelMatcher:\n    def __init__(\n        self,\n        fixed_labels: List[str],\n        embed_threshold: float\n    ):\n        self.fixed_labels = fixed_labels\n        self.embed_threshold = embed_threshold\n        self._update_embeddings()\n\n    def _update_embeddings(self):\n        self._embeddings = (\n            _sent_model.encode(self.fixed_labels, normalize_embeddings=True)\n            if self.fixed_labels else None\n        )\n\n    def _normalize_tokens(self, text: str) -> List[str]:\n        return re.split(r'[^a-zA-Z0-9]+', text.lower())\n\n    def _find_match(self, label: str) -> str:\n        if not self.fixed_labels:\n            return None\n\n        norm = label.lower()\n        # 1) token- or exact-match\n        for existing in self.fixed_labels:\n            if norm in self._normalize_tokens(existing):\n                return existing\n\n        # 2) embed-based similarity\n        emb = _sent_model.encode([label], normalize_embeddings=True)[0]\n        sims = util.cos_sim(emb, self._embeddings)[0].tolist()\n        best_idx = max(range(len(sims)), key=lambda i: sims[i])\n        best_label = self.fixed_labels[best_idx]\n        best_score = sims[best_idx]\n\n        if best_score >= self.embed_threshold:\n            return best_label\n\n        return None\n\n    def resolve(self, orig_label: str) -> str:\n        match = self._find_match(orig_label)\n        if match:\n            return match\n        # new label → add and update embeddings\n        self.fixed_labels.append(orig_label)\n        self._update_embeddings()\n        return orig_label\n\n\ndef resolve_label(\n    orig_label: str,\n    fixed_labels: List[str],\n    embed_threshold: float = 0.82\n) -> str:\n    \"\"\"\n    Given a newly generated label `orig_label` and a list of\n    `fixed_labels`, return the resolved label:\n\n      - If `orig_label` is similar to an existing label (by cosine\n        similarity or exact token match), returns that existing label.\n      - Otherwise, appends `orig_label` to `fixed_labels` and returns it.\n    \"\"\"\n    matcher = _CosineLabelMatcher(fixed_labels, embed_threshold)\n    return matcher.resolve(orig_label)\n",
   "id": "770c20a8a920a24e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:38.483461Z",
     "start_time": "2025-08-19T11:36:38.481671Z"
    }
   },
   "cell_type": "code",
   "source": "#fixed = [\"patient_id\", \"city_name\", \"medical_boolean\", \"date_of_birth\"]\n\n#resolved_label = resolve_label(\"patient_gender\", fixed, embed_threshold = 0.70,) #fuzz_threshold = 80)\n#print(\"Resolved label is: \", resolved_label)\n\n#fixed = [\"patient_ids\", \"city_name\", \"medical_boolean\", \"date_of_birth\"]\n#resolved_label = resolve_label(\"location\", fixed, embed_threshold = 0.50,)\n#print(\"Resolved label is: \", resolved_label)",
   "id": "3f0ad0852087fc42",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:38.544047Z",
     "start_time": "2025-08-19T11:36:38.542453Z"
    }
   },
   "cell_type": "code",
   "source": "#fixed = [\"location\", \"age\", \"patient_ids\", \"date_of_birth\"]\n\n#resolved_label = resolve_label(\"date_values\", fixed, embed_threshold = 0.40, fuzz_threshold = 40)\n#print(\"Resolved label is: \", resolved_label)\n#matcher = _FuzzyLabelMatcher(fixed, 0.82, 90)\n#print(matcher.remap_label(\"patient_id\", fixed))",
   "id": "b367fbc69ea0ce7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "76159fc4-0916-44b2-b99d-e6eca91be82e",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:39.752675Z",
     "start_time": "2025-08-19T11:36:38.621904Z"
    }
   },
   "source": "from accelerate import infer_auto_device_map, init_empty_weights, load_checkpoint_and_dispatch\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, \\\n    T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\nimport langchain\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain import PromptTemplate, LLMChain\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\n#sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n\n\ndef set_pipeline(k=1):\n    pipe = pipeline(\n        \"text-generation\",\n        model=base_model,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN,\n        temperature=0.5 * k,\n        top_p=0.80 - (0.1 * k),\n        repetition_penalty=1.3\n    )\n    local_llm = HuggingFacePipeline(pipeline=pipe)\n    llm_chain = LLMChain(prompt=pt,\n                         llm=local_llm\n                         )\n    return pipe, local_llm, llm_chain\n\n\ncurr_model = \"\"\n\n\ndef init_model(model):\n    curr_model = model\n    with torch.no_grad():\n        torch.cuda.empty_cache()\n    if model == \"llama-65b\":\n        LLAMA_PATH = \"/scratch/bf996/text-generation-webui/models/llama-65b-hf\"\n        MAX_LEN = 2048\n        tokenizer = LlamaTokenizer.from_pretrained(LLAMA_PATH)\n        config = AutoConfig.from_pretrained(LLAMA_PATH,\n                                            torch_dtype=torch.float16,\n                                            load_in_8bit=True)\n        with init_empty_weights():\n            base_model = AutoModelForCausalLM.from_config(config)\n        base_model.tie_weights()\n        device_map = infer_auto_device_map(base_model, max_memory={0: \"60GiB\", \"cpu\": \"96GiB\"})\n        base_model = load_checkpoint_and_dispatch(\n            base_model,\n            LLAMA_PATH,\n            device_map=device_map\n        )\n    elif model == \"alpaca-13b\":\n        MAX_LEN = 2048\n        tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n        #tokenizer = LlamaTokenizer.from_pretrained(model_path)\n        base_model = LlamaForCausalLM.from_pretrained(\n            #model_path,\n            \"chavinlo/alpaca-native\",\n            torch_dtype=torch.float16,\n            load_in_8bit=True,\n            device_map='auto',\n        )\n    elif model == \"alpaca-fine-tuned\":\n        MAX_LEN = 2048\n        base_model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n    elif model == \"vicuna-13b\":\n        MAX_LEN = 2048\n        tokenizer = AutoTokenizer.from_pretrained(\"eachadea/vicuna-13b\")\n        base_model = AutoModelForCausalLM.from_pretrained(\n            \"eachadea/vicuna-13b\",\n            torch_dtype=torch.float16,\n            load_in_8bit=True,\n            device_map='auto',\n        )\n    elif model == \"gpt4-x-alpaca\":\n        MAX_LEN = 2048\n        tokenizer = AutoTokenizer.from_pretrained(\"chavinlo/gpt4-x-alpaca\")\n        base_model = AutoModelForCausalLM.from_pretrained(\"chavinlo/gpt4-x-alpaca\", device_map=\"auto\",\n                                                          load_in_8bit=True)\n    elif model == \"t0pp\":\n        MAX_LEN = 512\n        tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0pp\")\n        base_model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\",\n                                                           torch_dtype=torch.float16, load_in_8bit=True)\n    elif model == \"flan-t5-xxl\":\n        MAX_LEN = 512\n        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n        base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\",\n                                                           torch_dtype=torch.float16, load_in_8bit=True)\n    elif model == \"flan-ul2\":\n        MAX_LEN = 512\n        base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16,\n                                                                device_map=\"auto\")\n        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n    elif model == \"galpaca-30b\":\n        MAX_LEN = 2048\n        tokenizer = AutoTokenizer.from_pretrained(\"GeorgiaTechResearchInstitute/galpaca-30b\", device_map=\"auto\",\n                                                  torch_dtype=torch.float16, load_in_8bit=True)\n        base_model = AutoModelForCausalLM.from_pretrained(\"GeorgiaTechResearchInstitute/galpaca-30b\")\n    elif model == \"opt-iml-max-30b\":\n        MAX_LEN = 2048\n        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-iml-max-30b\", use_fast=False, padding_side='left')\n        base_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-iml-max-30b\", device_map=\"auto\",\n                                                          torch_dtype=torch.float16)\n    if model in [\"flan-t5-xxl\", \"t0pp\", \"flan-ul2\"]:\n        template = \"\"\"{instruction}\"\"\"\n    else:\n        template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n        ### Instruction: \n        {instruction}\n\n        Answer:\"\"\"\n    pt = PromptTemplate(template=template, input_variables=[\"instruction\"])\n    #Convert length from tokens to characters, leave room for model response\n    MAX_LEN = MAX_LEN * EST_CHARS_PER_TOKEN - 200\n    return base_model, tokenizer, template, pt, MAX_LEN",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "faa69500-a350-4dc9-9204-f5bea462888c",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:39.926781Z",
     "start_time": "2025-08-19T11:36:39.908240Z"
    }
   },
   "source": "def get_sherlock_resp(df, gt_df, prompt_dict, model, label_indices, base_prompt, lsd):\n    isd4 = \"d4\" in lsd['name']\n    if \"sherlock\" in model:\n        model = sherlock_model\n        data_m = pd.Series(df[label_indices].astype(str).T.values.tolist())\n        extract_features(\n            \"../temporary.csv\",\n            data_m\n        )\n        feature_vectors = pd.read_csv(\"../temporary.csv\", dtype=np.float32)\n        predicted_labels = model.predict(feature_vectors, \"sherlock\")\n        iter_len = len(data_m)\n    elif \"doduo\" in model:\n        model = doduo_model\n        data_m = df[label_indices]\n        try:\n            annot_m = doduo_model.annotate_columns(data_m)\n            predicted_labels = annot_m.coltypes\n        except Exception as e:\n            print(f\"Exception {e} in Doduo, returning default \\n\")\n            predicted_labels = [\"text\" for i in range(len(data_m))]\n        iter_len = len(predicted_labels)\n    predicted_labels_dict = {i: sherlock_to_cta.get(predicted_labels[i], [predicted_labels[i]]) for i in\n                             range(iter_len)}\n\n    for idx, label_idx in zip(range(iter_len), label_indices):\n        prompt = base_prompt + \"_\" + str(label_idx)\n        if isd4:\n            ans = predicted_labels[0]\n            label = [s.lower() for s in lsd['d4_map'][gt_df]]\n        else:\n            gt_row = gt_df[gt_df['column_index'] == label_idx]\n            if len(gt_row) != 1:\n                continue\n            label = fix_labels(gt_row['label'].item(), lsd)\n            ans = [fix_labels(item, lsd) for item in predicted_labels_dict[idx]]\n        if isd4:\n            res = ans in label\n        else:\n            assert isinstance(ans, list), \"ans should be a list\"\n            res = label in ans\n        ans_dict = {\"response\": ans, \"context\": None, \"ground_truth\": label, \"correct\": res,\n                    \"orig_model_label\": predicted_labels[idx]}\n        prompt_dict[prompt] = ans_dict\n    return prompt\n\n\n@retry(Exception, tries=3, delay=3)\ndef get_chatgpt_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None,\n                     method=[\"similarity\"], max_len=15000):\n    fixed_labels = [fix_labels(s, lsd) for s in lsd['label_set']]\n    model = \"gpt-3.5\"\n    context_labels = \", \".join(fixed_labels)\n    fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n    prompt = prompt_context_insert(context_labels, context, max_len, \"gpt-3.5\")\n    d_p = prompt_dict.get(prompt, -1)\n    if d_p != -1 and \"skip-existing\" in method:\n        #recompute_results(prompt_dict, prompt, model, cbc_pred=None, label_set=lsd)\n        return prompt\n    elif d_p != -1:\n        while prompt_dict.get(prompt, -1) != -1:\n            prompt = prompt + \"*\"\n    if response:\n        ans = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            temperature=0,\n        ).choices[0]['message']['content']\n        #print(f\"Original ans is {ans}\")\n    ans_n = fuzzy_label_match(ans, fixed_labels, None, None, prompt, lsd, model, method=method)\n    #print(f\"Fuzzy ans is {ans_n}\")\n    res = ans_n == ground_truth\n    ans_dict = {\"response\": ans_n, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res,\n                \"original_model_answer\": ans}\n    prompt_dict[prompt] = ans_dict\n    return prompt\n\n\n@retry(Exception, tries=5, delay=3)\ndef get_ada_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None):\n    prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"ada-personal\")\n    if prompt_dict.get(prompt, -1) != -1:\n        #recompute_results(prompt_dict, prompt, \"ada-personal\", label_set=lsd)\n        return prompt\n    if response:\n        proc = subprocess.run(\n            [\"openai\", \"api\", \"completions.create\", \"-m\", \"ada:ft-personal:-2023-03-14-11-52-45\", \"-M\", \"3\", \"-p\",\n             prompt], capture_output=True, check=True)\n        ans = proc.stdout.decode(\"utf-8\")[len(prompt):].strip()\n    else:\n        ans = \"\"\n    res = ans.lower().strip().startswith(ground_truth)\n    ans_dict = {\"response\": ans, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res}\n    prompt_dict[prompt] = ans_dict\n    return prompt\n\ndef call_llama_model(session, link, prompt, lsd, var_params, generated_labels_list=None):\n    # Build the payload expected by the new LLaMA endpoint\n    payload = {\n        \"model\":      \"llama3.1:8b-instruct-q8_0\",\n        \"prompt\":     prompt,\n        \"stream\":     False,\n        \"options\": {\n            \"seed\": 42,\n            \"num_predict\": 30,\n        }\n    }\n\n    # Choose session-based or direct requests call\n    client = session or requests\n    resp = client.post(link, json=payload)\n    resp.raise_for_status()\n\n    # Extract the generated text\n    data = resp.json()\n    text = data.get(\"response\", \"\")\n    origina_model_response = text.strip()\n\n\n    #print(\"LLama model was called\")\n    resolve_response = resolve_label(origina_model_response, generated_labels_list, embed_threshold = 0.70)\n\n\n    print(\"Original model response: \", origina_model_response)\n    print(\"Resolved response: \", resolve_response)\n    # Resolved and original model response\n    return resolve_response, origina_model_response\n    \n    #return fix_labels(text.strip(), generated_labels_list)\n\ntemperature = 0\ntop_p = 0\n\ndef extract_answer(orig_ans: str) -> str:\n    \"\"\"\n    If orig_ans contains 'ANSWER:...', return the text after the colon.\n    Otherwise, return orig_ans unchanged (stripped).\n    \"\"\"\n    m = re.search(r\"ANSWER\\s*:\\s*(.*)\", orig_ans, re.IGNORECASE)\n    if m:\n        return m.group(1).strip()\n    return orig_ans.strip()\n\n#generated_labels_list = []\n\ndef generate_label(session, link, old_prompt, lsd, var_params, generated_labels_list, orig_ans):\n\n    if orig_ans.lower() != \"none\" and orig_ans not in generated_labels_list:\n        generated_labels_list.append(orig_ans)\n          \n    return picker_prompt, picked\n\n    \n    \n    \n\n@retry(Exception, tries=3, delay=3)\ndef get_topp_resp(prompt, k):\n    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n\n    temperature = 0.1 * k\n    top_p   = 0.90 - (0.1 * k)\n\n    outputs = base_model.generate(inputs,\n                                  max_length=MAX_LEN,\n                                  #do_sample=False,\n                                  #num_beams=1\n                                  temperature=temperature,\n                                  top_p=top_p,\n                                  repetition_penalty=1.3\n                                  )\n    orig_ans = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return extract_answer(orig_ans)\n\n\n\n@retry(Exception, tries=3, delay=3)\ndef get_llama_resp(lsd: dict, context: list, ground_truth: str, prompt_dict: dict, link: str, response=True,\n                   session=None, cbc=None, model=\"llama\", limited_context=None,\n                   method=[\"ans_contains_gt\", \"gt_contains_ans\", \"resample\"], generated_labels_list = None):\n    #print(f\"in get llama resp, gt is {ground_truth}, context is {context}\")\n    isd4 = \"d4\" in lsd['name']\n    if isd4:\n        gtv = lsd['d4_map'][ground_truth]\n        if isinstance(gtv, str):\n            gtv = [gtv]\n        ground_truth = [s.lower() for s in gtv]\n    if \"hierarchical\" in method and not isd4:\n        dtype = get_base_dtype(limited_context)\n        fixed_labels = sotab_top_hier[dtype]\n    else:\n        fixed_labels = list(set([fix_labels(s, lsd) for s in lsd['label_set']]))\n    context_labels = \", \".join(fixed_labels)\n    fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n    if model in [\"llama-zs\", \"opt-iml-30b-zs\"]:\n        pipe, local_llm, llm_chain = set_pipeline(k=1)\n    prompt = prompt_context_insert(context_labels, context, MAX_LEN, model, options = generated_labels_list)\n    d_p = prompt_dict.get(prompt, -1)\n    #skip existing logic\n    if d_p != -1 and \"skip-existing\" in method:\n        # recompute_results(prompt_dict, prompt, \"llama\", cbc, lsd)\n        return prompt, prompt_dict[prompt][\"response\"]\n    elif d_p != -1:\n        while prompt_dict.get(prompt, -1) != -1:\n            prompt = prompt + \"*\"\n    #print(\"GET LLAMA NEETY GREEDY:\")\n    #print(prompt)\n    #response logic\n    original_model_response = \"\"\n    if not response:\n        orig_ans = ans_n = \"\"\n        original_model_response = \"\"\n    else:\n        orig_ans = apply_basic_rules(limited_context, None)\n        if orig_ans is None:\n            orig_ans, original_model_response = query_correct_model(model, prompt, context_labels, context, session, link, lsd, generated_labels_list)\n            \n            #hierarchical matching logic\n            if \"hierarchical\" in method and dtype == \"other\" and orig_ans not in ['email', 'URL', 'WebHTMLAction',\n                                                                                  'Photograph']:\n                next_label_set = sotab_other_hier.get(orig_ans, -1)\n                if next_label_set == -1:\n                    print(f\"Original answer {orig_ans} not found in hierarchy\")\n                    next_label_set = sotab_other_hier['text']\n                fixed_labels = list(set([fix_labels(s, lsd) for s in next_label_set]))\n                context_labels = \", \".join(fixed_labels)\n                fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n                orig_ans, original_model_response = query_correct_model(model, prompt, context_labels, context, session, link, lsd, generated_labels_list)\n                #fuzzy matching logic\n            #print(\"Fuzzy matching logic\")\n            #print(f\"Fixed LABELSS: {generated_labels_list}\")\n            #print(f\"Fuzzy prompt: {prompt}\")\n            #print(f\"Fuzzy lsd: {lsd}\")\n            ans_n = orig_ans.lower()\n            original_model_response = original_model_response.lower()\n    \n        else:\n            ans_n = orig_ans.lower()\n            original_model_response = original_model_response.lower()\n\n    print(f\"LLM Picker 1 answer: {ans_n}... Should be none in the beginning\")\n\n    if ans_n.lower() != \"none\" and ans_n not in generated_labels_list:\n        generated_labels_list.append(ans_n)\n\n    print(f\"List of label so far: {generated_labels_list}\")\n\n \n\n    #print(f\"final label set was {fixed_labels}, prediction was {ans_n}, ground truth was {ground_truth} \\n\")\n    if isd4:\n        res = ans_n in ground_truth\n    else:\n        res = ans_n == ground_truth\n        \n    ans_dict = {\"response\": ans_n, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res,\n                \"original_model_answer\": orig_ans}\n\n    prompt_dict[prompt] = ans_dict\n\n    #print(f\"Final answer: {ans_n}\")\n    return prompt, ans_n, original_model_response\n\n@retry(Exception, tries=5, delay=3)\ndef get_bloomz_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None):\n    prompt = prompt_context_insert(context_labels, context, 2000, \"bloomz\")\n    if prompt_dict.get(prompt, -1) != -1:\n        return prompt\n    if response:\n        inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda:0\")\n        outputs = model.generate(inputs, max_new_tokens=5)\n    else:\n        response = \"\"\n    ans = tokenizer.decode(outputs[0]).split()[-1]\n    ans = ''.join(e for e in ans if e.isalnum()).lower()\n    res = ans == ground_truth\n    ans_dict = {\"response\": ans, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res}\n    prompt_dict[prompt] = ans_dict\n    return prompt",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "85950645-b679-4139-baed-fcaa30e91aa8",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:40.048015Z",
     "start_time": "2025-08-19T11:36:40.016855Z"
    }
   },
   "source": "def to_integer(val):\n    return pd.to_numeric(val, downcast='integer', errors='ignore')\n\n\ndef derive_meta_features(col):\n    features = {}\n    if not col.astype(str).apply(str.isnumeric).all():\n        return {\"std\": round(col.astype(str).str.len().std(), 2), \"mean\": round(col.astype(str).str.len().mean(), 2),\n                \"mode\": col.astype(str).str.len().mode().iloc[0].item(), \"median\": col.astype(str).str.len().median(),\n                \"max\": col.astype(str).str.len().max(), \"min\": col.astype(str).str.len().min(),\n                \"rolling-mean-window-4\": [0.0]}\n    col = col.dropna().astype(float)\n    if col.apply(float.is_integer).all():\n        col = col.astype(int)\n    #print(f\"Collecting metafeatures for column {col} \\n\")\n    features['std'] = round(col.std(), 2)\n    features['mean'] = round(col.mean(), 2)\n    features['mode'] = col.mode().iloc[0].item()\n    features['median'] = col.median()\n    features['max'] = col.max()\n    features['min'] = col.min()\n    indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=4)\n    features['rolling-mean-window-4'] = list(col.rolling(window=indexer, min_periods=1).mean())\n    return features\n\n\ndef fix_mode(d):\n    if isinstance(d['mode'], pd.Series):\n        d['mode'] = d['mode'].loc[0].item()\n    return d\n\n\ndef split_meta_features(d):\n    return pd.Series(\n        [d.get('std', \"N/A\"), d.get('mean', \"N/A\"), d.get('median', \"N/A\"), d.get('mode', \"N/A\"), d.get('max', \"N/A\"),\n         d.get('min', \"N/A\")])\n\n\ndef prompt_context_insert(context_labels: str, context: str, max_len: int = 2000, model: str = \"gpt-3.5\", options: list[str] = None):\n    if model == \"bloomz\":\n        s = f'SYSTEM: You are an AI research assistant. You use a tone that is technical and scientific. USER: Please select the field from {context_labels} which best describes the context below. Respond with the name of the field and nothing else. \\n CONTEXT: {context}'\n    elif model == \"gpt-3.5\":\n        s = f'SYSTEM: Please select the field from {context_labels} which best describes the context. Respond only with the name of the field. \\n CONTEXT: {context}'\n    elif model == \"ada-personal\":\n        s = f'{context}$'\n    elif model == \"llama-old\":\n        s = f'INSTRUCTION: Select the field from the category which matches the input. \\n CATEGORIES: {context_labels} \\n INPUT:{context} \\n OUTPUT: '\n    elif \"-zs\" in model:\n        ct = \"[\" + \", \".join(context).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")[\n                   :MAX_LEN - 100 - len(context_labels)] + \"]\"\n        lb = \"\\n\".join([\"- \" + c for c in context_labels.split(\", \")])\n        #s = f'How might one classify the following input? \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n ANSWER:'\n        if model == \"opt-iml-max-30b-zs\":\n            s = f'Select the option which best describes the input. \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n'\n        else:\n            # Original prompt\n            s = f'INSTRUCTION: Select the option which best describes the input. \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n ANSWER:'\n            \n    elif model == \"llama\":\n        ct = \"[\" + \", \".join(context).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")[\n                   :MAX_LEN - 100 - len(context_labels)] + \"]\"\n        lb = \"\\n\".join([\"- \" + c for c in context_labels.split(\", \")])\n        \n        #s = f'INSTRUCTION: Select the category which best matches the input. \\n INPUT:{context} .\\n OPTIONS:\\n{lb} \\n CATEGORY: '\n\n        #s = f'INSTRUCTION: Select the category which best matches the input. If category is not matching the input return none. Do not provide any further text, only label if it exists or none. \\n INPUT:{context} .\\n OPTIONS:\\n - none \\n CATEGORY: '\n        #opt_set = options + [\"none\"]\n        \n        options_str = \" - \".join(options)\n        \n        #if options is not None:\n        #    options_str = \" - \".join(options)\n        #else:\n        #    options_str = \"- none\"\n            \n        print(f\"Prompt context insert {options_str}\")\n\n        # Picker 1 prompt\n        s = picker_prompt = llm_prompts(ct, options_str)\n\n    elif model == \"llama-retry\":\n        s = f'INSTRUCTION: Select the category which best matches the input. \\n INPUT:{context} \\n CATEGORY: '\n    #Truncate if prompt exceeds maximum length\n    if len(s) > max_len:\n        s = s[:max_len - 3]\n\n    return s\n\n\ndef recompute_results(prompt_dict, prompt, model_str, cbc_pred, label_set):\n    dict_val = prompt_dict.get(prompt, -1)\n    dict_val['cbc_pred'] = cbc_pred\n    if model_str == \"llama\":\n        if cbc_pred and (cbc_pred in catboost_cats):\n            print(f\"using cbcpred label: {cbc_pred} \\n\")\n            dict_val['response'] = fix_labels(cbc_pred, label_set)\n        dict_val['correct'] = ((dict_val['ground_truth'] == dict_val['response']) or (\n                    dict_val['response'] and (dict_val['response']) in dict_val['ground_truth']))\n    prompt_dict[prompt] = dict_val\n\n\ndef make_json(prompt, var_params):\n    p = deepcopy(params)\n    if var_params:\n        for k, v in var_params.items():\n            p[k] = v\n    return {\n        \"data\": [\n            prompt,\n            p['max_new_tokens'],\n            p['do_sample'],\n            p['temperature'],\n            p['top_p'],\n            p['typical_p'],\n            p['repetition_penalty'],\n            p['encoder_repetition_penalty'],\n            p['top_k'],\n            p['min_length'],\n            p['no_repeat_ngram_size'],\n            p['num_beams'],\n            p['penalty_alpha'],\n            p['length_penalty'],\n            p['early_stopping'],\n            p['seed'],\n        ]\n    }\n\n\ndef ans_contains_gt(ans_n, fixed_labels):\n    for fixed_label in fixed_labels:\n        if fixed_label in ans_n:\n            print(f\"Fuzzy label {ans_n} contains gt label {fixed_label}: MATCH \\n\")\n            ans_n = fixed_label\n            return ans_n\n    return None\n\n\ndef gt_contains_ans(ans_n, fixed_labels):\n    if ans_n == \"\":\n        return None\n    for fixed_label in fixed_labels:\n        if ans_n in fixed_label:\n            print(f\"GT label {fixed_label} contains fuzzy label {ans_n}: MATCH \\n\")\n            ans_n = fixed_label\n            return ans_n\n    return None\n\n\ndef basic_contains(ans_n, fixed_labels, method):\n    #TODO: not sure the order should be fixed like this, could be made flexible\n    if ans_n in fixed_labels:\n        return ans_n\n    if \"ans_contains_gt\" in method:\n        res = ans_contains_gt(ans_n, fixed_labels)\n        if res:\n            return res\n    if \"gt_contains_ans\" in method:\n        res = gt_contains_ans(ans_n, fixed_labels)\n        if res:\n            return res\n    return None\n\n\ndef fuzzy_label_match(orig_ans, fixed_labels, session, link, prompt, lsd, model,\n                      method=[\"ans_contains_gt\", \"gt_contains_ans\", \"resample\"]):\n\n    #answer is already in label set, no fuzzy match needed\n    ans_n = fix_labels(orig_ans, lsd)\n    res = basic_contains(ans_n, fixed_labels, method)\n    if res:\n        return res\n    if \"similarity\" in method:\n        ans_embedding = sent_model.encode(ans_n)\n        lbl_embeddings = sent_model.encode(fixed_labels)\n        sims = {lbl: util.pytorch_cos_sim(ans_embedding, le) for lbl, le in zip(fixed_labels, lbl_embeddings)}\n        return max(sims, key=sims.get)\n    if \"resample\" in method:\n        #fuzzy label matching strategy\n        for k in range(2, 6):\n            if \"gpt\" in model:\n                ans_n = openai.ChatCompletion.create(\n                    model=\"gpt-3.5-turbo\",\n                    messages=[\n                        {\"role\": \"user\", \"content\": prompt},\n                    ],\n                    temperature=0 + k / 10,\n                ).choices[0]['message']['content'].lower()\n            elif model in [\"llama-zs\", \"opt-iml-30b-zs\"]:\n                pipe, local_llm, llm_chain = set_pipeline(k=k)\n                ans_n = llm_chain.run(prompt)\n            elif model in [\"topp-zs\", \"flan-ul2-zs\"]:\n                ans_n = get_topp_resp(prompt, k)\n            else:\n                rep_pen = params['repetition_penalty']\n                top_p = params['top_p']\n                temp = params['temperature']\n                ans_n = call_llama_model(session, link, prompt, lsd,\n                                         {'no_repeat_ngram_size': 1, 'top_p': top_p - (0.1 * k), 'temperature': 0.9})\n                params['top_p'] = top_p\n                params['temperature'] = temp\n            res = basic_contains(ans_n, fixed_labels, method)\n            if res:\n                return res\n    #print(\"Applying fallback label, 'text' \\n\")\n    return 'text'\n\n\nINTEGER_SET = set(r\"0123456789,/\\+-.^_()[] :\")\n\n\ndef get_base_dtype(context):\n    dtype = \"integer\"\n    for item in context:\n        if not all(char in INTEGER_SET for char in item):\n            #print(f\"String is OTHER because: {[char for char in item if char not in INTEGER_SET]}\")\n            return \"other\"\n        try:\n            if item.endswith(\".0\") or item.endswith(\",0\"):\n                item = item[:-2]\n                item = str(int(item))\n            if item.endswith(\".00\") or item.endswith(\",00\"):\n                item = item[:-3]\n                item = str(int(item))\n        except:\n            return \"float\"\n        temp_item = re.sub(r\"[^a-zA-Z0-9.]\", \"\", item)\n        if not temp_item.isdigit():\n            #print(f\"string is FLOAT because {temp_item} is not an integer\")\n            dtype = \"float\"\n    return dtype\n\n\ndef query_correct_model(model, prompt, context_labels, context, session, link, lsd, generated_labels_list = None):\n    original_model_response = \"\"\n    if model in [\"llama-zs\", \"opt-iml-max-30b-zs\"]:\n        orig_ans = llm_chain.run(prompt)\n        if orig_ans is None:\n            prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"llama-retry\")\n            orig_ans = llm_chain.run(prompt)\n    elif model in [\"topp-zs\", \"flan-ul2-zs\"]:\n        orig_ans = get_topp_resp(prompt, 1)\n    else:\n        orig_ans, original_model_response = call_llama_model(session, link, prompt, lsd, None, generated_labels_list)\n        if orig_ans is None:\n            prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"llama-retry\")\n            orig_ans, original_model_response = call_llama_model(session, link, prompt, lsd, None, generated_labels_list)\n    return orig_ans, original_model_response\n\n\ndef get_df_sample_col(col, rand_seed, len_context, min_variance=2, replace=False):\n    df = pd.Series(col)\n    ignore_list = [\"None\", 'none', 'NaN', 'nan', 'N/A', 'na', '']\n    sample_list = list(set(p[:75] for p in pd.unique(df.astype(str)[col]) if p not in ignore_list))\n    if len(sample_list) < 1:\n        return [\"None\"] * len_context\n    if len(sample_list) < len_context:\n        sample_list = sample_list * len_context\n    if len(sample_list) > len_context:\n        sample_list = sample_list[:len_context]\n    assert len(sample_list) == len_context, f\"An index in val_indices is length {len(sample_list)}\"\n    return sample_list\n\n\ndef check_substr_contains_only_set(str, acceptable_chars):\n    validation = set(str)\n    print(\"Checking if it contains only \", acceptable_chars)\n    if validation.issubset(acceptable_chars):\n        return True\n    else:\n        return False\n\n\ndef insert_source(context, fname):\n    pattern = r\"_([^_]*)_\"  # Matches substrings that start and end with \"_\"\n    matcher = re.search(pattern, fname)\n    addstr = str(matcher.group()).replace(\"_\", \"\").split(\".\")[0]\n    #context.insert(0, \"SRC_FILE: \" + addstr + \"COL_VALS: \")\n    context.insert(0, \"SRC: \" + addstr)\n    return context\n\n\ndef get_df_sample(df, rand_seed, val_indices, len_context, min_variance=1, replace=False, full=False, other_col=False,\n                  max_len=8000):\n    column_samples = {}\n    ignore_list = [\"None\", 'none', 'NaN', 'nan', 'N/A', 'na', '']\n    for col in df.columns:\n        sample_list = list(\n            set(p[:max_len // (len_context * 3)] for p in pd.unique(df.astype(str)[col]) if p not in ignore_list))\n        #reformat integer samples\n        sl_mod = []\n        # Meta-features\n        if full:\n            meta_features = derive_meta_features(df[col])\n            meta_features['rolling-mean-window-4'] = meta_features['rolling-mean-window-4'][:5]\n        # Sampling from other columns\n        if other_col:\n            sample_list_fill_size = len_context - len(sample_list)\n            nc = len(df.columns)\n            per_column_context = max(1, sample_list_fill_size // nc)\n            for idx, oc in enumerate(df.columns):\n                items = df[oc].astype(str).iloc[0:per_column_context].tolist()\n                sample_list = sample_list + [\"OC: \" + str(item) for item in items]\n        if not sample_list:\n            sample_list = [\"None\"]\n        if len(sample_list) < len_context:\n            sample_list = sample_list * len_context\n        if len(sample_list) > len_context:\n            sample_list = sample_list[:len_context]\n        assert len(sample_list) == len_context, \"An index in val_indices is length \" + str(len(sample_list))\n        if full:\n            if meta_features['std'] == \"N/A\":\n                sample_list = sample_list + [\"\" for k, v in meta_features.items()]\n            else:\n                sample_list = sample_list + [str(k) + \": \" + str(v) for k, v in meta_features.items()]\n        # print(\"sample list\")\n        # print(sample_list)\n        column_samples[col] = sample_list\n        # print(\"column samples\")\n        # print(column_samples)\n    return pd.DataFrame.from_dict(column_samples)\n\n\nNUMERIC_AND_COMMA = set('0123456789,')\n\nBOOLEAN_SET = [\"True\", \"true\", \"False\", \"false\", \"yes\", \"Yes\", \"No\", \"no\"]\n\n\ndef apply_basic_rules(context, lbl):\n    if not context:\n        return lbl\n    if not isinstance(context, list):\n        return lbl\n    try:\n        if all(s.endswith(\" g\") for s in context):\n            lbl = \"weight\"\n        if all(s.endswith(\" kg\") for s in context):\n            lbl = \"weight\"\n        if all(s.endswith(\" lb\") for s in context):\n            lbl = \"weight\"\n        if all(s.endswith(\" lbs\") for s in context):\n            lbl = \"weight\"\n        if all(s.endswith(\" pounds\") for s in context):\n            lbl = \"weight\"\n        if all(s.endswith(\" cal\") for s in context):\n            lbl = \"calories\"\n        if all(s.endswith(\" kcal\") for s in context):\n            lbl = \"calories\"\n        if all(s.endswith(\" calories\") for s in context):\n            lbl = \"calories\"\n        if all(\"review\" in s.lower() for s in context):\n            lbl = \"review\"\n        if all(\"recipe\" in s.lower() for s in context):\n            lbl = \"recipe\"\n        if lbl and \"openopen\" in lbl:\n            lbl = \"openinghours\"\n        if all(s in BOOLEAN_SET for s in context):\n            lbl = \"medical_boolean\"\n        return lbl\n    except Exception as e:\n        print(f\"Exception {e} in apply_basic_rules with context {context}\")\n        return lbl\n\n\ndef get_cbc_pred(orig_label, numeric_labels):\n    try:\n        #FOR VALIDATION\n        #cbc_filematch = dfv[dfv['df_path'] == str(f)]\n        #FOR TEST SET\n        cbc_filematch = dft[dft['df_path'] == str(f)]\n        cbc_labelmatch = cbc_filematch[cbc_filematch['label'] == orig_label]\n        if len(cbc_labelmatch) == 1:\n            cbc_pred = numeric_labels[cbc_labelmatch['preds'].item()]\n        else:\n            cbc_pred = None\n    except Exception as e:\n        print(\"cbc excpetion: \")\n        print(e)\n        cbc_pred = None\n\n\ndef run_val(model: str, save_path: str, inputs: list, label_set: list, input_df: pd.DataFrame, resume: bool = True,\n            results: bool = True, stop_early: int = -1, rand_seed: int = 13, sample_size: int = 5, link: str = None,\n            response: bool = True, summ_stats: bool = False, table_src: bool = False, other_col: bool = False,\n            skip_short: bool = False, min_var: int = 0, method: list = [\"similarity\"]):\n    inputs = [Path(f) for f in inputs]\n\n    infmods = \"sherlock\" in model or \"doduo\" in model\n    isd4 = \"d4\" in label_set['name']\n    if resume and os.path.isfile(save_path):\n        with open(save_path, 'r', encoding='utf-8') as f:\n            prompt_dict = json.load(f)\n    else:\n        prompt_dict = {}\n    s = requests.Session()\n    if \"-zs\" in model:\n        base_model.eval()\n    if isinstance(inputs, dict):\n        labels = [\"_\".join(k.split(\"_\")[:-1]) for k in inputs.keys()]\n        inputs = list(inputs.values())\n    for idx, f in tqdm(enumerate(inputs), total=len(inputs)):\n        if idx % 100 == 0:\n            with open(save_path, 'w', encoding='utf-8') as alt_f:\n                #print(\"pd\", prompt_dict, \"\\n\")\n                json.dump(prompt_dict, alt_f, ensure_ascii=False, indent=4)\n        if stop_early > -1 and idx == stop_early:\n            break\n        if isd4:\n            f_df = f\n            label_indices = [2]\n            gt_labels = labels[idx]\n        else:\n            gt_labels = input_df[input_df['table_name'] == f.name]\n            label_indices = gt_labels['column_index'].unique().tolist()\n\n            if f.suffix.lower() == '.csv':\n                f_df = pd.read_csv(f)\n            else:\n                f_df = pd.read_json(f, compression='infer', lines=True)\n\n        if infmods:\n            label_indices = [\"values\"]\n            key = get_sherlock_resp(f_df, gt_labels, prompt_dict, model, label_indices, str(f), label_set)\n            continue\n        sample_df = get_df_sample(f_df, rand_seed, label_indices, sample_size, full=summ_stats, other_col=other_col,\n                                  max_len=MAX_LEN)\n        #print(f\"in main loop, sample_df is {sample_df}\")\n        f_df_cols = f_df.columns\n        for idx, col in enumerate(f_df_cols):\n            if idx not in label_indices:\n                continue\n            #NOTE: skipping evaluation for columns with insufficient variance in the column\n            #       if len(pd.unique(sample_df.astype(str)[col])) < min_var:\n            #         continue\n            if isd4:\n                orig_label = gt_labels\n            else:\n                gt_row = gt_labels[gt_labels['column_index'] == idx]\n                orig_label = gt_row['label'].item()\n            label = fix_labels(orig_label, label_set)\n            limited_context = sample_df[col].tolist()[:sample_size]\n            #NOTE: could consider using min_var here\n            #if full and len(pd.unique(sample_df[col].tolist())) < 3:\n            if table_src:\n                context = insert_source(sample_df[col].tolist(), f.name)\n            else:\n                context = sample_df[col].tolist()\n            if \"gpt-3.5\" in model:\n                key = get_chatgpt_resp(label_set, context, label, prompt_dict, response=response, session=s,\n                                       method=method)\n            elif \"ada-personal\" in model:\n                key = get_ada_resp(label_set, context, label, prompt_dict, response=response, session=s)\n            elif \"bloomz\" in model:\n                key = get_bloomz_resp(label_set, context, label, prompt_dict, response=response, session=s)\n            elif \"llama\" in model or \"-zs\" in model:\n                #cbc_pred = get_cbc_pred(orig_label, numeric_labels)\n                cbc_pred = None\n                key = get_llama_resp(label_set, context, label, prompt_dict, link=link, response=response, session=s,\n                                     cbc=cbc_pred, model=model, limited_context=limited_context, method=method)\n                # print(\"Key: \", key, \"\\n\")\n                #print(\"pdk\", prompt_dict[key], \"\\n\")\n            prompt_dict[key]['original_label'] = orig_label\n            prompt_dict[key]['file+idx'] = str(f) + \"_\" + str(idx)\n    with open(save_path, 'w', encoding='utf-8') as my_f:\n        json.dump(prompt_dict, my_f, ensure_ascii=False, indent=4)\n    if results:\n        results_checker(save_path, skip_duplicates=False)",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "392f663e-4f8f-4671-ab88-21575e2fffbd",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:40.100317Z",
     "start_time": "2025-08-19T11:36:40.098483Z"
    }
   },
   "source": "from sklearn.metrics import classification_report",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "02993b81-f6b9-4b7b-b14f-a0ba901faef5",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:40.206807Z",
     "start_time": "2025-08-19T11:36:40.187693Z"
    }
   },
   "source": "import json\nfrom statistics import mean\n\nENDINGS = [\"ANSWER:\", \"CATEGORY:\"]\n\n\ndef results_checker_doduo(file_name, skip_duplicates=True):\n    with open(file_name, \"r\") as f:\n        d = json.load(f)\n    correct = 0\n    n = len(d)\n    per_class_results = dict()\n    for k, v in d.items():\n        response_set = set(v[\"response\"])\n        for r in response_set:\n            per_class_results.setdefault(r, {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"Total\": 0})\n        per_class_results.setdefault(v[\"ground_truth\"], {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"Total\": 0})\n        if v['correct'] == True:\n            correct += 1\n            per_class_results[v[\"ground_truth\"]][\"TP\"] += 1\n        else:\n            per_class_results[v[\"ground_truth\"]][\"FN\"] += 1\n            for r in response_set:\n                per_class_results[r][\"FP\"] += 1\n        per_class_results[v[\"ground_truth\"]][\"Total\"] += 1\n\n    for k, v in per_class_results.items():\n        v['F1'] = (2 * v[\"TP\"]) / (2 * v[\"TP\"] + v[\"FP\"] + v[\"FN\"])\n\n    weighted_f1 = sum([v[\"F1\"] * v[\"Total\"] for k, v in per_class_results.items()]) / n\n    unweighted_f1 = mean([v[\"F1\"] for k, v in per_class_results.items()])\n\n    print(\n        f\"Total entries: {n} \\n Accuracy: {round(correct / n, 4)} \\n Weighted F1: {round(weighted_f1, 4)} \\n Unweighted F1: {round(unweighted_f1, 4)}\")\n\n\ndef results_checker(file_name, skip_duplicates=True):\n    with open(file_name, \"r\") as f:\n        d = json.load(f)\n\n    if skip_duplicates:\n        d = {k: v for k, v in d.items() if \"CATEGORY: *\" not in str(k)}\n\n    # build the lists\n    y_true = [v[\"ground_truth\"] for v in d.values()]\n    y_pred = [v[\"response\"] for v in d.values()]\n\n    # overall stats\n    correct = sum(1 for gt, pred in zip(y_true, y_pred) if gt == pred)\n    n = len(y_true)\n    print(f\"Total entries: {n}\")\n    print(f\"Accuracy:     {correct / n:.4f}\\n\")\n\n    # per-class report\n    print(classification_report(\n        y_true,\n        y_pred,\n        digits=4,  # 4 decimal places\n        zero_division=0  # to avoid warnings if a class is never predicted\n    ))\n\n    # --- new: build a flattened metrics dict ---\n    raw_report = classification_report(\n        y_true, y_pred,\n        output_dict=True,\n        zero_division=0\n    )\n\n    flat = {}\n    # raw_report has keys for each label, plus 'macro avg', 'weighted avg', and 'accuracy'\n    for label, m in raw_report.items():\n        if label == \"accuracy\":\n            flat[\"accuracy\"] = m\n        else:\n            for metric_name, val in m.items():\n                flat[f\"{label}_{metric_name}\"] = val\n\n    # add summary fields\n    flat[\"total_entries\"] = n\n    # filename identifier: take it from your JSON filename variable\n    flat[\"run_name\"]      = os.path.basename(file_name).replace(\".json\",\"\")\n\n    # convert to one-row DataFrame\n    df = pd.DataFrame([flat])\n\n    metrics_csv = f\"{archetype_directory}/all_metrics.csv\"\n\n    # append (or create) the master CSV\n    if not os.path.isfile(metrics_csv):\n        df.to_csv(metrics_csv, index=False, float_format=\"%.4f\")\n    else:\n        df.to_csv(metrics_csv, mode=\"a\", header=False, index=False, float_format=\"%.4f\")\n\n    return df\n",
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "8db2960f-ac1d-4199-9cbc-85b6d60f66e2",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:40.312229Z",
     "start_time": "2025-08-19T11:36:40.307347Z"
    }
   },
   "source": "def missing_entries(f1, f2):\n    with open(f1, \"r\") as file1:\n        d1 = json.load(file1)\n    with open(f2, \"r\") as file2:\n        d2 = json.load(file2)\n    paths1 = set([v[\"file+idx\"] for _, v in d1.items()])\n    paths2 = set([v[\"file+idx\"] for _, v in d2.items()])\n    return paths1 - paths2",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "23213247-0e7a-4b24-9eb5-a623264183ae",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:40.400159Z",
     "start_time": "2025-08-19T11:36:40.377449Z"
    }
   },
   "source": "def run_val_parquet(\n        model: str,\n        save_path: str,\n        labels_path: str,\n        data_path: str,\n        label_set: dict,\n        resume: bool = True,\n        results: bool = True,\n        stop_early: int = -1,\n        rand_seed: int = 13,\n        sample_size: int = 5,\n        link: str = None,\n        response: bool = True,\n        summ_stats: bool = False,\n        table_src: bool = False,\n        other_col: bool = False,\n        skip_short: bool = False,\n        min_var: int = 0,\n        method: list = [\"similarity\"],\n        results_checker=None,\n        MAX_LEN: int = 1000\n):\n    \"\"\"\n    Validation loop adapted for parquet-based inputs:\n\n    - labels_path: path to a parquet file with columns ['__index_level_0__', 'type']\n    - data_path:   path to a parquet file with columns ['__index_level_0__', 'values']\n    - label_set:   dict containing 'name', 'label_set', 'dict_map', 'abbrev_map'\n\n    Each row in the merged DataFrame represents one column to predict:\n      - __index_level_0__ (column index)\n      - type (ground truth label)\n      - values (comma-separated or list of column values)\n    \"\"\"\n\n    total_inference_time = 0.0\n    inference_times = []\n\n    # llm output directory\n    llm_response_output_path = os.path.join(run_all_directory, \"temp-results\", \"llm-outputs.txt\")\n\n    # if directory exist\n    os.makedirs(os.path.dirname(llm_response_output_path), exist_ok=True)\n\n    # remove any existing files in the directory\n    if os.path.isfile(llm_response_output_path):\n        os.remove(llm_response_output_path)\n\n    generated_labels_list = []\n        \n    \n    # Load or initialize cache\n    if resume and os.path.isfile(save_path):\n        with open(save_path, 'r', encoding='utf-8') as f:\n            prompt_dict = json.load(f)\n    else:\n        prompt_dict = {}\n    \n\n    # Read parquet inputs and bring index into a column\n    # Read parquet inputs and bring index into a column\n    labels_df = pd.read_parquet(labels_path).reset_index()\n    data_df = pd.read_parquet(data_path).reset_index()\n\n    # Identify the index column name (either __index_level_0__ or generic index)\n    labels_idx_col = '__index_level_0__' if '__index_level_0__' in labels_df.columns else 'index'\n    data_idx_col = '__index_level_0__' if '__index_level_0__' in data_df.columns else 'index'\n\n    # Rename for clarity: index → col_idx, type → label, values stays values\n    labels_df = labels_df.rename(columns={labels_idx_col: 'col_idx', 'type': 'label'})\n    data_df = data_df.rename(columns={data_idx_col: 'col_idx', 'values': 'values'})\n\n    # Remap labels using LABEL_MAP_LC\n    # assumes remap_labels(series, mapping) is defined and LABEL_MAP_LC is available\n\n    #labels_df['label'] = remap_labels(labels_df['label'], LABEL_MAP_LC)\n\n    # Filter out __none__ labels\n    labels_df = labels_df[labels_df['label'] != \"__none__\"]\n\n    # Merge on column index\n    merged = pd.merge(labels_df, data_df, on='col_idx', how='inner')\n\n    # Prepare session and model\n    session = requests.Session()\n    if \"-zs\" in model:\n        base_model.eval()\n\n    # Iterate over each column instance\n    for idx, row in tqdm(enumerate(merged.itertuples(index=False)), total=len(merged)):\n        \n        # Periodic cache save\n        if idx % 100 == 0:\n            with open(save_path, 'w', encoding='utf-8') as f:\n                json.dump(prompt_dict, f, ensure_ascii=False, indent=4)\n        \n        if stop_early > -1 and idx == stop_early:\n            break\n\n        col_idx = row.col_idx\n        orig_label = row.label\n        raw_vals = row.values\n\n        # Parse raw values into a list\n        if isinstance(raw_vals, str):\n            vals = raw_vals.split(',')\n        else:\n            vals = list(raw_vals)\n\n        # Deduplicate and sample\n        vals = [str(x) for x in vals]\n        unique_vals = pd.unique(vals)\n        context_list = unique_vals.tolist()[:sample_size]\n\n        # Build context\n        if table_src:\n            context = insert_source(context_list, str(col_idx))\n        else:\n            context = context_list\n\n        start_time = time.perf_counter()\n        # Model call\n        if \"gpt-3.5\" in model:\n            key = get_chatgpt_resp(label_set, context, orig_label,\n                                   prompt_dict, response=response,\n                                   session=session, method=method)\n        elif \"ada-personal\" in model:\n            key = get_ada_resp(label_set, context, orig_label,\n                               prompt_dict, response=response,\n                               session=session)\n        elif \"bloomz\" in model:\n            key = get_bloomz_resp(label_set, context, orig_label,\n                                  prompt_dict, response=response,\n                                  session=session)\n        else:\n            raw_prompt, answer, original_model_response = get_llama_resp(label_set, context, orig_label,\n                                 prompt_dict, link=link,\n                                 response=response,\n                                 session=session,\n                                 cbc=None,\n                                 model=model,\n                                 limited_context=context_list,\n                                 method=method,\n                                 generated_labels_list = generated_labels_list)\n        \n        end_time = time.perf_counter()\n        # Record metadata\n        prompt_dict[raw_prompt]['original_label'] = orig_label\n        prompt_dict[raw_prompt]['file+idx'] = str(col_idx)\n\n        inference_time = end_time - start_time\n        inference_times.append(inference_time)\n        total_inference_time += inference_time\n\n        # but you also get to see the actual label answer:\n        print(\"PROMPT SENT:\\n\", raw_prompt)\n        print(\"MODEL ANSWER:\", answer)\n\n        with open(llm_response_output_path, \"a\", encoding='utf-8') as f:\n            f.write(f\"---\\n\")\n            f.write(f\"PROMPT SENT: {raw_prompt}\\n\")\n            f.write(f\"Original model answer: {original_model_response}\\n\")\n            f.write(f\"Final ANSWER: {answer}\\n\")\n\n    n_calls = len(inference_times)\n    if n_calls > 0:\n        print(f\"Total inference time: {total_inference_time:.2f}s over {n_calls} calls\")\n        print(f\"  → average per call: {total_inference_time/n_calls:.3f}s\")\n        print(f\"  → min / max per call: {min(inference_times):.3f}s / {max(inference_times):.3f}s\")\n\n    # Final cache save\n    with open(save_path, 'w', encoding='utf-8') as f:\n        json.dump(prompt_dict, f, ensure_ascii=False, indent=4)\n\n    # Optional result summary\n    #if results and results_checker is not None:\n    #    results_checker(save_path, skip_duplicates=False)\n\n    return prompt_dict, total_inference_time",
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "dbb07a01-53a9-4639-b197-fb24a0479456",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:40.669088Z",
     "start_time": "2025-08-19T11:36:40.665321Z"
    }
   },
   "source": "label_set = {\n  \"name\": \"custom_csv\",     # any string that does NOT contain \"d4\"\n  \"label_set\": LABELS,      # the list of your labels, used by similarity\n  \"dict_map\": { lab: lab for lab in LABELS },\n  \"abbrev_map\": {}          # or your real abbrev map\n}",
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "9cd83aaa-8825-403e-9f26-b62e8fd0765e",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:41.307290Z",
     "start_time": "2025-08-19T11:36:41.244460Z"
    }
   },
   "source": "#model_name = \"alpaca-fine-tuned\"\n#model_name = \"flan-ul2\"\n#model_name = \"alpaca-13b\"\n\nmodel_name=\"llama\"\n\nfilename = f\"custom-data-{model_name}-label-generation-{prompt_type}-{tune}.json\"\n\nsp = f\"{archetype_directory}/custom_data_logs/{filename}\"\n\n\ndirpath = os.path.dirname(sp)\nos.makedirs(dirpath, exist_ok=True)\n\n#model_name = \"flan-t5-xxl\"\n\n#base_model, tokenizer, template, pt, MAX_LEN = init_model(model_name)\n\n# Test set\n#labels_path = \"/home/omadbek/projects/Sherlock/custom_data/label_generation/test_labels_generation.parquet\"\n#data_path = \"/home/omadbek/projects/Sherlock/custom_data/label_generation/test_data_generation.parquet\"\n\n\n# LLAMA\nprompt_dict, inference_time = run_val_parquet(\n    model=\"llama\",\n    save_path=sp,\n    labels_path=labels_path,\n    data_path=data_path,\n    label_set=label_set,\n    method=[\"similarity\"],\n    resume=False,\n    sample_size=5,\n    #stop_early = 30,\n    link = \"http://localhost:11434/api/generate\"\n)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1c2dca54dc1445cb7ac7feb2204e810"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "fe4311a83efa0299",
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:36:42.072241Z",
     "start_time": "2025-08-19T11:36:42.067198Z"
    }
   },
   "source": "if os.path.exists(sp):\n    print(f\"✅ File exists: {sp}\")\nelse:\n    print(f\"❌ File not found: {sp}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File exists: /home/omadbek/projects/ArcheType/custom_data_logs/custom-data-llama-label-generation-zero-shot-0.0.json\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "2c18a5cc-b04b-4450-a816-0d60a5e2a173",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:56:19.639067Z",
     "start_time": "2025-08-10T12:56:19.629874Z"
    },
    "trusted": false
   },
   "source": "from collections import Counter\n\n\n# Fuzzy matcher\n\n# --- Evaluation function ---\ndef evaluate_and_remap(\n    file_name: str,\n    embed_threshold: float = 0.25, # cos_threshold (semantic)\n    #fuzz_threshold: int = 25 # fuzz_threshold (lexical)\n):\n    \"\"\"\n    Loads your JSON, uses fuzzy matching to snap each 'response' to the closest\n    ground-truth label (if similar), and counts fuzzy-correct matches.\n    Adds two new fields to each record in the loaded dict:\n      - 'resolved_response': the matched label or original response\n      - 'correct_fuzzy': bool, whether resolved_response == ground_truth\n    Returns the updated data dict and the fuzzy accuracy.\n    \"\"\"\n    # 1) Load JSON\n    with open(file_name, 'r') as f:\n        data = json.load(f)\n\n    # 2) Build matcher over your ground-truth set\n    label_set = sorted({v[\"ground_truth\"] for v in data.values()})\n    matcher = _CosineLabelMatcher(label_set, embed_threshold)\n\n    # 3) Remap each response & compute fuzzy correctness\n    total = 0\n    fuzzy_correct = 0\n    for record in data.values():\n        total += 1\n        orig = record[\"response\"]\n        match = matcher.resolve(orig)\n        resolved = match if match is not None else orig\n        record[\"resolved_response\"] = resolved\n        is_correct = (resolved == record[\"ground_truth\"])\n        record[\"correct_fuzzy\"] = is_correct\n        if is_correct:\n            fuzzy_correct += 1\n\n    # TEST\n    # 4) Build truth and pred lists\n    y_true = []\n    y_pred = []\n    for rec in data.values():\n        y_true.append(rec[\"ground_truth\"])\n        y_pred.append(rec[\"resolved_response\"])\n\n    # 5) Print out class‐support (how many of each ground truth label)\n    #support = Counter(y_true)\n    #print(\"Class counts (support):\")\n    #for label, cnt in support.items():\n    #    print(f\"  {label:15s} → {cnt}\")\n    #print()\n\n    # 6) Classification report (precision/recall/F₁ + support)\n    print(\"Per-class precision / recall / F₁ / support:\")\n    print(classification_report(\n        y_true,\n        y_pred,\n        #labels=sorted(support),   # ensures a fixed order\n        digits=4,\n    ))\n\n\n    raw_report = classification_report(\n        y_true, y_pred,\n        output_dict=True,\n        zero_division=0\n    )\n\n\n\n    # 7) Macro / micro F₁\n    #macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n    #micro_f1 = f1_score(y_true, y_pred, average=\"micro\")\n    #print(f\"Macro-average F₁: {macro_f1:.4f}\")\n    #print(f\"Micro-average F₁: {micro_f1:.4f}\")\n\n    # 4) Print fuzzy accuracy\n    acc = fuzzy_correct / total if total else 0.0\n    total_correct = f\"{fuzzy_correct}/{total}\" \n    print(f\"Fuzzy‐matched correct: {total_correct}  →  Accuracy: {acc:.4f}\")\n\n    return total_correct, acc, total, raw_report",
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "d33e0466-abe0-4074-b1a0-4c25c3302673",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:57:03.166426Z",
     "start_time": "2025-08-10T12:56:19.705136Z"
    },
    "trusted": false
   },
   "source": "total_correct, fuzzy_acc, total, raw_report = evaluate_and_remap(sp)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Per-class precision / recall / F₁ / support:\n                 precision    recall  f1-score   support\n\n            age     0.0000    0.0000    0.0000         3\n    case_status     0.0000    0.0000    0.0000         2\ncontact_setting     0.0000    0.0000    0.0000         1\n           date     1.0000    0.5833    0.7368        12\n         gender     0.0000    0.0000    0.0000         2\n             id     0.0000    0.0000    0.0000         5\n       location     0.0000    0.0000    0.0000         6\nmedical_boolean     0.4318    0.9500    0.5938        20\n     occupation     0.0000    0.0000    0.0000         1\n        outcome     0.0000    0.0000    0.0000         2\n       symptoms     0.0000    0.0000    0.0000         1\n\n       accuracy                         0.4727        55\n      macro avg     0.1302    0.1394    0.1210        55\n   weighted avg     0.3752    0.4727    0.3767        55\n\nFuzzy‐matched correct: 26/55  →  Accuracy: 0.4727\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/omadbek/.conda/envs/archetype/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/omadbek/.conda/envs/archetype/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/omadbek/.conda/envs/archetype/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:57:03.322264Z",
     "start_time": "2025-08-10T12:57:03.320359Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "",
   "id": "5e465bfe25954645",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a658307-6081-4049-a4f2-01da06172f95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:23.812692Z",
     "start_time": "2025-08-10T12:57:03.400828Z"
    },
    "trusted": false
   },
   "source": "import json, re, pandas as pd\n#from rapidfuzz import fuzz\nfrom sentence_transformers import SentenceTransformer, util\n\n# load data\ndata = json.load(open(sp))\nrows = []\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\nfor rec in data.values():\n    gt   = rec[\"ground_truth\"]\n    resp = rec[\"response\"]\n    # fuzzy\n    #fuzz_score = fuzz.token_sort_ratio(resp, gt)\n    # cosine\n    emb_r = model.encode([resp], normalize_embeddings=True)\n    emb_g = model.encode([gt],   normalize_embeddings=True)\n    cos_score = util.cos_sim(emb_r, emb_g)[0][0].item()\n    rows.append({\n        \"ground_truth\": gt,\n        \"response\":     resp,\n        #\"fuzz_score\":   fuzz_score,\n        \"cos_score\":    cos_score\n    })\n\ndf = pd.DataFrame(rows)",
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "4b1ac587-8505-46da-88d1-a04bbe3dec87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:23.886365Z",
     "start_time": "2025-08-10T12:58:23.876672Z"
    },
    "trusted": false
   },
   "source": "# Put to txt file\nprint(df)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "       ground_truth                           response  cos_score\n0                id  patient_ids_with_duplicate_values   0.213535\n1   medical_boolean                    medical_boolean   1.000000\n2           outcome                     patient_status   0.282859\n3   medical_boolean                    medical_boolean   1.000000\n4              date  patient_ids_with_duplicate_values   0.073634\n5              date  patient_ids_with_duplicate_values   0.073634\n6              date  patient_ids_with_duplicate_values   0.073634\n7              date  patient_ids_with_duplicate_values   0.073634\n8   medical_boolean  patient_ids_with_duplicate_values   0.285567\n9              date  patient_ids_with_duplicate_values   0.073634\n10  medical_boolean                    medical_boolean   1.000000\n11      case_status  patient_ids_with_duplicate_values   0.244408\n12           gender  patient_ids_with_duplicate_values   0.069014\n13  medical_boolean                    medical_boolean   1.000000\n14             date                    duplicate_dates   0.523879\n15  medical_boolean                    medical_boolean   1.000000\n16               id  patient_ids_with_duplicate_values   0.213535\n17              age  patient_ids_with_duplicate_values   0.059554\n18             date                    duplicate_dates   0.523879\n19  medical_boolean                    medical_boolean   1.000000\n20  medical_boolean  patient_ids_with_duplicate_values   0.285567\n21         location  patient_ids_with_duplicate_values  -0.018749\n22              age  patient_ids_with_duplicate_values   0.059554\n23  medical_boolean  patient_ids_with_duplicate_values   0.285567\n24               id  patient_ids_with_duplicate_values   0.213535\n25  medical_boolean                     patient_status   0.421058\n26         location  patient_ids_with_duplicate_values  -0.018749\n27             date                    duplicate_dates   0.523879\n28           gender  patient_ids_with_duplicate_values   0.069014\n29         location  patient_ids_with_duplicate_values  -0.018749\n30             date                    duplicate_dates   0.523879\n31  medical_boolean  patient_ids_with_duplicate_values   0.285567\n32  medical_boolean  patient_ids_with_duplicate_values   0.285567\n33  contact_setting                     patient_status   0.190693\n34              age  patient_ids_with_duplicate_values   0.059554\n35         location  patient_ids_with_duplicate_values  -0.018749\n36               id  patient_ids_with_duplicate_values   0.213535\n37             date                    duplicate_dates   0.523879\n38  medical_boolean  patient_ids_with_duplicate_values   0.285567\n39             date                    duplicate_dates   0.523879\n40  medical_boolean                    medical_boolean   1.000000\n41         location  patient_ids_with_duplicate_values  -0.018749\n42  medical_boolean                    medical_boolean   1.000000\n43             date                    duplicate_dates   0.523879\n44  medical_boolean                    medical_boolean   1.000000\n45          outcome  patient_ids_with_duplicate_values   0.110010\n46               id  patient_ids_with_duplicate_values   0.213535\n47  medical_boolean                    medical_boolean   1.000000\n48         location  patient_ids_with_duplicate_values  -0.018749\n49      case_status                    medical_boolean   0.330923\n50         symptoms                     patient_status   0.252156\n51       occupation                    medical_boolean   0.250093\n52  medical_boolean                    medical_boolean   1.000000\n53  medical_boolean                    medical_boolean   1.000000\n54  medical_boolean                    medical_boolean   1.000000\n"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "3985f1c2-a8de-4d77-96ae-a6588bb84c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:23.974902Z",
     "start_time": "2025-08-10T12:58:23.969134Z"
    },
    "trusted": false
   },
   "source": "out_path = f\"{run_all_directory}/temp-results/fuzzy_result.txt\"\n\nout_dir = os.path.dirname(out_path)   # “./temprorary”\nos.makedirs(out_dir, exist_ok=True)   # create it (if needed)\n\nwith open(out_path, \"a\") as f:\n    f.write(\"==== ArcheType Output. LLM label generation results ====\\n\")\n    f.write(f\"Fuzzy‐matched correct semantic types: {total_correct}\"\n            f\"  →  Accuracy: {fuzzy_acc:.4f}\\n\")\n    f.write(\"Similarity Scores: \\n\")\n    f.write(df.to_string(index=True))\n    f.write(\"\\n\")\n\nprint(f\"✅ Wrote fuzzy results to {out_path}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "✅ Wrote fuzzy results to /home/omadbek/projects/run_all/temp-results/fuzzy_result.txt\n"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.066345Z",
     "start_time": "2025-08-10T12:58:24.050243Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "flat = {}\n\nfor label, m in raw_report.items():\n    if label == \"accuracy\":\n        flat[\"accuracy\"] = m\n    else:\n        for metric_name, val in m.items():\n            flat[f\"{label}_{metric_name}\"] = val\n\nrun_name = f\"{model_name}-label-generation-{prompt_type}-{tune}\"\n# add summary fields\nflat[\"total_entries\"] = total\n# filename identifier: take it from your JSON filename variable\nflat[\"run_name\"] = run_name\nflat[\"inference_time\"] = inference_time\nflat[\"Matching accuracy\"] = f\"{fuzzy_acc:.2f}\"\n\n# convert to one-row DataFrame\ndf = pd.DataFrame([flat])\n\nmetrics_csv = f\"{archetype_directory}/all_metrics.csv\"\n\n# append (or create) the master CSV\nif not os.path.isfile(metrics_csv):\n    df.to_csv(metrics_csv, index=False, float_format=\"%.4f\")\nelse:\n    df.to_csv(metrics_csv, mode=\"a\", header=False, index=False, float_format=\"%.4f\")",
   "id": "7d81ec162056f853",
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "88c2bbabd084979a",
   "metadata": {},
   "source": "# Remapping and F1-score"
  },
  {
   "cell_type": "code",
   "id": "130f3f89-a2f9-4f64-908a-f7bfbf37bd25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.127845Z",
     "start_time": "2025-08-10T12:58:24.117752Z"
    },
    "trusted": false
   },
   "source": "\"\"\"\nimport json\nimport pandas as pd\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata_dir = \"../custom_data_logs\"\n\n# 1) Load results\nwith open(sp, 'r', encoding='utf-8') as f:\n    data = json.load(f)\n\n# 2) Flatten to DataFrame\nrecords = []\nfor entry in data.values():\n    records.append({\n        'file_idx': entry['file+idx'],\n        'ground_truth': entry['ground_truth'],\n        'predicted': entry['response']\n    })\ndf = pd.DataFrame(records)\n\nprint(\"Unique values: \", sorted(df['predicted'].unique()))\n\"\"\"",
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nimport json\\nimport pandas as pd\\nfrom sklearn.metrics import classification_report, confusion_matrix\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\ndata_dir = \"../custom_data_logs\"\\n\\n# 1) Load results\\nwith open(sp, \\'r\\', encoding=\\'utf-8\\') as f:\\n    data = json.load(f)\\n\\n# 2) Flatten to DataFrame\\nrecords = []\\nfor entry in data.values():\\n    records.append({\\n        \\'file_idx\\': entry[\\'file+idx\\'],\\n        \\'ground_truth\\': entry[\\'ground_truth\\'],\\n        \\'predicted\\': entry[\\'response\\']\\n    })\\ndf = pd.DataFrame(records)\\n\\nprint(\"Unique values: \", sorted(df[\\'predicted\\'].unique()))\\n'"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "79ecc133-27f6-48c2-9387-24e8eab89d32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.192064Z",
     "start_time": "2025-08-10T12:58:24.186981Z"
    },
    "trusted": false
   },
   "source": "\"\"\"\nsummary = (\n    df.groupby('predicted')['ground_truth']\n      .agg(['nunique', lambda x: list(x.unique())])\n      .rename(columns={'nunique':'# distinct', '<lambda_0>':'values'})\n)\nprint(summary)\n\"\"\"",
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nsummary = (\\n    df.groupby('predicted')['ground_truth']\\n      .agg(['nunique', lambda x: list(x.unique())])\\n      .rename(columns={'nunique':'# distinct', '<lambda_0>':'values'})\\n)\\nprint(summary)\\n\""
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "88259c1e-0fb6-4f44-a9e4-6f0fa627ee75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.285192Z",
     "start_time": "2025-08-10T12:58:24.279990Z"
    },
    "trusted": false
   },
   "source": "\"\"\"\n# build a dict of {predicted_label: list_of_unique_ground_truths}\nunique_vals = {\n    cat: df.loc[df['predicted'] == cat, 'ground_truth'].unique().tolist()\n    for cat in df['predicted'].unique()\n}\n\n# print them out\nfor cat, vals in unique_vals.items():\n    print(f\"{cat} ({len(vals)} distinct values):\")\n    print(vals, \"\\n\")\n\"\"\"",
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n# build a dict of {predicted_label: list_of_unique_ground_truths}\\nunique_vals = {\\n    cat: df.loc[df[\\'predicted\\'] == cat, \\'ground_truth\\'].unique().tolist()\\n    for cat in df[\\'predicted\\'].unique()\\n}\\n\\n# print them out\\nfor cat, vals in unique_vals.items():\\n    print(f\"{cat} ({len(vals)} distinct values):\")\\n    print(vals, \"\\n\")\\n'"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "48da796a-9465-42db-afa8-4774033a5766",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.400891Z",
     "start_time": "2025-08-10T12:58:24.394547Z"
    },
    "trusted": false
   },
   "source": "\"\"\"\nimport json\nimport pandas as pd\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 3) Define mapping dict\nlabel_mapping = {\n    # identifiers\n    'patient_id':                'id',\n    'patient_ids':               'id',\n\n    # yes/no, true/false\n    'response_status':          'medical_boolean',\n    'medical_boolean':          'medical_boolean',\n\n    # recovery/outcome\n    'patient_recovery_status':   'outcome',\n\n    # dates\n    'date_range':               'date',\n\n    # gender\n    'gender_type':              'gender',\n\n    # locations\n    'country_list':             'location',\n\n    # free‐text details\n    'diagnosis_details':        'symptoms',\n    'role_types':               'occupation',\n}\n\n# 4) Apply mapping\ndf['mapped_predicted'] = df['predicted'].map(label_mapping).fillna(df['predicted'])\n\n# 5) Display DataFrame\n#print(df[['file_idx', 'ground_truth', 'predicted', 'mapped_predicted']])\n\n# 6) Classification report\nprint(\"Classification Report:\")\nprint(classification_report(df['ground_truth'], df['mapped_predicted'], zero_division=0))\n\"\"\"",
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nimport json\\nimport pandas as pd\\nfrom sklearn.metrics import classification_report, confusion_matrix\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# 3) Define mapping dict\\nlabel_mapping = {\\n    # identifiers\\n    \\'patient_id\\':                \\'id\\',\\n    \\'patient_ids\\':               \\'id\\',\\n\\n    # yes/no, true/false\\n    \\'response_status\\':          \\'medical_boolean\\',\\n    \\'medical_boolean\\':          \\'medical_boolean\\',\\n\\n    # recovery/outcome\\n    \\'patient_recovery_status\\':   \\'outcome\\',\\n\\n    # dates\\n    \\'date_range\\':               \\'date\\',\\n\\n    # gender\\n    \\'gender_type\\':              \\'gender\\',\\n\\n    # locations\\n    \\'country_list\\':             \\'location\\',\\n\\n    # free‐text details\\n    \\'diagnosis_details\\':        \\'symptoms\\',\\n    \\'role_types\\':               \\'occupation\\',\\n}\\n\\n# 4) Apply mapping\\ndf[\\'mapped_predicted\\'] = df[\\'predicted\\'].map(label_mapping).fillna(df[\\'predicted\\'])\\n\\n# 5) Display DataFrame\\n#print(df[[\\'file_idx\\', \\'ground_truth\\', \\'predicted\\', \\'mapped_predicted\\']])\\n\\n# 6) Classification report\\nprint(\"Classification Report:\")\\nprint(classification_report(df[\\'ground_truth\\'], df[\\'mapped_predicted\\'], zero_division=0))\\n'"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "b58b45c2-1068-45dc-80bb-e58d7ecc21ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.478350Z",
     "start_time": "2025-08-10T12:58:24.475307Z"
    },
    "trusted": false
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.516325Z",
     "start_time": "2025-08-10T12:58:24.513518Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "",
   "id": "53929977b924aa0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.611921Z",
     "start_time": "2025-08-10T12:58:24.609252Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "",
   "id": "ac4ee197ebdfa942",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
