{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc22338c85d486a2",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "data_path     = \"./custom_data/test_data.parquet\"\n",
    "labels_path   = \"./custom_data/test_labels.parquet\"\n",
    "archetype_directory = \"/home/omadbek/projects/ArcheType\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99491884dd57ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omadbek/.conda/envs/archetype/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#https://colab.research.google.com/drive/1BEZ_qgtVqSmOmCTuhHs7lHiYB5M5_myg?usp=sharing\n",
    "\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "#import gzipƒ\n",
    "from tqdm.auto import tqdm\n",
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from itertools import chain\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from retry import retry\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d565ecc3-2433-4b7b-bfa7-d8225b0db815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EST_CHARS_PER_TOKEN=4\n",
      "MAX_LEN=2000*EST_CHARS_PER_TOKEN\n",
      "INTEGER_SET = set(r\"0123456789,/\\+-.^_()[] :\")\n",
      "BOOLEAN_SET = set([\"True\", \"true\", \"False\", \"false\", \"yes\", \"Yes\", \"No\", \"no\"])\n",
      "\n",
      "ARCHETYPE_PATH = \"/home/omadbek/projects/ArcheType\"\n",
      "DOTENV_PATH = \"/home/omadbek/projects/ArcheType/.env\"\n"
     ]
    }
   ],
   "source": [
    "!cat /home/omadbek/projects/ArcheType/src/const.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "881a695b-e8d4-4c8b-ab64-52f8152df91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/omadbek/projects/ArcheType\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dafde7b-0eef-4af8-a652-012a533151a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())      # → 1\n",
    "print(torch.cuda.get_device_name(0))  # → the one you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "229d1025-a37a-4c7d-95f9-5714da81c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_labels(label, label_set):\n",
    "  label = label.lower().strip()\n",
    "  ldm = {k.lower().strip() : v.lower().strip() for k, v in label_set['dict_map'].items()}\n",
    "  if label_set.get(\"abbrev_map\", -1) != -1:\n",
    "    lda = {k.lower().strip() : v.lower().strip() for k, v in label_set['abbrev_map'].items()}\n",
    "    ldares = lda.get(label, \"\")\n",
    "    if ldares != \"\":\n",
    "      label = ldares\n",
    "  if label.endswith(\"/name\"):\n",
    "    label = label[:-5]\n",
    "  remap = ldm.get(label, -1)\n",
    "  if remap != -1:\n",
    "    label = remap\n",
    "  return label.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b47fba85-2e32-4803-82d4-278a673a03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LABELS = ['age', 'case_status', 'contact_setting', 'date', 'gender', 'id',\n",
    "#       'location', 'medical_boolean', 'occupation', 'outcome', 'symptoms']\n",
    "\n",
    "LABELS = ['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1db475a1-be44-4499-980d-127a62cab476",
   "metadata": {},
   "outputs": [],
   "source": [
    "sotab_integer_labels = [\"age\", \"date\"]\n",
    "sotab_float_labels   = []\n",
    "\n",
    "# everything else must go here\n",
    "sotab_other_labels = [\n",
    "  \"case_status\",\n",
    "  \"gender\",\n",
    "  \"id\",\n",
    "  \"location\",\n",
    "  \"medical_boolean\",\n",
    "  \"occupation\",\n",
    "  \"outcome\",\n",
    "  \"symptoms\"\n",
    "]\n",
    "\n",
    "sotab_top_hier = {\n",
    "  \"integer\": sotab_integer_labels,\n",
    "  \"float\":   sotab_float_labels,\n",
    "  \"other\":   sotab_other_labels\n",
    "}\n",
    "\n",
    "sotab_identifier = [\"id\"]\n",
    "sotab_category   = [\"gender\", \"medical_boolean\", \"outcome\"]\n",
    "sotab_text       = [\"location\", \"symptoms\", \"occupation\"]\n",
    "\n",
    "\n",
    "sotab_other_hier = {\n",
    "  \"Identifier\": sotab_identifier,\n",
    "  \"category\":   sotab_category,\n",
    "  \"text\":       sotab_text\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e79430-47a7-4d5a-9f97-b5524ed9305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed=13\n",
    "EST_CHARS_PER_TOKEN=4\n",
    "MAX_LEN=2000*EST_CHARS_PER_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff3bbf8b-5706-486b-9cd0-857411301200",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/omadbek/projects/alpaca/outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c10f1c02-1fd2-4795-9ee0-17b6b16ad4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPTS\n",
    "\n",
    "def llm_prompts(agent, input_list, options_str = None):\n",
    "\n",
    "    if agent == \"generator\":\n",
    "        # Generate agent\n",
    "        prompt = f\"\"\"\n",
    "            System: You are an epidemiology data steward.\n",
    "            \n",
    "            INSTRUCTIONS:\n",
    "            • You’ll get a comma-separated list of anonymized tokens (IDs, dates, names, etc.).\n",
    "            • Invent exactly one **broad**, high-level snake_case label that best summarizes what kind of field this is.\n",
    "              – Think “date”, “identifier”, “status”, “count”, “category”, etc.\n",
    "              – Avoid overly specific terms like “case_date_range_2020_july” or “lab_test_ids”.\n",
    "            • Do NOT copy or overlap any part of the input tokens.\n",
    "            • Return only that one label, no extra text.\n",
    "            \n",
    "            Now it’s your turn:\n",
    "            Input: {input_list}\n",
    "            Output:\n",
    "        \"\"\"\n",
    "    else:\n",
    "        \n",
    "        # Picker agent\n",
    "        prompt = f\"\"\"\n",
    "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
    "                    \n",
    "                    INSTRUCTIONS:\n",
    "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
    "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
    "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
    "                    \n",
    "                    INPUT: {input_list}\n",
    "                    OPTIONS: {options_str}\n",
    "                    ANSWER:\n",
    "                \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7bdfa-04c5-4106-b791-166e74bbc98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba93a7e3-1802-4db0-96e5-6e9f8c9e1921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Load the embedding model once at module scope\n",
    "_sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
    "\n",
    "def _normalize_label(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Lowercase, drop non-alphanumerics,\n",
    "    and strip a trailing 's' for crude singularization.\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-z0-9]', '', s)\n",
    "    if s.endswith('s'):\n",
    "        s = s[:-1]\n",
    "    return s\n",
    "\n",
    "def _embed_labels(labels: List[str]):\n",
    "    \"\"\"\n",
    "    Encode a list of labels into normalized sentence‐embeddings.\n",
    "    \"\"\"\n",
    "    return _sent_model.encode(labels, normalize_embeddings=True)\n",
    "\n",
    "class _FuzzyLabelMatcher:\n",
    "    def __init__(\n",
    "        self,\n",
    "        fixed_labels: List[str],\n",
    "        embed_threshold: float,\n",
    "        fuzz_threshold: int\n",
    "    ):\n",
    "        self.fixed_labels = list(fixed_labels)\n",
    "        self.embed_threshold = embed_threshold\n",
    "        self.fuzz_threshold = fuzz_threshold\n",
    "        self._update_embeddings()\n",
    "\n",
    "    def _update_embeddings(self):\n",
    "        self._embeddings = (\n",
    "            _embed_labels(self.fixed_labels)\n",
    "            if self.fixed_labels else\n",
    "            None\n",
    "        )\n",
    "\n",
    "    def _find_match(self, label: str) -> str:\n",
    "        \"\"\"\n",
    "        Return an existing label if `label` is similar enough,\n",
    "        otherwise return None.\n",
    "        Matching steps in order:\n",
    "          1) Token‐match on underscores/hyphens (e.g. \"id\" in \"patient_id\")\n",
    "          2) Exact normalized match\n",
    "          3) Fuzzy string match\n",
    "          4) Embedding similarity\n",
    "        \"\"\"\n",
    "        norm_label = label.lower()\n",
    "\n",
    "        # 1) Token‐match: split existing labels on non-alphanumerics\n",
    "        for existing in self.fixed_labels:\n",
    "            tokens = re.split(r'[^a-zA-Z0-9]+', existing.lower())\n",
    "            if norm_label in tokens:\n",
    "                return existing\n",
    "\n",
    "        # 2) Exact normalized match\n",
    "        norm_new = _normalize_label(label)\n",
    "        for existing in self.fixed_labels:\n",
    "            if _normalize_label(existing) == norm_new:\n",
    "                return existing\n",
    "\n",
    "        # 3) Fuzzy string match\n",
    "        for existing in self.fixed_labels:\n",
    "            if fuzz.token_sort_ratio(label, existing) >= self.fuzz_threshold:\n",
    "                return existing\n",
    "\n",
    "        # 4) Embedding similarity\n",
    "        if self._embeddings is not None:\n",
    "            emb = _sent_model.encode([label], normalize_embeddings=True)[0]\n",
    "            sims = util.cos_sim(emb, self._embeddings)[0]\n",
    "            best_idx = sims.argmax().item()\n",
    "            if sims[best_idx] >= self.embed_threshold:\n",
    "                return self.fixed_labels[best_idx]\n",
    "\n",
    "        return None\n",
    "\n",
    "    def resolve(self, orig_label: str) -> str:\n",
    "        \"\"\"\n",
    "        If `orig_label` matches an existing one, return that existing label.\n",
    "        Otherwise, add `orig_label` to fixed_labels and return it.\n",
    "        \"\"\"\n",
    "        match = self._find_match(orig_label)\n",
    "        if match:\n",
    "            return match\n",
    "\n",
    "        # new label → add and update embeddings\n",
    "        self.fixed_labels.append(orig_label)\n",
    "        self._update_embeddings()\n",
    "        return orig_label\n",
    "\n",
    "def resolve_label(\n",
    "    orig_label: str,\n",
    "    fixed_labels: List[str],\n",
    "    embed_threshold: float = 0.82,\n",
    "    fuzz_threshold: int = 90\n",
    ") -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Given a newly generated label `orig_label` and a list of\n",
    "    `fixed_labels`, return a tuple:\n",
    "      (resolved_label, updated_fixed_labels).\n",
    "\n",
    "    - If `orig_label` is similar to an existing label, `resolved_label`\n",
    "      is that existing label.\n",
    "    - Otherwise, `orig_label` is appended to fixed_labels, and returned.\n",
    "    \"\"\"\n",
    "    matcher = _FuzzyLabelMatcher(fixed_labels, embed_threshold, fuzz_threshold)\n",
    "    resolved = matcher.resolve(orig_label)\n",
    "    return resolved, matcher.fixed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76159fc4-0916-44b2-b99d-e6eca91be82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import infer_auto_device_map, init_empty_weights, load_checkpoint_and_dispatch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, \\\n",
    "    T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "import langchain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "#sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
    "\n",
    "\n",
    "def set_pipeline(k=1):\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=MAX_LEN,\n",
    "        temperature=0.5 * k,\n",
    "        top_p=0.80 - (0.1 * k),\n",
    "        repetition_penalty=1.3\n",
    "    )\n",
    "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    llm_chain = LLMChain(prompt=pt,\n",
    "                         llm=local_llm\n",
    "                         )\n",
    "    return pipe, local_llm, llm_chain\n",
    "\n",
    "\n",
    "curr_model = \"\"\n",
    "\n",
    "\n",
    "def init_model(model):\n",
    "    curr_model = model\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    if model == \"llama-65b\":\n",
    "        LLAMA_PATH = \"/scratch/bf996/text-generation-webui/models/llama-65b-hf\"\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(LLAMA_PATH)\n",
    "        config = AutoConfig.from_pretrained(LLAMA_PATH,\n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            load_in_8bit=True)\n",
    "        with init_empty_weights():\n",
    "            base_model = AutoModelForCausalLM.from_config(config)\n",
    "        base_model.tie_weights()\n",
    "        device_map = infer_auto_device_map(base_model, max_memory={0: \"60GiB\", \"cpu\": \"96GiB\"})\n",
    "        base_model = load_checkpoint_and_dispatch(\n",
    "            base_model,\n",
    "            LLAMA_PATH,\n",
    "            device_map=device_map\n",
    "        )\n",
    "    elif model == \"alpaca-13b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "        #tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "        base_model = LlamaForCausalLM.from_pretrained(\n",
    "            #model_path,\n",
    "            \"chavinlo/alpaca-native\",\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_8bit=True,\n",
    "            device_map='auto',\n",
    "        )\n",
    "    elif model == \"alpaca-fine-tuned\":\n",
    "        MAX_LEN = 2048\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    elif model == \"vicuna-13b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"eachadea/vicuna-13b\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"eachadea/vicuna-13b\",\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_8bit=True,\n",
    "            device_map='auto',\n",
    "        )\n",
    "    elif model == \"gpt4-x-alpaca\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"chavinlo/gpt4-x-alpaca\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\"chavinlo/gpt4-x-alpaca\", device_map=\"auto\",\n",
    "                                                          load_in_8bit=True)\n",
    "    elif model == \"t0pp\":\n",
    "        MAX_LEN = 512\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0pp\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\",\n",
    "                                                           torch_dtype=torch.float16, load_in_8bit=True)\n",
    "    elif model == \"flan-t5-xxl\":\n",
    "        MAX_LEN = 512\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\",\n",
    "                                                           torch_dtype=torch.float16, load_in_8bit=True)\n",
    "    elif model == \"flan-ul2\":\n",
    "        MAX_LEN = 512\n",
    "        base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16,\n",
    "                                                                device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n",
    "    elif model == \"galpaca-30b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"GeorgiaTechResearchInstitute/galpaca-30b\", device_map=\"auto\",\n",
    "                                                  torch_dtype=torch.float16, load_in_8bit=True)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\"GeorgiaTechResearchInstitute/galpaca-30b\")\n",
    "    elif model == \"opt-iml-max-30b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-iml-max-30b\", use_fast=False, padding_side='left')\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-iml-max-30b\", device_map=\"auto\",\n",
    "                                                          torch_dtype=torch.float16)\n",
    "    if model in [\"flan-t5-xxl\", \"t0pp\", \"flan-ul2\"]:\n",
    "        template = \"\"\"{instruction}\"\"\"\n",
    "    else:\n",
    "        template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "        ### Instruction: \n",
    "        {instruction}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    pt = PromptTemplate(template=template, input_variables=[\"instruction\"])\n",
    "    #Convert length from tokens to characters, leave room for model response\n",
    "    MAX_LEN = MAX_LEN * EST_CHARS_PER_TOKEN - 200\n",
    "    return base_model, tokenizer, template, pt, MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faa69500-a350-4dc9-9204-f5bea462888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sherlock_resp(df, gt_df, prompt_dict, model, label_indices, base_prompt, lsd):\n",
    "    isd4 = \"d4\" in lsd['name']\n",
    "    if \"sherlock\" in model:\n",
    "        model = sherlock_model\n",
    "        data_m = pd.Series(df[label_indices].astype(str).T.values.tolist())\n",
    "        extract_features(\n",
    "            \"../temporary.csv\",\n",
    "            data_m\n",
    "        )\n",
    "        feature_vectors = pd.read_csv(\"../temporary.csv\", dtype=np.float32)\n",
    "        predicted_labels = model.predict(feature_vectors, \"sherlock\")\n",
    "        iter_len = len(data_m)\n",
    "    elif \"doduo\" in model:\n",
    "        model = doduo_model\n",
    "        data_m = df[label_indices]\n",
    "        try:\n",
    "            annot_m = doduo_model.annotate_columns(data_m)\n",
    "            predicted_labels = annot_m.coltypes\n",
    "        except Exception as e:\n",
    "            print(f\"Exception {e} in Doduo, returning default \\n\")\n",
    "            predicted_labels = [\"text\" for i in range(len(data_m))]\n",
    "        iter_len = len(predicted_labels)\n",
    "    predicted_labels_dict = {i: sherlock_to_cta.get(predicted_labels[i], [predicted_labels[i]]) for i in\n",
    "                             range(iter_len)}\n",
    "\n",
    "    for idx, label_idx in zip(range(iter_len), label_indices):\n",
    "        prompt = base_prompt + \"_\" + str(label_idx)\n",
    "        if isd4:\n",
    "            ans = predicted_labels[0]\n",
    "            label = [s.lower() for s in lsd['d4_map'][gt_df]]\n",
    "        else:\n",
    "            gt_row = gt_df[gt_df['column_index'] == label_idx]\n",
    "            if len(gt_row) != 1:\n",
    "                continue\n",
    "            label = fix_labels(gt_row['label'].item(), lsd)\n",
    "            ans = [fix_labels(item, lsd) for item in predicted_labels_dict[idx]]\n",
    "        if isd4:\n",
    "            res = ans in label\n",
    "        else:\n",
    "            assert isinstance(ans, list), \"ans should be a list\"\n",
    "            res = label in ans\n",
    "        ans_dict = {\"response\": ans, \"context\": None, \"ground_truth\": label, \"correct\": res,\n",
    "                    \"orig_model_label\": predicted_labels[idx]}\n",
    "        prompt_dict[prompt] = ans_dict\n",
    "    return prompt\n",
    "\n",
    "\n",
    "@retry(Exception, tries=3, delay=3)\n",
    "def get_chatgpt_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None,\n",
    "                     method=[\"similarity\"], max_len=15000):\n",
    "    fixed_labels = [fix_labels(s, lsd) for s in lsd['label_set']]\n",
    "    model = \"gpt-3.5\"\n",
    "    context_labels = \", \".join(fixed_labels)\n",
    "    fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n",
    "    prompt = prompt_context_insert(context_labels, context, max_len, \"gpt-3.5\")\n",
    "    d_p = prompt_dict.get(prompt, -1)\n",
    "    if d_p != -1 and \"skip-existing\" in method:\n",
    "        #recompute_results(prompt_dict, prompt, model, cbc_pred=None, label_set=lsd)\n",
    "        return prompt\n",
    "    elif d_p != -1:\n",
    "        while prompt_dict.get(prompt, -1) != -1:\n",
    "            prompt = prompt + \"*\"\n",
    "    if response:\n",
    "        ans = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0,\n",
    "        ).choices[0]['message']['content']\n",
    "        #print(f\"Original ans is {ans}\")\n",
    "    ans_n = fuzzy_label_match(ans, fixed_labels, None, None, prompt, lsd, model, method=method)\n",
    "    #print(f\"Fuzzy ans is {ans_n}\")\n",
    "    res = ans_n == ground_truth\n",
    "    ans_dict = {\"response\": ans_n, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res,\n",
    "                \"original_model_answer\": ans}\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "    return prompt\n",
    "\n",
    "\n",
    "@retry(Exception, tries=5, delay=3)\n",
    "def get_ada_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None):\n",
    "    prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"ada-personal\")\n",
    "    if prompt_dict.get(prompt, -1) != -1:\n",
    "        #recompute_results(prompt_dict, prompt, \"ada-personal\", label_set=lsd)\n",
    "        return prompt\n",
    "    if response:\n",
    "        proc = subprocess.run(\n",
    "            [\"openai\", \"api\", \"completions.create\", \"-m\", \"ada:ft-personal:-2023-03-14-11-52-45\", \"-M\", \"3\", \"-p\",\n",
    "             prompt], capture_output=True, check=True)\n",
    "        ans = proc.stdout.decode(\"utf-8\")[len(prompt):].strip()\n",
    "    else:\n",
    "        ans = \"\"\n",
    "    res = ans.lower().strip().startswith(ground_truth)\n",
    "    ans_dict = {\"response\": ans, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res}\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "    return prompt\n",
    "\n",
    "def call_llama_model(session, link, prompt, lsd, var_params):\n",
    "    # Build the payload expected by the new LLaMA endpoint\n",
    "    payload = {\n",
    "        \"model\":      \"llama3.1:8b-instruct-q8_0\",\n",
    "        \"prompt\":     prompt,\n",
    "        \"max_tokens\": 30,\n",
    "        \"stream\":     False\n",
    "    }\n",
    "\n",
    "    # Choose session-based or direct requests call\n",
    "    client = session or requests\n",
    "    resp = client.post(link, json=payload)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    # Extract the generated text\n",
    "    data = resp.json()\n",
    "    text = data.get(\"response\", \"\")\n",
    "\n",
    "    #if text == \"none\":\n",
    "    #    return generate_label(session, link, prompt, lsd, var_params)\n",
    "    #else:\n",
    "\n",
    "    print(\"LLama model was called\")\n",
    "    \n",
    "    return fix_labels(text.strip(), lsd)\n",
    "\n",
    "temperature = 0\n",
    "top_p = 0\n",
    "\n",
    "def extract_answer(orig_ans: str) -> str:\n",
    "    \"\"\"\n",
    "    If orig_ans contains 'ANSWER:...', return the text after the colon.\n",
    "    Otherwise, return orig_ans unchanged (stripped).\n",
    "    \"\"\"\n",
    "    m = re.search(r\"ANSWER\\s*:\\s*(.*)\", orig_ans, re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return orig_ans.strip()\n",
    "\n",
    "#generated_labels_list = []\n",
    "\n",
    "def generate_label(session, link, old_prompt, lsd, var_params, generated_labels_list, orig_ans):\n",
    "\n",
    "    if orig_ans.lower() != \"none\" and orig_ans not in generated_labels_list:\n",
    "        generated_labels_list.append(orig_ans)\n",
    "          \n",
    "    return picker_prompt, picked\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "@retry(Exception, tries=3, delay=3)\n",
    "def get_topp_resp(prompt, k):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "\n",
    "    temperature = 0.1 * k\n",
    "    top_p   = 0.90 - (0.1 * k)\n",
    "\n",
    "    outputs = base_model.generate(inputs,\n",
    "                                  max_length=MAX_LEN,\n",
    "                                  #do_sample=False,\n",
    "                                  #num_beams=1\n",
    "                                  temperature=temperature,\n",
    "                                  top_p=top_p,\n",
    "                                  repetition_penalty=1.3\n",
    "                                  )\n",
    "    orig_ans = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return extract_answer(orig_ans)\n",
    "\n",
    "\n",
    "\n",
    "@retry(Exception, tries=3, delay=3)\n",
    "def get_llama_resp(lsd: dict, context: list, ground_truth: str, prompt_dict: dict, link: str, response=True,\n",
    "                   session=None, cbc=None, model=\"llama\", limited_context=None,\n",
    "                   method=[\"ans_contains_gt\", \"gt_contains_ans\", \"resample\"], generated_labels_list = None):\n",
    "    #print(f\"in get llama resp, gt is {ground_truth}, context is {context}\")\n",
    "    isd4 = \"d4\" in lsd['name']\n",
    "    if isd4:\n",
    "        gtv = lsd['d4_map'][ground_truth]\n",
    "        if isinstance(gtv, str):\n",
    "            gtv = [gtv]\n",
    "        ground_truth = [s.lower() for s in gtv]\n",
    "    if \"hierarchical\" in method and not isd4:\n",
    "        dtype = get_base_dtype(limited_context)\n",
    "        fixed_labels = sotab_top_hier[dtype]\n",
    "    else:\n",
    "        fixed_labels = list(set([fix_labels(s, lsd) for s in lsd['label_set']]))\n",
    "    context_labels = \", \".join(fixed_labels)\n",
    "    fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n",
    "    if model in [\"llama-zs\", \"opt-iml-30b-zs\"]:\n",
    "        pipe, local_llm, llm_chain = set_pipeline(k=1)\n",
    "    prompt = prompt_context_insert(context_labels, context, MAX_LEN, model, options = generated_labels_list)\n",
    "    d_p = prompt_dict.get(prompt, -1)\n",
    "    #skip existing logic\n",
    "    if d_p != -1 and \"skip-existing\" in method:\n",
    "        # recompute_results(prompt_dict, prompt, \"llama\", cbc, lsd)\n",
    "        return prompt, prompt_dict[prompt][\"response\"]\n",
    "    elif d_p != -1:\n",
    "        while prompt_dict.get(prompt, -1) != -1:\n",
    "            prompt = prompt + \"*\"\n",
    "    #print(\"GET LLAMA NEETY GREEDY:\")\n",
    "    #print(prompt)\n",
    "    #response logic\n",
    "    if not response:\n",
    "        orig_ans = ans_n = \"\"\n",
    "    else:\n",
    "        orig_ans = apply_basic_rules(limited_context, None)\n",
    "        if orig_ans is None:\n",
    "            orig_ans = query_correct_model(model, prompt, context_labels, context, session, link, lsd)\n",
    "            \n",
    "            #hierarchical matching logic\n",
    "            if \"hierarchical\" in method and dtype == \"other\" and orig_ans not in ['email', 'URL', 'WebHTMLAction',\n",
    "                                                                                  'Photograph']:\n",
    "                next_label_set = sotab_other_hier.get(orig_ans, -1)\n",
    "                if next_label_set == -1:\n",
    "                    print(f\"Original answer {orig_ans} not found in hierarchy\")\n",
    "                    next_label_set = sotab_other_hier['text']\n",
    "                fixed_labels = list(set([fix_labels(s, lsd) for s in next_label_set]))\n",
    "                context_labels = \", \".join(fixed_labels)\n",
    "                fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n",
    "                orig_ans = query_correct_model(model, prompt, context_labels, context, session, link, lsd)\n",
    "                #fuzzy matching logic\n",
    "            #print(\"Fuzzy matching logic\")\n",
    "            #print(f\"Fixed LABELSS: {generated_labels_list}\")\n",
    "            #print(f\"Fuzzy prompt: {prompt}\")\n",
    "            #print(f\"Fuzzy lsd: {lsd}\")\n",
    "            ans_n = orig_ans.lower()\n",
    "\n",
    "            \n",
    "    \n",
    "        else:\n",
    "            ans_n = orig_ans.lower()\n",
    "\n",
    "    print(f\"LLM Picker 1 answer: {ans_n}... Should be none in the beginning\")\n",
    "\n",
    "    if ans_n.lower() != \"none\" and ans_n not in generated_labels_list:\n",
    "        generated_labels_list.append(ans_n)\n",
    "\n",
    "    print(f\"List of label so far: {generated_labels_list}\")\n",
    "\n",
    " \n",
    "\n",
    "    #print(f\"final label set was {fixed_labels}, prediction was {ans_n}, ground truth was {ground_truth} \\n\")\n",
    "    if isd4:\n",
    "        res = ans_n in ground_truth\n",
    "    else:\n",
    "        res = ans_n == ground_truth\n",
    "        \n",
    "    ans_dict = {\"response\": ans_n, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res,\n",
    "                \"original_model_answer\": orig_ans}\n",
    "\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "\n",
    "    #print(f\"Final answer: {ans_n}\")\n",
    "    return prompt, ans_n\n",
    "\n",
    "@retry(Exception, tries=5, delay=3)\n",
    "def get_bloomz_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None):\n",
    "    prompt = prompt_context_insert(context_labels, context, 2000, \"bloomz\")\n",
    "    if prompt_dict.get(prompt, -1) != -1:\n",
    "        return prompt\n",
    "    if response:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda:0\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=5)\n",
    "    else:\n",
    "        response = \"\"\n",
    "    ans = tokenizer.decode(outputs[0]).split()[-1]\n",
    "    ans = ''.join(e for e in ans if e.isalnum()).lower()\n",
    "    res = ans == ground_truth\n",
    "    ans_dict = {\"response\": ans, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res}\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85950645-b679-4139-baed-fcaa30e91aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_integer(val):\n",
    "    return pd.to_numeric(val, downcast='integer', errors='ignore')\n",
    "\n",
    "\n",
    "def derive_meta_features(col):\n",
    "    features = {}\n",
    "    if not col.astype(str).apply(str.isnumeric).all():\n",
    "        return {\"std\": round(col.astype(str).str.len().std(), 2), \"mean\": round(col.astype(str).str.len().mean(), 2),\n",
    "                \"mode\": col.astype(str).str.len().mode().iloc[0].item(), \"median\": col.astype(str).str.len().median(),\n",
    "                \"max\": col.astype(str).str.len().max(), \"min\": col.astype(str).str.len().min(),\n",
    "                \"rolling-mean-window-4\": [0.0]}\n",
    "    col = col.dropna().astype(float)\n",
    "    if col.apply(float.is_integer).all():\n",
    "        col = col.astype(int)\n",
    "    #print(f\"Collecting metafeatures for column {col} \\n\")\n",
    "    features['std'] = round(col.std(), 2)\n",
    "    features['mean'] = round(col.mean(), 2)\n",
    "    features['mode'] = col.mode().iloc[0].item()\n",
    "    features['median'] = col.median()\n",
    "    features['max'] = col.max()\n",
    "    features['min'] = col.min()\n",
    "    indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=4)\n",
    "    features['rolling-mean-window-4'] = list(col.rolling(window=indexer, min_periods=1).mean())\n",
    "    return features\n",
    "\n",
    "\n",
    "def fix_mode(d):\n",
    "    if isinstance(d['mode'], pd.Series):\n",
    "        d['mode'] = d['mode'].loc[0].item()\n",
    "    return d\n",
    "\n",
    "\n",
    "def split_meta_features(d):\n",
    "    return pd.Series(\n",
    "        [d.get('std', \"N/A\"), d.get('mean', \"N/A\"), d.get('median', \"N/A\"), d.get('mode', \"N/A\"), d.get('max', \"N/A\"),\n",
    "         d.get('min', \"N/A\")])\n",
    "\n",
    "\n",
    "def prompt_context_insert(context_labels: str, context: str, max_len: int = 2000, model: str = \"gpt-3.5\", options: list[str] = None):\n",
    "    if model == \"bloomz\":\n",
    "        s = f'SYSTEM: You are an AI research assistant. You use a tone that is technical and scientific. USER: Please select the field from {context_labels} which best describes the context below. Respond with the name of the field and nothing else. \\n CONTEXT: {context}'\n",
    "    elif model == \"gpt-3.5\":\n",
    "        s = f'SYSTEM: Please select the field from {context_labels} which best describes the context. Respond only with the name of the field. \\n CONTEXT: {context}'\n",
    "    elif model == \"ada-personal\":\n",
    "        s = f'{context}$'\n",
    "    elif model == \"llama-old\":\n",
    "        s = f'INSTRUCTION: Select the field from the category which matches the input. \\n CATEGORIES: {context_labels} \\n INPUT:{context} \\n OUTPUT: '\n",
    "    elif \"-zs\" in model:\n",
    "        ct = \"[\" + \", \".join(context).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")[\n",
    "                   :MAX_LEN - 100 - len(context_labels)] + \"]\"\n",
    "        lb = \"\\n\".join([\"- \" + c for c in context_labels.split(\", \")])\n",
    "        #s = f'How might one classify the following input? \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n ANSWER:'\n",
    "        if model == \"opt-iml-max-30b-zs\":\n",
    "            s = f'Select the option which best describes the input. \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n'\n",
    "        else:\n",
    "            # Original prompt\n",
    "            s = f'INSTRUCTION: Select the option which best describes the input. \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n ANSWER:'\n",
    "            \n",
    "    elif model == \"llama\":\n",
    "        ct = \"[\" + \", \".join(context).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")[\n",
    "                   :MAX_LEN - 100 - len(context_labels)] + \"]\"\n",
    "        lb = \"\\n\".join([\"- \" + c for c in context_labels.split(\", \")])\n",
    "        \n",
    "        #s = f'INSTRUCTION: Select the category which best matches the input. \\n INPUT:{context} .\\n OPTIONS:\\n{lb} \\n CATEGORY: '\n",
    "\n",
    "        #s = f'INSTRUCTION: Select the category which best matches the input. If category is not matching the input return none. Do not provide any further text, only label if it exists or none. \\n INPUT:{context} .\\n OPTIONS:\\n - none \\n CATEGORY: '\n",
    "        #opt_set = options + [\"none\"]\n",
    "        \n",
    "        options_str = \" - \".join(options)\n",
    "        \n",
    "        #if options is not None:\n",
    "        #    options_str = \" - \".join(options)\n",
    "        #else:\n",
    "        #    options_str = \"- none\"\n",
    "            \n",
    "        print(f\"Prompt context insert {options_str}\")\n",
    "\n",
    "        # Picker 1 prompt\n",
    "        s = picker_prompt = llm_prompts(\"picker\", ct, options_str)\n",
    "\n",
    "    elif model == \"llama-retry\":\n",
    "        s = f'INSTRUCTION: Select the category which best matches the input. \\n INPUT:{context} \\n CATEGORY: '\n",
    "    #Truncate if prompt exceeds maximum length\n",
    "    if len(s) > max_len:\n",
    "        s = s[:max_len - 3]\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def recompute_results(prompt_dict, prompt, model_str, cbc_pred, label_set):\n",
    "    dict_val = prompt_dict.get(prompt, -1)\n",
    "    dict_val['cbc_pred'] = cbc_pred\n",
    "    if model_str == \"llama\":\n",
    "        if cbc_pred and (cbc_pred in catboost_cats):\n",
    "            print(f\"using cbcpred label: {cbc_pred} \\n\")\n",
    "            dict_val['response'] = fix_labels(cbc_pred, label_set)\n",
    "        dict_val['correct'] = ((dict_val['ground_truth'] == dict_val['response']) or (\n",
    "                    dict_val['response'] and (dict_val['response']) in dict_val['ground_truth']))\n",
    "    prompt_dict[prompt] = dict_val\n",
    "\n",
    "\n",
    "def make_json(prompt, var_params):\n",
    "    p = deepcopy(params)\n",
    "    if var_params:\n",
    "        for k, v in var_params.items():\n",
    "            p[k] = v\n",
    "    return {\n",
    "        \"data\": [\n",
    "            prompt,\n",
    "            p['max_new_tokens'],\n",
    "            p['do_sample'],\n",
    "            p['temperature'],\n",
    "            p['top_p'],\n",
    "            p['typical_p'],\n",
    "            p['repetition_penalty'],\n",
    "            p['encoder_repetition_penalty'],\n",
    "            p['top_k'],\n",
    "            p['min_length'],\n",
    "            p['no_repeat_ngram_size'],\n",
    "            p['num_beams'],\n",
    "            p['penalty_alpha'],\n",
    "            p['length_penalty'],\n",
    "            p['early_stopping'],\n",
    "            p['seed'],\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def ans_contains_gt(ans_n, fixed_labels):\n",
    "    for fixed_label in fixed_labels:\n",
    "        if fixed_label in ans_n:\n",
    "            print(f\"Fuzzy label {ans_n} contains gt label {fixed_label}: MATCH \\n\")\n",
    "            ans_n = fixed_label\n",
    "            return ans_n\n",
    "    return None\n",
    "\n",
    "\n",
    "def gt_contains_ans(ans_n, fixed_labels):\n",
    "    if ans_n == \"\":\n",
    "        return None\n",
    "    for fixed_label in fixed_labels:\n",
    "        if ans_n in fixed_label:\n",
    "            print(f\"GT label {fixed_label} contains fuzzy label {ans_n}: MATCH \\n\")\n",
    "            ans_n = fixed_label\n",
    "            return ans_n\n",
    "    return None\n",
    "\n",
    "\n",
    "def basic_contains(ans_n, fixed_labels, method):\n",
    "    #TODO: not sure the order should be fixed like this, could be made flexible\n",
    "    if ans_n in fixed_labels:\n",
    "        return ans_n\n",
    "    if \"ans_contains_gt\" in method:\n",
    "        res = ans_contains_gt(ans_n, fixed_labels)\n",
    "        if res:\n",
    "            return res\n",
    "    if \"gt_contains_ans\" in method:\n",
    "        res = gt_contains_ans(ans_n, fixed_labels)\n",
    "        if res:\n",
    "            return res\n",
    "    return None\n",
    "\n",
    "\n",
    "def fuzzy_label_match(orig_ans, fixed_labels, session, link, prompt, lsd, model,\n",
    "                      method=[\"ans_contains_gt\", \"gt_contains_ans\", \"resample\"]):\n",
    "\n",
    "    #answer is already in label set, no fuzzy match needed\n",
    "    ans_n = fix_labels(orig_ans, lsd)\n",
    "    res = basic_contains(ans_n, fixed_labels, method)\n",
    "    if res:\n",
    "        return res\n",
    "    if \"similarity\" in method:\n",
    "        ans_embedding = sent_model.encode(ans_n)\n",
    "        lbl_embeddings = sent_model.encode(fixed_labels)\n",
    "        sims = {lbl: util.pytorch_cos_sim(ans_embedding, le) for lbl, le in zip(fixed_labels, lbl_embeddings)}\n",
    "        return max(sims, key=sims.get)\n",
    "    if \"resample\" in method:\n",
    "        #fuzzy label matching strategy\n",
    "        for k in range(2, 6):\n",
    "            if \"gpt\" in model:\n",
    "                ans_n = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    temperature=0 + k / 10,\n",
    "                ).choices[0]['message']['content'].lower()\n",
    "            elif model in [\"llama-zs\", \"opt-iml-30b-zs\"]:\n",
    "                pipe, local_llm, llm_chain = set_pipeline(k=k)\n",
    "                ans_n = llm_chain.run(prompt)\n",
    "            elif model in [\"topp-zs\", \"flan-ul2-zs\"]:\n",
    "                ans_n = get_topp_resp(prompt, k)\n",
    "            else:\n",
    "                rep_pen = params['repetition_penalty']\n",
    "                top_p = params['top_p']\n",
    "                temp = params['temperature']\n",
    "                ans_n = call_llama_model(session, link, prompt, lsd,\n",
    "                                         {'no_repeat_ngram_size': 1, 'top_p': top_p - (0.1 * k), 'temperature': 0.9})\n",
    "                params['top_p'] = top_p\n",
    "                params['temperature'] = temp\n",
    "            res = basic_contains(ans_n, fixed_labels, method)\n",
    "            if res:\n",
    "                return res\n",
    "    #print(\"Applying fallback label, 'text' \\n\")\n",
    "    return 'text'\n",
    "\n",
    "\n",
    "INTEGER_SET = set(r\"0123456789,/\\+-.^_()[] :\")\n",
    "\n",
    "\n",
    "def get_base_dtype(context):\n",
    "    dtype = \"integer\"\n",
    "    for item in context:\n",
    "        if not all(char in INTEGER_SET for char in item):\n",
    "            #print(f\"String is OTHER because: {[char for char in item if char not in INTEGER_SET]}\")\n",
    "            return \"other\"\n",
    "        try:\n",
    "            if item.endswith(\".0\") or item.endswith(\",0\"):\n",
    "                item = item[:-2]\n",
    "                item = str(int(item))\n",
    "            if item.endswith(\".00\") or item.endswith(\",00\"):\n",
    "                item = item[:-3]\n",
    "                item = str(int(item))\n",
    "        except:\n",
    "            return \"float\"\n",
    "        temp_item = re.sub(r\"[^a-zA-Z0-9.]\", \"\", item)\n",
    "        if not temp_item.isdigit():\n",
    "            #print(f\"string is FLOAT because {temp_item} is not an integer\")\n",
    "            dtype = \"float\"\n",
    "    return dtype\n",
    "\n",
    "\n",
    "def query_correct_model(model, prompt, context_labels, context, session, link, lsd):\n",
    "    if model in [\"llama-zs\", \"opt-iml-max-30b-zs\"]:\n",
    "        orig_ans = llm_chain.run(prompt)\n",
    "        if orig_ans is None:\n",
    "            prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"llama-retry\")\n",
    "            orig_ans = llm_chain.run(prompt)\n",
    "    elif model in [\"topp-zs\", \"flan-ul2-zs\"]:\n",
    "        orig_ans = get_topp_resp(prompt, 1)\n",
    "    else:\n",
    "        orig_ans = call_llama_model(session, link, prompt, lsd, None)\n",
    "        if orig_ans is None:\n",
    "            prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"llama-retry\")\n",
    "            orig_ans = call_llama_model(session, link, prompt, lsd, None)\n",
    "    return orig_ans\n",
    "\n",
    "\n",
    "def get_df_sample_col(col, rand_seed, len_context, min_variance=2, replace=False):\n",
    "    df = pd.Series(col)\n",
    "    ignore_list = [\"None\", 'none', 'NaN', 'nan', 'N/A', 'na', '']\n",
    "    sample_list = list(set(p[:75] for p in pd.unique(df.astype(str)[col]) if p not in ignore_list))\n",
    "    if len(sample_list) < 1:\n",
    "        return [\"None\"] * len_context\n",
    "    if len(sample_list) < len_context:\n",
    "        sample_list = sample_list * len_context\n",
    "    if len(sample_list) > len_context:\n",
    "        sample_list = sample_list[:len_context]\n",
    "    assert len(sample_list) == len_context, f\"An index in val_indices is length {len(sample_list)}\"\n",
    "    return sample_list\n",
    "\n",
    "\n",
    "def check_substr_contains_only_set(str, acceptable_chars):\n",
    "    validation = set(str)\n",
    "    print(\"Checking if it contains only \", acceptable_chars)\n",
    "    if validation.issubset(acceptable_chars):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def insert_source(context, fname):\n",
    "    pattern = r\"_([^_]*)_\"  # Matches substrings that start and end with \"_\"\n",
    "    matcher = re.search(pattern, fname)\n",
    "    addstr = str(matcher.group()).replace(\"_\", \"\").split(\".\")[0]\n",
    "    #context.insert(0, \"SRC_FILE: \" + addstr + \"COL_VALS: \")\n",
    "    context.insert(0, \"SRC: \" + addstr)\n",
    "    return context\n",
    "\n",
    "\n",
    "def get_df_sample(df, rand_seed, val_indices, len_context, min_variance=1, replace=False, full=False, other_col=False,\n",
    "                  max_len=8000):\n",
    "    column_samples = {}\n",
    "    ignore_list = [\"None\", 'none', 'NaN', 'nan', 'N/A', 'na', '']\n",
    "    for col in df.columns:\n",
    "        sample_list = list(\n",
    "            set(p[:max_len // (len_context * 3)] for p in pd.unique(df.astype(str)[col]) if p not in ignore_list))\n",
    "        #reformat integer samples\n",
    "        sl_mod = []\n",
    "        # Meta-features\n",
    "        if full:\n",
    "            meta_features = derive_meta_features(df[col])\n",
    "            meta_features['rolling-mean-window-4'] = meta_features['rolling-mean-window-4'][:5]\n",
    "        # Sampling from other columns\n",
    "        if other_col:\n",
    "            sample_list_fill_size = len_context - len(sample_list)\n",
    "            nc = len(df.columns)\n",
    "            per_column_context = max(1, sample_list_fill_size // nc)\n",
    "            for idx, oc in enumerate(df.columns):\n",
    "                items = df[oc].astype(str).iloc[0:per_column_context].tolist()\n",
    "                sample_list = sample_list + [\"OC: \" + str(item) for item in items]\n",
    "        if not sample_list:\n",
    "            sample_list = [\"None\"]\n",
    "        if len(sample_list) < len_context:\n",
    "            sample_list = sample_list * len_context\n",
    "        if len(sample_list) > len_context:\n",
    "            sample_list = sample_list[:len_context]\n",
    "        assert len(sample_list) == len_context, \"An index in val_indices is length \" + str(len(sample_list))\n",
    "        if full:\n",
    "            if meta_features['std'] == \"N/A\":\n",
    "                sample_list = sample_list + [\"\" for k, v in meta_features.items()]\n",
    "            else:\n",
    "                sample_list = sample_list + [str(k) + \": \" + str(v) for k, v in meta_features.items()]\n",
    "        # print(\"sample list\")\n",
    "        # print(sample_list)\n",
    "        column_samples[col] = sample_list\n",
    "        # print(\"column samples\")\n",
    "        # print(column_samples)\n",
    "    return pd.DataFrame.from_dict(column_samples)\n",
    "\n",
    "\n",
    "NUMERIC_AND_COMMA = set('0123456789,')\n",
    "\n",
    "BOOLEAN_SET = [\"True\", \"true\", \"False\", \"false\", \"yes\", \"Yes\", \"No\", \"no\"]\n",
    "\n",
    "\n",
    "def apply_basic_rules(context, lbl):\n",
    "    if not context:\n",
    "        return lbl\n",
    "    if not isinstance(context, list):\n",
    "        return lbl\n",
    "    try:\n",
    "        if all(s.endswith(\" g\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" kg\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" lb\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" lbs\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" pounds\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" cal\") for s in context):\n",
    "            lbl = \"calories\"\n",
    "        if all(s.endswith(\" kcal\") for s in context):\n",
    "            lbl = \"calories\"\n",
    "        if all(s.endswith(\" calories\") for s in context):\n",
    "            lbl = \"calories\"\n",
    "        if all(\"review\" in s.lower() for s in context):\n",
    "            lbl = \"review\"\n",
    "        if all(\"recipe\" in s.lower() for s in context):\n",
    "            lbl = \"recipe\"\n",
    "        if lbl and \"openopen\" in lbl:\n",
    "            lbl = \"openinghours\"\n",
    "        if all(s in BOOLEAN_SET for s in context):\n",
    "            lbl = \"medical_boolean\"\n",
    "        return lbl\n",
    "    except Exception as e:\n",
    "        print(f\"Exception {e} in apply_basic_rules with context {context}\")\n",
    "        return lbl\n",
    "\n",
    "\n",
    "def get_cbc_pred(orig_label, numeric_labels):\n",
    "    try:\n",
    "        #FOR VALIDATION\n",
    "        #cbc_filematch = dfv[dfv['df_path'] == str(f)]\n",
    "        #FOR TEST SET\n",
    "        cbc_filematch = dft[dft['df_path'] == str(f)]\n",
    "        cbc_labelmatch = cbc_filematch[cbc_filematch['label'] == orig_label]\n",
    "        if len(cbc_labelmatch) == 1:\n",
    "            cbc_pred = numeric_labels[cbc_labelmatch['preds'].item()]\n",
    "        else:\n",
    "            cbc_pred = None\n",
    "    except Exception as e:\n",
    "        print(\"cbc excpetion: \")\n",
    "        print(e)\n",
    "        cbc_pred = None\n",
    "\n",
    "\n",
    "def run_val(model: str, save_path: str, inputs: list, label_set: list, input_df: pd.DataFrame, resume: bool = True,\n",
    "            results: bool = True, stop_early: int = -1, rand_seed: int = 13, sample_size: int = 5, link: str = None,\n",
    "            response: bool = True, summ_stats: bool = False, table_src: bool = False, other_col: bool = False,\n",
    "            skip_short: bool = False, min_var: int = 0, method: list = [\"similarity\"]):\n",
    "    inputs = [Path(f) for f in inputs]\n",
    "\n",
    "    infmods = \"sherlock\" in model or \"doduo\" in model\n",
    "    isd4 = \"d4\" in label_set['name']\n",
    "    if resume and os.path.isfile(save_path):\n",
    "        with open(save_path, 'r', encoding='utf-8') as f:\n",
    "            prompt_dict = json.load(f)\n",
    "    else:\n",
    "        prompt_dict = {}\n",
    "    s = requests.Session()\n",
    "    if \"-zs\" in model:\n",
    "        base_model.eval()\n",
    "    if isinstance(inputs, dict):\n",
    "        labels = [\"_\".join(k.split(\"_\")[:-1]) for k in inputs.keys()]\n",
    "        inputs = list(inputs.values())\n",
    "    for idx, f in tqdm(enumerate(inputs), total=len(inputs)):\n",
    "        if idx % 100 == 0:\n",
    "            with open(save_path, 'w', encoding='utf-8') as alt_f:\n",
    "                #print(\"pd\", prompt_dict, \"\\n\")\n",
    "                json.dump(prompt_dict, alt_f, ensure_ascii=False, indent=4)\n",
    "        if stop_early > -1 and idx == stop_early:\n",
    "            break\n",
    "        if isd4:\n",
    "            f_df = f\n",
    "            label_indices = [2]\n",
    "            gt_labels = labels[idx]\n",
    "        else:\n",
    "            gt_labels = input_df[input_df['table_name'] == f.name]\n",
    "            label_indices = gt_labels['column_index'].unique().tolist()\n",
    "\n",
    "            if f.suffix.lower() == '.csv':\n",
    "                f_df = pd.read_csv(f)\n",
    "            else:\n",
    "                f_df = pd.read_json(f, compression='infer', lines=True)\n",
    "\n",
    "        if infmods:\n",
    "            label_indices = [\"values\"]\n",
    "            key = get_sherlock_resp(f_df, gt_labels, prompt_dict, model, label_indices, str(f), label_set)\n",
    "            continue\n",
    "        sample_df = get_df_sample(f_df, rand_seed, label_indices, sample_size, full=summ_stats, other_col=other_col,\n",
    "                                  max_len=MAX_LEN)\n",
    "        #print(f\"in main loop, sample_df is {sample_df}\")\n",
    "        f_df_cols = f_df.columns\n",
    "        for idx, col in enumerate(f_df_cols):\n",
    "            if idx not in label_indices:\n",
    "                continue\n",
    "            #NOTE: skipping evaluation for columns with insufficient variance in the column\n",
    "            #       if len(pd.unique(sample_df.astype(str)[col])) < min_var:\n",
    "            #         continue\n",
    "            if isd4:\n",
    "                orig_label = gt_labels\n",
    "            else:\n",
    "                gt_row = gt_labels[gt_labels['column_index'] == idx]\n",
    "                orig_label = gt_row['label'].item()\n",
    "            label = fix_labels(orig_label, label_set)\n",
    "            limited_context = sample_df[col].tolist()[:sample_size]\n",
    "            #NOTE: could consider using min_var here\n",
    "            #if full and len(pd.unique(sample_df[col].tolist())) < 3:\n",
    "            if table_src:\n",
    "                context = insert_source(sample_df[col].tolist(), f.name)\n",
    "            else:\n",
    "                context = sample_df[col].tolist()\n",
    "            if \"gpt-3.5\" in model:\n",
    "                key = get_chatgpt_resp(label_set, context, label, prompt_dict, response=response, session=s,\n",
    "                                       method=method)\n",
    "            elif \"ada-personal\" in model:\n",
    "                key = get_ada_resp(label_set, context, label, prompt_dict, response=response, session=s)\n",
    "            elif \"bloomz\" in model:\n",
    "                key = get_bloomz_resp(label_set, context, label, prompt_dict, response=response, session=s)\n",
    "            elif \"llama\" in model or \"-zs\" in model:\n",
    "                #cbc_pred = get_cbc_pred(orig_label, numeric_labels)\n",
    "                cbc_pred = None\n",
    "                key = get_llama_resp(label_set, context, label, prompt_dict, link=link, response=response, session=s,\n",
    "                                     cbc=cbc_pred, model=model, limited_context=limited_context, method=method)\n",
    "                # print(\"Key: \", key, \"\\n\")\n",
    "                #print(\"pdk\", prompt_dict[key], \"\\n\")\n",
    "            prompt_dict[key]['original_label'] = orig_label\n",
    "            prompt_dict[key]['file+idx'] = str(f) + \"_\" + str(idx)\n",
    "    with open(save_path, 'w', encoding='utf-8') as my_f:\n",
    "        json.dump(prompt_dict, my_f, ensure_ascii=False, indent=4)\n",
    "    if results:\n",
    "        results_checker(save_path, skip_duplicates=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "392f663e-4f8f-4671-ab88-21575e2fffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02993b81-f6b9-4b7b-b14f-a0ba901faef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from statistics import mean\n",
    "\n",
    "ENDINGS = [\"ANSWER:\", \"CATEGORY:\"]\n",
    "\n",
    "\n",
    "def results_checker_doduo(file_name, skip_duplicates=True):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "    correct = 0\n",
    "    n = len(d)\n",
    "    per_class_results = dict()\n",
    "    for k, v in d.items():\n",
    "        response_set = set(v[\"response\"])\n",
    "        for r in response_set:\n",
    "            per_class_results.setdefault(r, {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"Total\": 0})\n",
    "        per_class_results.setdefault(v[\"ground_truth\"], {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"Total\": 0})\n",
    "        if v['correct'] == True:\n",
    "            correct += 1\n",
    "            per_class_results[v[\"ground_truth\"]][\"TP\"] += 1\n",
    "        else:\n",
    "            per_class_results[v[\"ground_truth\"]][\"FN\"] += 1\n",
    "            for r in response_set:\n",
    "                per_class_results[r][\"FP\"] += 1\n",
    "        per_class_results[v[\"ground_truth\"]][\"Total\"] += 1\n",
    "\n",
    "    for k, v in per_class_results.items():\n",
    "        v['F1'] = (2 * v[\"TP\"]) / (2 * v[\"TP\"] + v[\"FP\"] + v[\"FN\"])\n",
    "\n",
    "    weighted_f1 = sum([v[\"F1\"] * v[\"Total\"] for k, v in per_class_results.items()]) / n\n",
    "    unweighted_f1 = mean([v[\"F1\"] for k, v in per_class_results.items()])\n",
    "\n",
    "    print(\n",
    "        f\"Total entries: {n} \\n Accuracy: {round(correct / n, 4)} \\n Weighted F1: {round(weighted_f1, 4)} \\n Unweighted F1: {round(unweighted_f1, 4)}\")\n",
    "\n",
    "\n",
    "def results_checker(file_name, skip_duplicates=True):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    if skip_duplicates:\n",
    "        d = {k: v for k, v in d.items() if \"CATEGORY: *\" not in str(k)}\n",
    "\n",
    "    # build the lists\n",
    "    y_true = [v[\"ground_truth\"] for v in d.values()]\n",
    "    y_pred = [v[\"response\"] for v in d.values()]\n",
    "\n",
    "    # overall stats\n",
    "    correct = sum(1 for gt, pred in zip(y_true, y_pred) if gt == pred)\n",
    "    n = len(y_true)\n",
    "    print(f\"Total entries: {n}\")\n",
    "    print(f\"Accuracy:     {correct / n:.4f}\\n\")\n",
    "\n",
    "    # per-class report\n",
    "    print(classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        digits=4,  # 4 decimal places\n",
    "        zero_division=0  # to avoid warnings if a class is never predicted\n",
    "    ))\n",
    "\n",
    "    # --- new: build a flattened metrics dict ---\n",
    "    raw_report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    flat = {}\n",
    "    # raw_report has keys for each label, plus 'macro avg', 'weighted avg', and 'accuracy'\n",
    "    for label, m in raw_report.items():\n",
    "        if label == \"accuracy\":\n",
    "            flat[\"accuracy\"] = m\n",
    "        else:\n",
    "            for metric_name, val in m.items():\n",
    "                flat[f\"{label}_{metric_name}\"] = val\n",
    "\n",
    "    # add summary fields\n",
    "    flat[\"total_entries\"] = n\n",
    "    # filename identifier: take it from your JSON filename variable\n",
    "    flat[\"run_name\"]      = os.path.basename(file_name).replace(\".json\",\"\")\n",
    "\n",
    "    # convert to one-row DataFrame\n",
    "    df = pd.DataFrame([flat])\n",
    "\n",
    "    metrics_csv = f\"{archetype_directory}/all_metrics.csv\"\n",
    "\n",
    "    # append (or create) the master CSV\n",
    "    if not os.path.isfile(metrics_csv):\n",
    "        df.to_csv(metrics_csv, index=False, float_format=\"%.4f\")\n",
    "    else:\n",
    "        df.to_csv(metrics_csv, mode=\"a\", header=False, index=False, float_format=\"%.4f\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8db2960f-ac1d-4199-9cbc-85b6d60f66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_entries(f1, f2):\n",
    "    with open(f1, \"r\") as file1:\n",
    "        d1 = json.load(file1)\n",
    "    with open(f2, \"r\") as file2:\n",
    "        d2 = json.load(file2)\n",
    "    paths1 = set([v[\"file+idx\"] for _, v in d1.items()])\n",
    "    paths2 = set([v[\"file+idx\"] for _, v in d2.items()])\n",
    "    return paths1 - paths2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23213247-0e7a-4b24-9eb5-a623264183ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_val_parquet(\n",
    "        model: str,\n",
    "        save_path: str,\n",
    "        labels_path: str,\n",
    "        data_path: str,\n",
    "        label_set: dict,\n",
    "        resume: bool = True,\n",
    "        results: bool = True,\n",
    "        stop_early: int = -1,\n",
    "        rand_seed: int = 13,\n",
    "        sample_size: int = 5,\n",
    "        link: str = None,\n",
    "        response: bool = True,\n",
    "        summ_stats: bool = False,\n",
    "        table_src: bool = False,\n",
    "        other_col: bool = False,\n",
    "        skip_short: bool = False,\n",
    "        min_var: int = 0,\n",
    "        method: list = [\"similarity\"],\n",
    "        results_checker=None,\n",
    "        MAX_LEN: int = 1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Validation loop adapted for parquet-based inputs:\n",
    "\n",
    "    - labels_path: path to a parquet file with columns ['__index_level_0__', 'type']\n",
    "    - data_path:   path to a parquet file with columns ['__index_level_0__', 'values']\n",
    "    - label_set:   dict containing 'name', 'label_set', 'dict_map', 'abbrev_map'\n",
    "\n",
    "    Each row in the merged DataFrame represents one column to predict:\n",
    "      - __index_level_0__ (column index)\n",
    "      - type (ground truth label)\n",
    "      - values (comma-separated or list of column values)\n",
    "    \"\"\"\n",
    "\n",
    "    generated_labels_list = []\n",
    "        \n",
    "    \n",
    "    # Load or initialize cache\n",
    "    if resume and os.path.isfile(save_path):\n",
    "        with open(save_path, 'r', encoding='utf-8') as f:\n",
    "            prompt_dict = json.load(f)\n",
    "    else:\n",
    "        prompt_dict = {}\n",
    "    \n",
    "\n",
    "    # Read parquet inputs and bring index into a column\n",
    "    labels_df = pd.read_parquet(labels_path).reset_index()\n",
    "    data_df = pd.read_parquet(data_path).reset_index()\n",
    "\n",
    "    # Identify the index column name (either __index_level_0__ or generic index)\n",
    "    labels_idx_col = '__index_level_0__' if '__index_level_0__' in labels_df.columns else 'index'\n",
    "    data_idx_col = '__index_level_0__' if '__index_level_0__' in data_df.columns else 'index'\n",
    "\n",
    "    # Rename for clarity: index → col_idx, type → label, values stays values\n",
    "    labels_df = labels_df.rename(columns={labels_idx_col: 'col_idx', 'type': 'label'})\n",
    "    data_df = data_df.rename(columns={data_idx_col: 'col_idx', 'values': 'values'})\n",
    "\n",
    "    # Remap labels using LABEL_MAP_LC\n",
    "    # assumes remap_labels(series, mapping) is defined and LABEL_MAP_LC is available\n",
    "\n",
    "    #labels_df['label'] = remap_labels(labels_df['label'], LABEL_MAP_LC)\n",
    "\n",
    "    # Filter out __none__ labels\n",
    "    labels_df = labels_df[labels_df['label'] != \"__none__\"]\n",
    "\n",
    "    # Merge on column index\n",
    "    merged = pd.merge(labels_df, data_df, on='col_idx', how='inner')\n",
    "\n",
    "    # Prepare session and model\n",
    "    session = requests.Session()\n",
    "    if \"-zs\" in model:\n",
    "        base_model.eval()\n",
    "\n",
    "    # Iterate over each column instance\n",
    "    for idx, row in tqdm(enumerate(merged.itertuples(index=False)), total=len(merged)):\n",
    "        \n",
    "        # Periodic cache save\n",
    "        if idx % 100 == 0:\n",
    "            with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(prompt_dict, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        if stop_early > -1 and idx == stop_early:\n",
    "            break\n",
    "\n",
    "        col_idx = row.col_idx\n",
    "        orig_label = row.label\n",
    "        raw_vals = row.values\n",
    "\n",
    "        # Parse raw values into a list\n",
    "        if isinstance(raw_vals, str):\n",
    "            vals = raw_vals.split(',')\n",
    "        else:\n",
    "            vals = list(raw_vals)\n",
    "\n",
    "        # Deduplicate and sample\n",
    "        vals = [str(x) for x in vals]\n",
    "        unique_vals = pd.unique(vals)\n",
    "        context_list = unique_vals.tolist()[:sample_size]\n",
    "\n",
    "        # Build context\n",
    "        if table_src:\n",
    "            context = insert_source(context_list, str(col_idx))\n",
    "        else:\n",
    "            context = context_list\n",
    "\n",
    "        # Model call\n",
    "        if \"gpt-3.5\" in model:\n",
    "            key = get_chatgpt_resp(label_set, context, orig_label,\n",
    "                                   prompt_dict, response=response,\n",
    "                                   session=session, method=method)\n",
    "        elif \"ada-personal\" in model:\n",
    "            key = get_ada_resp(label_set, context, orig_label,\n",
    "                               prompt_dict, response=response,\n",
    "                               session=session)\n",
    "        elif \"bloomz\" in model:\n",
    "            key = get_bloomz_resp(label_set, context, orig_label,\n",
    "                                  prompt_dict, response=response,\n",
    "                                  session=session)\n",
    "        else:\n",
    "            raw_prompt, answer = get_llama_resp(label_set, context, orig_label,\n",
    "                                 prompt_dict, link=link,\n",
    "                                 response=response,\n",
    "                                 session=session,\n",
    "                                 cbc=None,\n",
    "                                 model=model,\n",
    "                                 limited_context=context_list,\n",
    "                                 method=method,\n",
    "                                 generated_labels_list = generated_labels_list)\n",
    "        \n",
    "\n",
    "        # Record metadata\n",
    "        prompt_dict[raw_prompt]['original_label'] = orig_label\n",
    "        prompt_dict[raw_prompt]['file+idx'] = str(col_idx)\n",
    "\n",
    "        # but you also get to see the actual label answer:\n",
    "        print(\"PROMPT SENT:\\n\", raw_prompt)\n",
    "        print(\"MODEL ANSWER:\", answer)\n",
    "\n",
    "    # Final cache save\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(prompt_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Optional result summary\n",
    "    #if results and results_checker is not None:\n",
    "    #    results_checker(save_path, skip_duplicates=False)\n",
    "\n",
    "    return prompt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbb07a01-53a9-4639-b197-fb24a0479456",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = {\n",
    "  \"name\": \"custom_csv\",     # any string that does NOT contain \"d4\"\n",
    "  \"label_set\": LABELS,      # the list of your labels, used by similarity\n",
    "  \"dict_map\": { lab: lab for lab in LABELS },\n",
    "  \"abbrev_map\": {}          # or your real abbrev map\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cd83aaa-8825-403e-9f26-b62e8fd0765e",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2707259907e4d9abbcb43aa5b4c2f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2077729/2465693990.py:98: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unique_vals = pd.unique(vals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt context insert \n",
      "LLama model was called\n",
      "LLM Picker 1 answer: response_status... Should be none in the beginning\n",
      "List of label so far: ['response_status']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [YES, NO]\n",
      "                    OPTIONS: \n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: response_status\n",
      "Prompt context insert response_status\n",
      "LLama model was called\n",
      "LLM Picker 1 answer: diarrhea_clinical_manifestation... Should be none in the beginning\n",
      "List of label so far: ['response_status', 'diarrhea_clinical_manifestation']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Profuse watery diarrhea|, Rice-water stools|, Severe dehydration|Vomiting|Profuse watery diarrhea|, Profuse watery diarrhea|Severe dehydration|, Profuse watery diarrhea|Vomiting|Severe dehydration|]\n",
      "                    OPTIONS: response_status\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: diarrhea_clinical_manifestation\n",
      "Prompt context insert response_status - diarrhea_clinical_manifestation\n",
      "LLama model was called\n",
      "LLM Picker 1 answer: education_level... Should be none in the beginning\n",
      "List of label so far: ['response_status', 'diarrhea_clinical_manifestation', 'education_level']\n",
      "PROMPT SENT:\n",
      " \n",
      "                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\n",
      "\n",
      "                    INSTRUCTIONS:\n",
      "                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\n",
      "                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                    INPUT: [Pensioner, Scholar, Grant Administrator]\n",
      "                    OPTIONS: response_status - diarrhea_clinical_manifestation\n",
      "                    ANSWER:\n",
      "                \n",
      "MODEL ANSWER: education_level\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\"\\n                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\\n\\n                    INSTRUCTIONS:\\n                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\\n                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\\n                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\\n\\n                    INPUT: [YES, NO]\\n                    OPTIONS: \\n                    ANSWER:\\n                \": {'response': 'response_status',\n",
       "  'context': ['YES', 'NO'],\n",
       "  'ground_truth': 'contact_setting',\n",
       "  'correct': False,\n",
       "  'original_model_answer': 'response_status',\n",
       "  'original_label': 'contact_setting',\n",
       "  'file+idx': '33'},\n",
       " \"\\n                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\\n\\n                    INSTRUCTIONS:\\n                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\\n                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\\n                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\\n\\n                    INPUT: [Profuse watery diarrhea|, Rice-water stools|, Severe dehydration|Vomiting|Profuse watery diarrhea|, Profuse watery diarrhea|Severe dehydration|, Profuse watery diarrhea|Vomiting|Severe dehydration|]\\n                    OPTIONS: response_status\\n                    ANSWER:\\n                \": {'response': 'diarrhea_clinical_manifestation',\n",
       "  'context': ['Profuse watery diarrhea|',\n",
       "   'Rice-water stools|',\n",
       "   'Severe dehydration|Vomiting|Profuse watery diarrhea|',\n",
       "   'Profuse watery diarrhea|Severe dehydration|',\n",
       "   'Profuse watery diarrhea|Vomiting|Severe dehydration|'],\n",
       "  'ground_truth': 'symptoms',\n",
       "  'correct': False,\n",
       "  'original_model_answer': 'diarrhea_clinical_manifestation',\n",
       "  'original_label': 'symptoms',\n",
       "  'file+idx': '50'},\n",
       " \"\\n                    SYSTEM: You are an epidemiology data steward labeling anonymized columns.\\n\\n                    INSTRUCTIONS:\\n                    • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\\n                    • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format.\\n                    • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\\n\\n                    INPUT: [Pensioner, Scholar, Grant Administrator]\\n                    OPTIONS: response_status - diarrhea_clinical_manifestation\\n                    ANSWER:\\n                \": {'response': 'education_level',\n",
       "  'context': ['Pensioner', 'Scholar', 'Grant Administrator'],\n",
       "  'ground_truth': 'occupation',\n",
       "  'correct': False,\n",
       "  'original_model_answer': 'education_level',\n",
       "  'original_label': 'occupation',\n",
       "  'file+idx': '51'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_name = \"alpaca-fine-tuned\"\n",
    "#model_name = \"flan-ul2\"\n",
    "#model_name = \"alpaca-13b\"\n",
    "\n",
    "model_name=\"llama\"\n",
    "\n",
    "filename = f\"custom-data-{model_name}-label-generation.json\"\n",
    "\n",
    "sp = f\"{archetype_directory}/custom_data_logs/{filename}\"\n",
    "\n",
    "\n",
    "dirpath = os.path.dirname(sp)\n",
    "os.makedirs(dirpath, exist_ok=True)\n",
    "\n",
    "#model_name = \"flan-t5-xxl\"\n",
    "\n",
    "#base_model, tokenizer, template, pt, MAX_LEN = init_model(model_name)\n",
    "\n",
    "# Test set\n",
    "#labels_path = \"/home/omadbek/projects/Sherlock/custom_data/label_generation/test_labels_generation.parquet\"\n",
    "#data_path = \"/home/omadbek/projects/Sherlock/custom_data/label_generation/test_data_generation.parquet\"\n",
    "\n",
    "\n",
    "# LLAMA\n",
    "run_val_parquet(\n",
    "    model=\"llama\",\n",
    "    save_path=sp,\n",
    "    labels_path=labels_path,\n",
    "    data_path=data_path,\n",
    "    label_set=label_set,\n",
    "    method=[\"similarity\"],\n",
    "    resume=False,\n",
    "    sample_size=5,\n",
    "    #stop_early = 30,\n",
    "    link = \"http://localhost:11434/api/generate\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe4311a83efa0299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File exists: ./custom_data_logs/custom-data-llama-label-generation.json\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(sp):\n",
    "    print(f\"✅ File exists: {sp}\")\n",
    "else:\n",
    "    print(f\"❌ File not found: {sp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1ad97-c094-4b25-bace-e4e645453f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8536b25-b8ce-48ce-a5e6-d33ad261362d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8d392-38ea-45d5-8e85-281049b5bdca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad31181-a79e-4f90-a55b-7d104570a862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5dc90d553807d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c18a5cc-b04b-4450-a816-0d60a5e2a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy matcher\n",
    "\n",
    "# --- Evaluation function ---\n",
    "def evaluate_and_remap(\n",
    "    file_name: str,\n",
    "    embed_threshold: float = 0.40,\n",
    "    fuzz_threshold: int = 30\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads your JSON, uses fuzzy matching to snap each 'response' to the closest\n",
    "    ground-truth label (if similar), and counts fuzzy-correct matches.\n",
    "    Adds two new fields to each record in the loaded dict:\n",
    "      - 'resolved_response': the matched label or original response\n",
    "      - 'correct_fuzzy': bool, whether resolved_response == ground_truth\n",
    "    Returns the updated data dict and the fuzzy accuracy.\n",
    "    \"\"\"\n",
    "    # 1) Load JSON\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 2) Build matcher over your ground-truth set\n",
    "    label_set = sorted({v[\"ground_truth\"] for v in data.values()})\n",
    "    matcher = _FuzzyLabelMatcher(label_set, embed_threshold, fuzz_threshold)\n",
    "\n",
    "    # 3) Remap each response & compute fuzzy correctness\n",
    "    total = 0\n",
    "    fuzzy_correct = 0\n",
    "    for record in data.values():\n",
    "        total += 1\n",
    "        orig = record[\"response\"]\n",
    "        match = matcher.resolve(orig)\n",
    "        resolved = match if match is not None else orig\n",
    "        record[\"resolved_response\"] = resolved\n",
    "        is_correct = (resolved == record[\"ground_truth\"])\n",
    "        record[\"correct_fuzzy\"] = is_correct\n",
    "        if is_correct:\n",
    "            fuzzy_correct += 1\n",
    "\n",
    "    # 4) Print fuzzy accuracy\n",
    "    acc = fuzzy_correct / total if total else 0.0\n",
    "    total_correct = f\"{fuzzy_correct}/{total}\" \n",
    "    print(f\"Fuzzy‐matched correct: {fuzzy_correct}/{total}  →  Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return total_correct, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d33e0466-abe0-4074-b1a0-4c25c3302673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy‐matched correct: 2/3  →  Accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "total_correct, fuzzy_acc = evaluate_and_remap(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a658307-6081-4049-a4f2-01da06172f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ground_truth                         response  fuzz_score  cos_score\n",
      "0  contact_setting                  response_status   40.000000   0.264160\n",
      "1         symptoms  diarrhea_clinical_manifestation   15.384615   0.405099\n",
      "2       occupation                  education_level   48.000000   0.379917\n"
     ]
    }
   ],
   "source": [
    "import json, re, pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# load data\n",
    "data = json.load(open(f\"{archetype_directory}/custom_data_logs/custom-data-llama-0.0.json\"))\n",
    "rows = []\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
    "for rec in data.values():\n",
    "    gt   = rec[\"ground_truth\"]\n",
    "    resp = rec[\"response\"]\n",
    "    # fuzzy\n",
    "    fuzz_score = fuzz.token_sort_ratio(resp, gt)\n",
    "    # cosine\n",
    "    emb_r = model.encode([resp], normalize_embeddings=True)\n",
    "    emb_g = model.encode([gt],   normalize_embeddings=True)\n",
    "    cos_score = util.cos_sim(emb_r, emb_g)[0][0].item()\n",
    "    rows.append({\n",
    "        \"ground_truth\": gt,\n",
    "        \"response\":     resp,\n",
    "        \"fuzz_score\":   fuzz_score,\n",
    "        \"cos_score\":    cos_score\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.describe())      # summary of scores\n",
    "print(df.head(20))        # inspect first 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b1ac587-8505-46da-88d1-a04bbe3dec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put to txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3985f1c2-a8de-4d77-96ae-a6588bb84c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote fuzzy results to ./temprorary/fuzzy_result.txt\n"
     ]
    }
   ],
   "source": [
    "out_path = f\"{archetype_directory}/temprorary/fuzzy_result.txt\"\n",
    "\n",
    "out_dir = os.path.dirname(out_path)   # “./temprorary”\n",
    "os.makedirs(out_dir, exist_ok=True)   # create it (if needed)\n",
    "\n",
    "with open(out_path, \"w\") as f:\n",
    "    f.write(f\"Fuzzy‐matched correct semantic types: {total_correct}\"\n",
    "            f\"  →  Accuracy: {fuzzy_acc:.4f}\\n\")\n",
    "    f.write(\"Similarity Scores: \\n\")\n",
    "    f.write(df.head(20).to_string(index=True))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Wrote fuzzy results to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2bbabd084979a",
   "metadata": {},
   "source": [
    "# Remapping and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f3f89-a2f9-4f64-908a-f7bfbf37bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"../custom_data_logs\"\n",
    "\n",
    "# 1) Load results\n",
    "with open(f'{data_dir}/custom-data-llama-0.0.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2) Flatten to DataFrame\n",
    "records = []\n",
    "for entry in data.values():\n",
    "    records.append({\n",
    "        'file_idx': entry['file+idx'],\n",
    "        'ground_truth': entry['ground_truth'],\n",
    "        'predicted': entry['response']\n",
    "    })\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "print(\"Unique values: \", sorted(df['predicted'].unique()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ecc133-27f6-48c2-9387-24e8eab89d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "summary = (\n",
    "    df.groupby('predicted')['ground_truth']\n",
    "      .agg(['nunique', lambda x: list(x.unique())])\n",
    "      .rename(columns={'nunique':'# distinct', '<lambda_0>':'values'})\n",
    ")\n",
    "print(summary)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88259c1e-0fb6-4f44-a9e4-6f0fa627ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# build a dict of {predicted_label: list_of_unique_ground_truths}\n",
    "unique_vals = {\n",
    "    cat: df.loc[df['predicted'] == cat, 'ground_truth'].unique().tolist()\n",
    "    for cat in df['predicted'].unique()\n",
    "}\n",
    "\n",
    "# print them out\n",
    "for cat, vals in unique_vals.items():\n",
    "    print(f\"{cat} ({len(vals)} distinct values):\")\n",
    "    print(vals, \"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48da796a-9465-42db-afa8-4774033a5766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 3) Define mapping dict\n",
    "label_mapping = {\n",
    "    # identifiers\n",
    "    'patient_id':                'id',\n",
    "    'patient_ids':               'id',\n",
    "\n",
    "    # yes/no, true/false\n",
    "    'response_status':          'medical_boolean',\n",
    "    'medical_boolean':          'medical_boolean',\n",
    "\n",
    "    # recovery/outcome\n",
    "    'patient_recovery_status':   'outcome',\n",
    "\n",
    "    # dates\n",
    "    'date_range':               'date',\n",
    "\n",
    "    # gender\n",
    "    'gender_type':              'gender',\n",
    "\n",
    "    # locations\n",
    "    'country_list':             'location',\n",
    "\n",
    "    # free‐text details\n",
    "    'diagnosis_details':        'symptoms',\n",
    "    'role_types':               'occupation',\n",
    "}\n",
    "\n",
    "# 4) Apply mapping\n",
    "df['mapped_predicted'] = df['predicted'].map(label_mapping).fillna(df['predicted'])\n",
    "\n",
    "# 5) Display DataFrame\n",
    "#print(df[['file_idx', 'ground_truth', 'predicted', 'mapped_predicted']])\n",
    "\n",
    "# 6) Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(df['ground_truth'], df['mapped_predicted'], zero_division=0))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b45c2-1068-45dc-80bb-e58d7ecc21ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
