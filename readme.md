# Extensions and Modifications in This Repository

This repository extends the original ArcheType framework with additional notebooks for dataset creation, custom experiments, and semantic type generation using large language models (LLMs). Below is an overview of the added components:

`archetype/notebooks/`

Contains notebooks used throughout the project for dataset preparation, fine-tuning, and evaluation:

- **`create_custom_dataset.ipynb`**  
  Notebook that creates an instruction-tuned dataset for fine-tuning LLM models.  
  This step ensures the model can be trained on task-specific instructions for semantic type annotation.

- **`label-generation.ipynb`**  
  Notebook that runs semantic type generation using the LLaMA model.  
  It explores how LLMs can generate new column labels beyond the fixed type set.

- **`custom_inference.ipynb`**  
  Notebook for running the Column Type Annotation (CTA) task using an LLM model.  
  It performs direct inference on datasets to classify columns into semantic types.

- **`archetype_label_generation.ipynb`**  
  Notebook used for final experiments in semantic type generation.  
  Multiple runs were conducted here to ensure stability and reproducibility of results.

- **`archetype_experiments_custom.ipynb`**  
  Notebook used for final experiments on the CTA task.  
  Like the label generation experiments, this was repeated multiple times for evaluation consistency.

---

These extensions make the repository more practical by enabling:  
- Creation of custom, instruction-tuned datasets.  
- Fine-tuning and inference of LLMs (e.g., LLaMA) for CTA and semantic type generation.  
- Systematic evaluation of LLM-based approaches through repeated experiments.  

# ArcheType: A novel framework for open-source column type annotation using large language models 

This repository contains the codebase of our paper ArcheType: A novel framework for open-source column type annotation using large language models, available at arXiv.

## Hardware Requirements

ArcheType requires an NVIDIA GPU with at least 12GB VRAM.

## Installation

```console
$ git clone [link to repo]
$ cd archetype
$ pip install -r requirements.txt 
```

With `conda`, create a virtual environment and install the required packages as below:


```console
$ conda create --name archetype python=3.7.10
$ conda activate archetype
$ pip install -r requirements.txt
```

### Doduo

Doduo must be installed separately -- please follow the instructions in [this repository](https://github.com/megagonlabs/doduo).

Don't forget to add Doduo to your Python path.

```console
$export PYTHONPATH=<DODUO_PATH>:$PYTHONPATH
```

### Editing Paths

In const.py, replace the ARCHETYPE_PATH variable with the absolute path to your ArcheType installation.
In const.py, replace the DOTENV_PATH variable with the absolute path to the directory containing your dotenv with your OpenAI API key.
  
## Benchmarks

### SOTAB-27 and SOTAB-91

The SOTAB-27 and SOTAB-91 benchmarks were constructed from the SOTAB dataset. The SOTAB train, validation and test files, as well as instructions for their use, can be acquired via [their website.](http://webdatacommons.org/structureddata/sotab/)

### D4-20

The D4 data originated [here](https://zenodo.org/record/3647613), but our codebase includes all the necessary metadata to reconstruct the benchmark -- you can run D4Tables without any additional downloads.

### Amstr-56 and PubChem-20

While the exact evaluation CSVs used in our paper are available, generating new ones is a straightforward process; the preprocessed PubChem and Amstr datasets can be downloaded from [here](https://huggingface.co/datasets/penfever/archetype-datasets). Please note that you will need to use Unix cat (or an equivalent) to reassemble the largest SQF file. 

```bash
cat sqf_part_* > reassembled.sqf
```

Our evaluation CSVs were generated by randomly sampling unique values from columnar formatted data with replacement to create synthetic tables. e.g., the file `pubchemtables_pc_book_000001.json` contains around 1,900 values for the CTA field 'book' in the PubChem dataset. Samples from the resulting problems can be found in `dataset_samples`.

### Amstr-56

The evaluation CSVs are [here](https://huggingface.co/datasets/penfever/archetype-amstr).

### PubChem-20

The PubChem-20 evaluation CSVs can be downloaded from [here](https://huggingface.co/datasets/penfever/archetype-pubchem).

## Training

ArcheType training makes use of the [Alpaca-7B](https://github.com/tatsu-lab/stanford_alpaca) repository and model. In order to train your own ArcheType model as described in the paper, you will need to acquire a copy of the [Alpaca-7B weights](https://huggingface.co/chavinlo/alpaca-native) as well as the training code.

### Dataset Creation

You can use our [pre-formatted training JSON](https://huggingface.co/datasets/penfever/archetype-ft-sotab) for SOTAB.

You can also reproduce this JSON yourself (or generate a training JSON for another dataset of your choosing) via a two-stage process -- 

1. To generate the formatted columns, run a standard ArcheType task for the downstream architecture you intend to use, but drop the `--response` flag.
```console
python archetype/src/run.py --model_name="ArcheType-llama" --model_path=%%GEN_MODEL_PATH%% --save_path=%%JSON_OUT_PATH%% --input_files="/scratch/bf996/datasets/sotab/Train" --input_labels="/scratch/bf996/datasets/sotab/CTA_training_gt.csv" --label_set="SOTAB-91" --method ans_contains_gt gt_contains_ans resample
```
2. To convert the generated JSON to the format expected by Alpaca, you can either write a script yourself or customize ours --
```console
python archetype/src/make_dataset.py --in_file %%JSON_OUT_PATH%%
```

### Training Command

We used the following command to train our models --

```console
torchrun --nproc_per_node=4 --rdzv_backend=c10d --rdzv_endpoint=<MASTER_ADDR>:<MASTER_PORT> train.py --model_name_or_path <ALPACA_WEIGHTS> --data_path <TRAINING_JSON> --bf16 True --output_dir <SAVE_DIR --num_train_epochs 3 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_steps 16 --evaluation_strategy "no" --save_strategy "steps" --save_steps 2000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type "cosine" --logging_steps 1 --fsdp "full_shard auto_wrap" --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' --tf32 True
```

### Usage Examples

To reproduce our zero-shot SOTAB results using the flan-t5 architecture and S prompt, use the following command --

```console
python src/run.py --model_name="flan-t5-xxl-zs-shortprompt" --save_path=<SAVE_PATH> --input_files="<PATH>/Test" --input_labels="<PATH>/CTA_test_gt.csv" --label_set="SOTAB-91" --method ans_contains_gt gt_contains_ans resample --results --rules --response;
```

To reproduce our fine-tuned model experiments, you must also provide the path to model weights.

```console
--model_name="ArcheType-llama" --model_path=<WEIGHTS_PATH>
```

To reproduce the D4-20 results, make the following substitutions --

```console
--input_files="D4" --input_labels="D4" --label_set="D4-ZS"
```

For Amstr-56 --

```console
--input_files="<PATH>/amstr_csv" --input_labels="amstr" --label_set="amstr-ZS"
```

For PubChem-20 --

```console
--input_files="<PATH>/pubchem_csv" --input_labels="pubchem" --label_set="pubchem-ZS"
```

To reproduce the C-Baseline on T5 --

```
--model_name="flan-t5-xxl-zs-chorusprompt" --method similarity simple_random_sampling
```

To reproduce the K-Baseline on T5 --

```
--model_name="flan-t5-xxl-zs-koriniprompt" --method first_sampling
```

To vary the choice of prompt, substitute the correct prompt name in the model field.

Here are the commands for all six prompts listed in our paper (K, C, S, N, I, O)

```
--model_name="flan-t5-xxl-zs-koriniprompt" --model_name="flan-t5-xxl-zs-chorusprompt" --model_name="flan-t5-xxl-zs-shortprompt" --model_name="flan-t5-xxl-zs-noisyprompt" --model_name="flan-t5-xxl-zs-invertedprompt" --model_name="flan-t5-xxl-zs"
```

The model_name field also controls which model is called at the model querying stage.

```console
--model_name="gpt-3.5-turbo", --model_name="flan-ul2-zs", --model_name="flan-t5-xxl-zs"
```

To only evaluate a subset of the data, use the `--stop_early` flag. To reproduce the ArcheType+ results, include the `--rules` flag.

## Custom Labels and Data

Any LLM supported by ArcheType can be used to perform zero-shot CTA on your own custom data from the command line, using a label set of your choice.

Your data can be in any format recognized by Pandas, including csv, tsv, Excel, parquet and sql.

### From a Colab Notebook

Please follow our example notebook, `notebooks/Custom_Predictions_with_ArcheType.ipynb`

### From the Command Line

This example uses ArcheType-gpt-3.5, and has a set of four custom labels: text number id, and place.

```console
$ python archetype/src/run.py --model_name="gpt-3.5" --save_path="<SAVE_PATH>/results.json" --input_files="<CUSTOM_PATH>/Test" --input_labels="skip-eval" --label_set="custom" --custom-labels text number id place --method ans_contains_gt gt_contains_ans resample --response
```

## Evaluation

You can evaluate a results json using eval.py. Add --confusion_matrix to generate confusion matrices at the same time.

```
python archetype/src/eval.py --input_path <FILE_PATH> --label_set <LABEL_SET_NAME> --confusion_matrix
```

You can also evaluate only certain classes in the entire test set.

```
python src/eval.py --input_path <FILE_PATH> --label_set "SOTAB-91" --ignore_classes "weight, Energy, Review, Recipe/name, openingHours, Boolean, EducationalOccupationalCredential, Action, Photograph, URL, ItemList, EventAttendanceModeEnumeration, EventStatusType, DayOfWeek, ItemAvailability, RestrictedDiet, OfferItemCondition"
```

## Label Sets in our Paper

SOTAB-27: 
```
[
 'Boolean',
 'Coordinates',
 'Country',
 'CreativeWork',
 'Date',
 'Event',
 'Gender',
 'JobPosting',
 'Language',
 'Company',
 'Number',
 'Organization',
 'Person',
 'Product',
 'SportsTeam',
 'Text',
 'Time',
 'URL',
 'category',
 'currency',
 'email',
 'price',
 'streetAddress',
 'telephone',
 'Age',
 'weight',
 'zipCode']
```

SOTAB-91: See the [associated site.](http://webdatacommons.org/structureddata/sotab/#toc8)

D4Tables:
```
['School ID',
 'Ethnicity',
 'Letter Grade',
 'Educational Organization',
 'School DBN',
 'Region in Brooklyn',
 'Region in Bronx',
 'Permit Type',
 'Region in Queens',
 'Region in Manhattan',
 'Region in Staten Island',
 'County',
 'Elevator or Staircase',
 'Short City Agency Name',
 'Color',
 'Full City Agency Name',
 'Country',
 'State',
 'Month',
 'License plate type']
```

PubChemTables:
```
['Person\'s First Name and Middle Initials', 
'Molecular Formula', 
'Book Title', 
'Cell Alternative Label', 
'Book Title',
'Disease Alternative Label', 
'MD5 Hash', 
'Person\'s Last Name', 
'Biological Formula', 
'Taxonomy Label',
'InChI (International Chemical Identifier)', 
'SMILES (Simplified Molecular Input Line Entry System)', 
'Abstract for Patent', 
'Organization', 
'Book ISBN', 
'Concept Broader Term', 
'Journal Title', 
'Chemical',
'Person\'s Full Name', 
'Journal ISSN', 
'Patent Title']
```

AmstrTables

Here, `<STATE_NAME>` stands in for all fifty U.S. states.

```
['Newspaper or Publication',
'Numeric Identifier',
'Town',
'State',
'Headline',
'Author Byline',
'Article from <STATE_NAME>]
```

## Citation

If you find this useful, please cite our work --

```
@article{10.14778/3665844.3665857,
  author = {Feuer, Benjamin and Liu, Yurong and Hegde, Chinmay and Freire, Juliana},
  title = {ArcheType: A Novel Framework for Open-Source Column Type Annotation Using Large Language Models},
  year = {2024},
  issue_date = {May 2024},
  publisher = {VLDB Endowment},
  volume = {17},
  number = {9},
  issn = {2150-8097},
  url = {https://doi-org.proxy.library.nyu.edu/10.14778/3665844.3665857},
  doi = {10.14778/3665844.3665857},
  abstract = {Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type; incur high run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve CTA problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes a new state-of-the-art performance on zero-shot CTA benchmarks (including three new domain-specific benchmarks which we release along with this paper), and when used in conjunction with classical CTA techniques, it outperforms a SOTA DoDuo model on the fine-tuned SOTAB benchmark.},
  journal = {Proc. VLDB Endow.},
  month = may,
  pages = {2279–2292},
  numpages = {14}
}
```

## Acknowledgments

Datasets in this repo include variations of the "Sato" and "TURL" datasets.

Sato refers to the dataset used in ["Sato: Contextual Semantic Type Detection in Tables." Proceedings of the VLDB Endowment Vol. 13, No.11](https://github.com/megagonlabs/sato). The dataset was generated from the [VizNet](https://github.com/mitmedialab/viznet) corpus.

URL refers to the dataset used in ["TURL: table understanding through representation learning." Proceedings of the VLDB Endowment 14.3 (2020): 307-319](https://github.com/sunlab-osu/TURL). The dataset was generated from the [WikiTable](http://websail-fe.cs.northwestern.edu/TabEL/) corpus.
